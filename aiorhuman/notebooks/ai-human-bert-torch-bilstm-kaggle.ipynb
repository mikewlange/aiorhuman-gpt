{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19eba1a3",
   "metadata": {
    "papermill": {
     "duration": 0.012901,
     "end_time": "2024-01-21T00:13:08.234799",
     "exception": false,
     "start_time": "2024-01-21T00:13:08.221898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# War of the Words: Human vs AI Essay Challenge\n",
    "<img src=\"https://mikewlange.github.io/ai-or-human/images/ai_or_human_overview.png\" alt=\"Alt Text\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11a17f",
   "metadata": {
    "papermill": {
     "duration": 0.012245,
     "end_time": "2024-01-21T00:13:08.259734",
     "exception": false,
     "start_time": "2024-01-21T00:13:08.247489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction \n",
    "\n",
    "&#10132; This notebook explores the capability of machine learning algorithms to distinguish between essays written by humans and those generated by AI models. \n",
    "\n",
    "> <span style=\"display: inline-block;padding: 5px;background-color: #f4f3ee;margin-bottom: 5px;line-height: 1.5;color: #333;\" class=\"tip\"> <b>Merlin's Beard! </b>Anytime you pass \"automated judgment\" on a human, and this IS that, it's not a bad idea to do so in a thoughtful and transparent way. </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e322d9",
   "metadata": {
    "papermill": {
     "duration": 0.012507,
     "end_time": "2024-01-21T00:13:08.285233",
     "exception": false,
     "start_time": "2024-01-21T00:13:08.272726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Competition:** https://www.kaggle.com/competitions/llm-detect-ai-generated-text\n",
    "\n",
    "\"*Can you help build a model to identify which essay was written by middle and high school students, and which was written using a large language model?*\"\n",
    "\n",
    "**Perhaps..**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeca86b",
   "metadata": {
    "papermill": {
     "duration": 0.013514,
     "end_time": "2024-01-21T00:13:08.311556",
     "exception": false,
     "start_time": "2024-01-21T00:13:08.298042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Notebooks\n",
    "\n",
    "Data: \n",
    "- https://www.kaggle.com/datasets/geraltrivia/llm-detect-gpt354-generated-and-rewritten-essays\n",
    "- https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset \n",
    "\n",
    "Models: \n",
    "- https://www.kaggle.com/code/geraltrivia/ai-human-pytorchbertsequenceclassifier-model\n",
    "- https://www.kaggle.com/code/geraltrivia/ai-human-pytorchcustombertclassifier-model\n",
    "\n",
    "- Main Notebook: https://www.kaggle.com/code/geraltrivia/ai-human-pytorchbert-2-ebm\n",
    "\n",
    "note: these are the two models in this notebook. Had to train in another, it's too expensive to run all at once.\n",
    "\n",
    "Notebooks: This code and the ClearML pipelines including the Essay Generation are here https://github.com/mikewlange/ai-or-human. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20696517",
   "metadata": {
    "papermill": {
     "duration": 0.012732,
     "end_time": "2024-01-21T00:13:08.337186",
     "exception": false,
     "start_time": "2024-01-21T00:13:08.324454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Setup for offline Run. \n",
    "\n",
    "It's possible I'm doing this all wrong. But to submit to a contest that disables the internet you neeed to add pypy packages and other code/models that are not installed in the Kagle docker image https://github.com/Kaggle/docker-python, into a Dataset (I.E sotrage) and use that as iinput to your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8095760c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:08.366190Z",
     "iopub.status.busy": "2024-01-21T00:13:08.365795Z",
     "iopub.status.idle": "2024-01-21T00:13:08.372398Z",
     "shell.execute_reply": "2024-01-21T00:13:08.371578Z"
    },
    "papermill": {
     "duration": 0.024558,
     "end_time": "2024-01-21T00:13:08.374721",
     "exception": false,
     "start_time": "2024-01-21T00:13:08.350163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn off he internet and doing your pip installs, the packages you can't install, add to the libray var below \n",
    "\n",
    "# TURN ON INTERNET FOR THIS\n",
    "# creates a wheelhouse to add \n",
    "# \n",
    "# library = \\\n",
    "# '''\n",
    "# textstat\n",
    "# clearml\n",
    "# sentence_transformers\n",
    "# optuna\n",
    "# interpret\n",
    "# torchsummary\n",
    "# empath\n",
    "# benepar\n",
    "# '''.lstrip('\\n')\n",
    "# with open('requirements.txt', 'w+') as f:\n",
    "#     f.write(library)\n",
    "    \n",
    "#!mkdir wheelhouse && pip download -r requirements.txt -d wheelhouse\n",
    "\n",
    "# # Move requrements\n",
    "# !mv requirements.txt wheelhouse/requirements.txt\n",
    "\n",
    "## Zip it up and then you can download\n",
    "# import os\n",
    "# from zipfile import ZipFile\n",
    "\n",
    "# dirName = \"./\"\n",
    "# zipName = \"packages.zip\"\n",
    "\n",
    "# # Create a ZipFile Object\n",
    "# with ZipFile(zipName, 'w') as zipObj:\n",
    "#     # Iterate over all the files in directory\n",
    "#     for folderName, subfolders, filenames in os.walk(dirName):\n",
    "#         for filename in filenames:\n",
    "#             if (filename != zipName):\n",
    "#                 # create complete filepath of file in directory\n",
    "#                 filePath = os.path.join(folderName, filename)\n",
    "#                 # Add file to zip\n",
    "#                 zipObj.write(filePath)\n",
    "# create a new dataset \n",
    "# Take that zip file and add it to a dataset + button -> new dataset -> add all you need -> use as input here\n",
    "#TURN OFF INTERNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c5aeb6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:08.401451Z",
     "iopub.status.busy": "2024-01-21T00:13:08.401111Z",
     "iopub.status.idle": "2024-01-21T00:13:09.469271Z",
     "shell.execute_reply": "2024-01-21T00:13:09.467917Z"
    },
    "papermill": {
     "duration": 1.084769,
     "end_time": "2024-01-21T00:13:09.472126",
     "exception": false,
     "start_time": "2024-01-21T00:13:08.387357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wipe before any run. test. submission errors are no fun. \n",
    "!rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd018f7",
   "metadata": {
    "papermill": {
     "duration": 0.011973,
     "end_time": "2024-01-21T00:13:09.496866",
     "exception": false,
     "start_time": "2024-01-21T00:13:09.484893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install Requirements\n",
    "\n",
    "This is for files and libraries that are not in the kaggle docker runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e09fee",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:09.523517Z",
     "iopub.status.busy": "2024-01-21T00:13:09.523087Z",
     "iopub.status.idle": "2024-01-21T00:13:17.407555Z",
     "shell.execute_reply": "2024-01-21T00:13:17.406428Z"
    },
    "papermill": {
     "duration": 7.901396,
     "end_time": "2024-01-21T00:13:17.410400",
     "exception": false,
     "start_time": "2024-01-21T00:13:09.509004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#!cp -r /kaggle/input/pip-installs/wheelhouse /kaggle/working/\n",
    "#!cp -r /kaggle/input/pip-installs/benepar_en3 /kaggle/working/\n",
    "\n",
    "# install benepar\n",
    "#!pip install --no-index --find-links=/kaggle/working/wheelhouse /kaggle/working/wheelhouse/benepar-0.2.0/benepar-0.2.0\n",
    "\n",
    "# not installing, just link it\n",
    "import sys \n",
    "sys.path.append(\"/kaggle/input/pip-installs/wheelhouse/sentence-transformers-2.2.2/sentence-transformers-2.2.2\") \n",
    "import sentence_transformers\n",
    "\n",
    "sys.path.append(\"/kaggle/input/pip-installs/wheelhouse/empath-0.89/empath-0.89\") \n",
    "from empath import Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d02a2ad",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:17.437197Z",
     "iopub.status.busy": "2024-01-21T00:13:17.436409Z",
     "iopub.status.idle": "2024-01-21T00:13:41.330409Z",
     "shell.execute_reply": "2024-01-21T00:13:41.328479Z"
    },
    "papermill": {
     "duration": 23.909781,
     "end_time": "2024-01-21T00:13:41.332966",
     "exception": false,
     "start_time": "2024-01-21T00:13:17.423185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "textstat\r\n",
      "clearml\r\n",
      "optuna\r\n",
      "interpret\r\n",
      "torchsummary\r\n",
      "Looking in links: /kaggle/input/pip-installs/wheelhouse\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/textstat-0.7.3-py3-none-any.whl (from -r /kaggle/working/requirements.txt (line 2))\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/clearml-1.14.1-py2.py3-none-any.whl (from -r /kaggle/working/requirements.txt (line 3))\r\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/requirements.txt (line 4)) (3.5.0)\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/interpret-0.5.0-py3-none-any.whl (from -r /kaggle/working/requirements.txt (line 5))\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/torchsummary-1.5.1-py3-none-any.whl (from -r /kaggle/working/requirements.txt (line 6))\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/pyphen-0.14.0-py3-none-any.whl (from textstat->-r /kaggle/working/requirements.txt (line 2))\r\n",
      "Requirement already satisfied: attrs>=18.0 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (23.1.0)\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/furl-2.1.3-py2.py3-none-any.whl (from clearml->-r /kaggle/working/requirements.txt (line 3))\r\n",
      "Requirement already satisfied: jsonschema>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (4.19.0)\r\n",
      "Requirement already satisfied: numpy>=1.10 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (1.24.3)\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/pathlib2-2.3.7.post1-py2.py3-none-any.whl (from clearml->-r /kaggle/working/requirements.txt (line 3))\r\n",
      "Requirement already satisfied: Pillow>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (9.5.0)\r\n",
      "Requirement already satisfied: psutil>=3.4.2 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (5.9.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (2.8.2)\r\n",
      "Requirement already satisfied: PyYAML>=3.12 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (6.0.1)\r\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (2.31.0)\r\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (1.16.0)\r\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (1.26.15)\r\n",
      "Requirement already satisfied: pyjwt<2.9.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (2.8.0)\r\n",
      "Requirement already satisfied: referencing<0.40 in /opt/conda/lib/python3.10/site-packages (from clearml->-r /kaggle/working/requirements.txt (line 3)) (0.30.2)\r\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna->-r /kaggle/working/requirements.txt (line 4)) (1.13.1)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->-r /kaggle/working/requirements.txt (line 4)) (6.8.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna->-r /kaggle/working/requirements.txt (line 4)) (21.3)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna->-r /kaggle/working/requirements.txt (line 4)) (2.0.20)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna->-r /kaggle/working/requirements.txt (line 4)) (4.66.1)\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/interpret_core-0.5.0-py3-none-any.whl (from interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (1.11.4)\r\n",
      "Requirement already satisfied: pandas>=0.19.2 in /opt/conda/lib/python3.10/site-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (2.0.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (1.3.2)\r\n",
      "Requirement already satisfied: plotly>=3.8.1 in /opt/conda/lib/python3.10/site-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (5.16.1)\r\n",
      "Requirement already satisfied: ipykernel>=4.10.0 in /opt/conda/lib/python3.10/site-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (6.25.1)\r\n",
      "Requirement already satisfied: ipython>=5.5.0 in /opt/conda/lib/python3.10/site-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (8.14.0)\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/dash-2.14.2-py3-none-any.whl (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/dash_core_components-2.0.0-py3-none-any.whl (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/dash_html_components-2.0.0-py3-none-any.whl (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/dash_table-5.0.0-py3-none-any.whl (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/dash_cytoscape-0.3.0-py3-none-any.whl (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/gevent-23.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/salib-1.4.7-py3-none-any.whl (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Requirement already satisfied: shap>=0.28.5 in /opt/conda/lib/python3.10/site-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.44.0)\r\n",
      "Requirement already satisfied: dill>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.3.7)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r /kaggle/working/requirements.txt (line 4)) (1.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->-r /kaggle/working/requirements.txt (line 4)) (4.5.0)\r\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from furl>=2.0.0->clearml->-r /kaggle/working/requirements.txt (line 3)) (1.0.1)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6.0->clearml->-r /kaggle/working/requirements.txt (line 3)) (2023.7.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6.0->clearml->-r /kaggle/working/requirements.txt (line 3)) (0.9.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->clearml->-r /kaggle/working/requirements.txt (line 3)) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->clearml->-r /kaggle/working/requirements.txt (line 3)) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->clearml->-r /kaggle/working/requirements.txt (line 3)) (2023.11.17)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna->-r /kaggle/working/requirements.txt (line 4)) (2.0.2)\r\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in /opt/conda/lib/python3.10/site-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (3.0.0)\r\n",
      "Requirement already satisfied: Werkzeug<3.1 in /opt/conda/lib/python3.10/site-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (3.0.1)\r\n",
      "Requirement already satisfied: retrying in /opt/conda/lib/python3.10/site-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (1.3.3)\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/ansi2html-1.9.1-py3-none-any.whl (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (1.5.6)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (68.1.2)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (6.8.0)\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/zope.event-5.0-py3-none-any.whl (from gevent>=1.3.6->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Processing /kaggle/input/pip-installs/wheelhouse/zope.interface-6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from gevent>=1.3.6->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5))\r\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.1.4)\r\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (1.6.7.post1)\r\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (7.4.9)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (5.3.1)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.1.6)\r\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (24.0.1)\r\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (6.3.3)\r\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (5.9.0)\r\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.19.0)\r\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (3.0.39)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (2.16.1)\r\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.6.2)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (4.8.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.19.2->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.19.2->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (2023.3)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=3.8.1->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (8.2.3)\r\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (3.7.4)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.70.15)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.18.1->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (3.2.0)\r\n",
      "Requirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.10/site-packages (from shap>=0.28.5->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.0.7)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from shap>=0.28.5->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.57.1)\r\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from shap>=0.28.5->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (2.2.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna->-r /kaggle/working/requirements.txt (line 4)) (2.1.3)\r\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (3.1.2)\r\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (2.1.2)\r\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (8.1.7)\r\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (1.7.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.8.3)\r\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.4)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (4.1.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.2.2->SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.2.2->SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.2.2->SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (4.42.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.2.2->SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (1.4.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.2.6)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (3.16.2)\r\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->shap>=0.28.5->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.40.1)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (1.2.0)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (2.2.1)\r\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.0->interpret->-r /kaggle/working/requirements.txt (line 5)) (0.2.2)\r\n",
      "Installing collected packages: torchsummary, dash-table, dash-html-components, dash-core-components, zope.interface, zope.event, pyphen, pathlib2, ansi2html, textstat, gevent, furl, SALib, interpret-core, dash, dash-cytoscape, clearml, interpret\r\n",
      "Successfully installed SALib-1.4.7 ansi2html-1.9.1 clearml-1.14.1 dash-2.14.2 dash-core-components-2.0.0 dash-cytoscape-0.3.0 dash-html-components-2.0.0 dash-table-5.0.0 furl-2.1.3 gevent-23.9.1 interpret-0.5.0 interpret-core-0.5.0 pathlib2-2.3.7.post1 pyphen-0.14.0 textstat-0.7.3 torchsummary-1.5.1 zope.event-5.0 zope.interface-6.1\r\n"
     ]
    }
   ],
   "source": [
    "# Creating this in realtime just in case we have to add-remove. \n",
    "requirements = \"\"\"\n",
    "textstat\n",
    "clearml\n",
    "optuna\n",
    "interpret\n",
    "torchsummary\n",
    "\"\"\"\n",
    "\n",
    "with open('/kaggle/working/requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "!cat /kaggle/working/requirements.txt\n",
    "\n",
    "## install\n",
    "!pip install -r /kaggle/working/requirements.txt --no-index --find-links /kaggle/input/pip-installs/wheelhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f66430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:41.372112Z",
     "iopub.status.busy": "2024-01-21T00:13:41.371108Z",
     "iopub.status.idle": "2024-01-21T00:13:45.060978Z",
     "shell.execute_reply": "2024-01-21T00:13:45.060127Z"
    },
    "papermill": {
     "duration": 3.711479,
     "end_time": "2024-01-21T00:13:45.063527",
     "exception": false,
     "start_time": "2024-01-21T00:13:41.352048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## Prepare Benepar\n",
    "import sys\n",
    "import spacy\n",
    "#import benepar\n",
    "import torchsummary\n",
    "# fixes ealier issue\n",
    "sys.path.insert(0, '/kaggle/working/')\n",
    "#nlp = spacy.load('en_core_web_lg') \n",
    "#nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d13dc6",
   "metadata": {
    "papermill": {
     "duration": 0.017139,
     "end_time": "2024-01-21T00:13:45.097638",
     "exception": false,
     "start_time": "2024-01-21T00:13:45.080499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f3c381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:45.182866Z",
     "iopub.status.busy": "2024-01-21T00:13:45.181815Z",
     "iopub.status.idle": "2024-01-21T00:13:48.273879Z",
     "shell.execute_reply": "2024-01-21T00:13:48.272713Z"
    },
    "papermill": {
     "duration": 3.112612,
     "end_time": "2024-01-21T00:13:48.276333",
     "exception": false,
     "start_time": "2024-01-21T00:13:45.163721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import logging\n",
    "from clearml.automation.controller import PipelineDecorator\n",
    "from clearml import TaskTypes, PipelineController, StorageManager, Dataset, Task\n",
    "from clearml import InputModel, OutputModel\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226cc72",
   "metadata": {
    "papermill": {
     "duration": 0.017499,
     "end_time": "2024-01-21T00:13:48.311160",
     "exception": false,
     "start_time": "2024-01-21T00:13:48.293661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reproduceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b7351c6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:48.348070Z",
     "iopub.status.busy": "2024-01-21T00:13:48.346923Z",
     "iopub.status.idle": "2024-01-21T00:13:48.358202Z",
     "shell.execute_reply": "2024-01-21T00:13:48.357236Z"
    },
    "papermill": {
     "duration": 0.03209,
     "end_time": "2024-01-21T00:13:48.360491",
     "exception": false,
     "start_time": "2024-01-21T00:13:48.328401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed for PyTorch\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Set random seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set random seed for random module\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a62c5",
   "metadata": {
    "papermill": {
     "duration": 0.017318,
     "end_time": "2024-01-21T00:13:48.395046",
     "exception": false,
     "start_time": "2024-01-21T00:13:48.377728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2448eaeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:48.431704Z",
     "iopub.status.busy": "2024-01-21T00:13:48.431250Z",
     "iopub.status.idle": "2024-01-21T00:13:48.440312Z",
     "shell.execute_reply": "2024-01-21T00:13:48.439231Z"
    },
    "papermill": {
     "duration": 0.030366,
     "end_time": "2024-01-21T00:13:48.442782",
     "exception": false,
     "start_time": "2024-01-21T00:13:48.412416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# CLEARML Goodies. Add your keys here\n",
    "# %env CLEARML_WEB_HOST=https://app.clear.ml  # Or your onprem deployment\n",
    "# %env CLEARML_API_HOST=https://api.clear.ml # Or your onprem deployment\n",
    "# %env CLEARML_FILES_HOST=https://files.clear.ml # Or your onprem deployment\n",
    "# %env CLEARML_API_ACCESS_KEY=\n",
    "# %env CLEARML_API_SECRET_KEY=\n",
    "\n",
    "# Open AI API Key \n",
    "# as I trust my community and peers, here is a temp key to use when running the 'Explain Code' funtionality. \n",
    "# I threw a few $$ on the key so I can use it for the project, and you can use it until the $$ run out :)\n",
    "# don't use it anywhere else please. \n",
    "# Just kiding - I was concidering that.. but it's so so insecure. \n",
    "#os.environ['OPENAI_API_KEY'] = \n",
    "\n",
    "class CFG:\n",
    "    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    CLEAR_ML_TRAINING_DATASET_ID = 'e71bc7e41b114a549ac1eaf1dff43099'  \n",
    "    CLEAR_ML_KAGGLE_TRAIN_DATA = '24596ea241c34c6eb5013152a6122e48' \n",
    "    CLEAR_ML_AI_GENERATED_ESSAYS = '593fff56e3784e4fbfa4bf82096b0127'\n",
    "    CLEAR_ML_AI_REWRITTEN_ESSAYS = '624315dd0e9b4314aa266654ebd71918'\n",
    "    \n",
    "    \n",
    "    DATA_ETL_STRATEGY = 1\n",
    "    TRAINING_DATA_COUNT = 50000\n",
    "    CLEARML_OFFLINE_MODE = False\n",
    "    CLEARML_ON = False\n",
    "    KAGGLE_INPUT = '/kaggle/input'\n",
    "    SCRATCH_PATH = '/kaggle/working'\n",
    "    ARTIFACTS_PATH = '/kaggle/working'\n",
    "    TRANSFORMERS_PATH = '/benepar'\n",
    "    ENSAMBLE_STRATEGY = 1\n",
    "    KAGGLE_RUN = True\n",
    "    SUBMISSION_RUN = True\n",
    "    EXPLAIN_CODE=False\n",
    "    BERT_MODEL = '/kaggle/input/bert-base-uncased'\n",
    "    EBM_ONLY = False\n",
    "    RETRAIN = False\n",
    "\n",
    "\n",
    "# hmm .. \n",
    "cfg_dict = {key: value for key, value in CFG.__dict__.items() if not key.startswith('__')}\n",
    "\n",
    "feature_list = list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef68bdb",
   "metadata": {
    "papermill": {
     "duration": 0.016904,
     "end_time": "2024-01-21T00:13:48.476801",
     "exception": false,
     "start_time": "2024-01-21T00:13:48.459897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logging \n",
    "\n",
    "A little logging class that integrates ClearML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f4ef19",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:48.513596Z",
     "iopub.status.busy": "2024-01-21T00:13:48.513220Z",
     "iopub.status.idle": "2024-01-21T00:13:48.541584Z",
     "shell.execute_reply": "2024-01-21T00:13:48.540414Z"
    },
    "papermill": {
     "duration": 0.050107,
     "end_time": "2024-01-21T00:13:48.544018",
     "exception": false,
     "start_time": "2024-01-21T00:13:48.493911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import clearml\n",
    "class ClearMLTaskHandler:\n",
    "    def __init__(self, project_name, task_name, config=None):\n",
    "        self.task = self.get_or_create_task(project_name, task_name)\n",
    "        self.logger = None  # Initialize logger attribute\n",
    "        self.setup_widget_logger()\n",
    "\n",
    "        if config:\n",
    "            self.set_config(config)\n",
    "\n",
    "    def get_or_create_task(self, project_name, task_name):\n",
    "        try:\n",
    "            tasks = []\n",
    "            if(CFG.CLEARML_OFFLINE_MODE):\n",
    "                Task.set_offline(offline_mode=True)\n",
    "            else:\n",
    "                tasks = Task.get_tasks(project_name=project_name, task_name=task_name)\n",
    "            \n",
    "            if tasks:\n",
    "                if(tasks[0].get_status() == \"created\" and task[0].task_name == task_name):\n",
    "                    task = tasks[0]\n",
    "                    return task\n",
    "                else:\n",
    "                    if(CFG.CLEARML_OFFLINE_MODE):\n",
    "                        Task.set_offline(offline_mode=True)\n",
    "                        \n",
    "                    task = Task.init(project_name=project_name, task_name=task_name)\n",
    "                    return task\n",
    "            else:\n",
    "                if(CFG.CLEARML_OFFLINE_MODE):\n",
    "                    Task.set_offline(offline_mode=True)\n",
    "                    task = Task.init(project_name=project_name, task_name=task_name)\n",
    "                else:\n",
    "                    task = Task.init(project_name=project_name, task_name=task_name)\n",
    "                return task\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while searching for existing task: {e}\")\n",
    "            return None\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"\n",
    "        Set hyperparameters for the task.\n",
    "        :param parameters: Dictionary of parameters to set.\n",
    "        \"\"\"\n",
    "        self.task.set_parameters(parameters)\n",
    "\n",
    "    def set_config(self, config):\n",
    "        if isinstance(config, dict):\n",
    "            self.task.connect(config)\n",
    "        elif isinstance(config, argparse.Namespace):\n",
    "            self.task.connect(config.__dict__)\n",
    "        elif isinstance(config, (InputModel, OutputModel, type, object)):\n",
    "            self.task.connect_configuration(config)\n",
    "        else:\n",
    "            logging.warning(\"Unsupported configuration type\")\n",
    "\n",
    "    def log_data(self, data, title):\n",
    "        self.task.get_logger()\n",
    "        if isinstance(data, np.ndarray):\n",
    "            self.task.get_logger().report_image(title, 'array', iteration=0, image=data)\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            self.task.get_logger().report_table(title, 'dataframe', iteration=0, table_plot=data)\n",
    "        elif isinstance(data, str) and os.path.exists(data):\n",
    "            self.task.get_logger().report_artifact(title, artifact_object=data)\n",
    "        else:\n",
    "            self.task.get_logger().report_text(f\"{title}: {data}\")\n",
    "    \n",
    "    def upload_artifact(self, name, artifact):\n",
    "        \"\"\"\n",
    "        Upload an artifact to the ClearML server.\n",
    "        :param name: Name of the artifact.\n",
    "        :param artifact: Artifact object or file path.\n",
    "        \"\"\"\n",
    "        self.task.upload_artifact(name, artifact_object=artifact)\n",
    "\n",
    "    def get_artifact(self, name):\n",
    "        \"\"\"\n",
    "        Retrieve an artifact from the ClearML server.\n",
    "        :param name: Name of the artifact to retrieve.\n",
    "        :return: Artifact object.\n",
    "        \"\"\"\n",
    "        return self.task.artifacts[name].get()\n",
    "    \n",
    "    def setup_widget_logger(self):\n",
    "            handler = OutputWidgetHandler()\n",
    "            handler.setFormatter(logging.Formatter('%(asctime)s  - [%(levelname)s] %(message)s'))\n",
    "            self.logger = logging.getLogger()  # Create a new logger instance\n",
    "            self.logger.addHandler(handler)\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Just in case we can't use clearml in kaggle\n",
    "class OutputWidgetHandler(logging.Handler):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(OutputWidgetHandler, self).__init__(*args, **kwargs)\n",
    "        layout = {'width': '100%', 'border': '1px solid black'}\n",
    "        self.out = widgets.Output(layout=layout)\n",
    "\n",
    "    def emit(self, record):\n",
    "        formatted_record = self.format(record)\n",
    "        new_output = {'name': 'stdout', 'output_type': 'stream', 'text': formatted_record+'\\n'}\n",
    "        self.out.outputs = (new_output, ) + self.out.outputs\n",
    "\n",
    "    def show_logs(self):\n",
    "        display(self.out)\n",
    "\n",
    "    def clear_logs(self):\n",
    "        self.out.clear_output()\n",
    "\n",
    "# Keeping this out for simpicity \n",
    "def upload_dataset_from_dataframe(dataframe, new_dataset_name, dataset_project, description=\"\", tags=[], file_name=\"dataset.pkl\"):\n",
    "    from pathlib import Path\n",
    "    from clearml import Dataset\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    try:\n",
    "        print(dataframe.head())\n",
    "        file_path = Path(file_name)\n",
    "        pd.to_pickle(dataframe, file_path)\n",
    "        new_dataset = Dataset.create(new_dataset_name,dataset_project, description=description)\n",
    "        new_dataset.add_files(str(file_path))\n",
    "        if description:\n",
    "            new_dataset.set_description(description)\n",
    "        if tags:\n",
    "            new_dataset.add_tags(tags)\n",
    "        new_dataset.upload()\n",
    "        new_dataset.finalize()\n",
    "        return new_dataset\n",
    "    except Exception as e:\n",
    "        return logging.error(f\"Error occurred while uploading dataset: {e}\")\n",
    "    \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b918eb",
   "metadata": {
    "papermill": {
     "duration": 0.017242,
     "end_time": "2024-01-21T00:13:48.578975",
     "exception": false,
     "start_time": "2024-01-21T00:13:48.561733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ClearML Task Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1016cd53",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:48.615329Z",
     "iopub.status.busy": "2024-01-21T00:13:48.614971Z",
     "iopub.status.idle": "2024-01-21T00:13:48.620892Z",
     "shell.execute_reply": "2024-01-21T00:13:48.619936Z"
    },
    "papermill": {
     "duration": 0.026671,
     "end_time": "2024-01-21T00:13:48.623099",
     "exception": false,
     "start_time": "2024-01-21T00:13:48.596428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(CFG.CLEARML_ON):\n",
    "    clearml_handler = ClearMLTaskHandler(\n",
    "        project_name='LLM-detect-ai-gen-text-LIVE/dev/notebook/preprocess',\n",
    "        task_name='Load Data and Generate Features'\n",
    "    )\n",
    "\n",
    "    clearml_handler.set_parameters({'etl_strategy': cfg_dict['DATA_ETL_STRATEGY'], 'train_data_count': cfg_dict['TRAINING_DATA_COUNT']})\n",
    "    clearml_handler.set_config(cfg_dict)\n",
    "    task = clearml_handler.task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6f1c6",
   "metadata": {
    "papermill": {
     "duration": 0.017207,
     "end_time": "2024-01-21T00:13:48.658156",
     "exception": false,
     "start_time": "2024-01-21T00:13:48.640949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare Training Data - Notebook Run\n",
    "<img src=\"https://mikewlange.github.io/ai-or-human/images/aiorhuman_prepare_training_data.drawio.png\" alt=\"Alt Text\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98952c3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:48.694451Z",
     "iopub.status.busy": "2024-01-21T00:13:48.693650Z",
     "iopub.status.idle": "2024-01-21T00:13:52.651626Z",
     "shell.execute_reply": "2024-01-21T00:13:52.650696Z"
    },
    "papermill": {
     "duration": 3.978803,
     "end_time": "2024-01-21T00:13:52.654316",
     "exception": false,
     "start_time": "2024-01-21T00:13:48.675513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kaggle_training_data = pd.read_csv(f'{CFG.KAGGLE_INPUT}/daigt-v2-train-dataset/train_v2_drcat_02.csv')[['text','label','source']]\n",
    "ai_generated_essays = pd.read_csv(f'{CFG.KAGGLE_INPUT}/llm-detect-gpt354-generated-and-rewritten-essays/ai_generated_essays_llm_detect_kaggle.csv')[['text','label','source']]\n",
    "random_kaggle_training_data = kaggle_training_data[kaggle_training_data['label'] == 1]#.sample(n=10000) # from kaggle dataset\n",
    "random_generated_training_data = ai_generated_essays[ai_generated_essays['label'] == 1]#.sample(n=14000) # via the essay generation pipelint gpt3.5-4 essays written by AI and rewrittn\n",
    "kaggle_training_student = kaggle_training_data[kaggle_training_data['label'] == 0]    \n",
    "argugpt = pd.read_csv('/kaggle/input/argugpt/argugpt.csv')[['text','model']]\n",
    "argugpt['label'] = 1\n",
    "argugpt = argugpt.rename(columns={'model': 'source'})\n",
    "\n",
    "# Drop rows with missing values in 'text' column for each DataFrame\n",
    "random_kaggle_training_data = random_kaggle_training_data.dropna(subset=['text'])\n",
    "random_generated_training_data = random_generated_training_data.dropna(subset=['text'])\n",
    "kaggle_training_student = kaggle_training_student.dropna(subset=['text'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "combined_data = pd.concat([random_kaggle_training_data, random_generated_training_data, kaggle_training_student,argugpt], ignore_index=True)\n",
    "\n",
    "# Reset index and drop duplicates\n",
    "df_combined = combined_data.reset_index(drop=True)\n",
    "df_combined.drop_duplicates(inplace=True)\n",
    "\n",
    "# Assuming combined_essays is a typo and should be df_combined\n",
    "df_essays = df_combined[['text', 'label', 'source']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9cceda",
   "metadata": {
    "papermill": {
     "duration": 0.017476,
     "end_time": "2024-01-21T00:13:52.689406",
     "exception": false,
     "start_time": "2024-01-21T00:13:52.671930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sample Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf7d1f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:52.727387Z",
     "iopub.status.busy": "2024-01-21T00:13:52.726534Z",
     "iopub.status.idle": "2024-01-21T00:13:53.094546Z",
     "shell.execute_reply": "2024-01-21T00:13:53.093566Z"
    },
    "papermill": {
     "duration": 0.389753,
     "end_time": "2024-01-21T00:13:53.097182",
     "exception": false,
     "start_time": "2024-01-21T00:13:52.707429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = CFG.TRAINING_DATA_COUNT/2 #int(CFG.TRAINING_DATA_COUNT / 2)\n",
    "df_label_1 = df_essays[df_essays['label'] == 1].sample(n=27200, random_state=42)\n",
    "df_label_0 = df_essays[df_essays['label'] == 0].sample(n=27200, random_state=42)\n",
    "# Concatenate the DataFrames\n",
    "combined_data = pd.concat([df_label_1, df_label_0], ignore_index=True)\n",
    "\n",
    "# Reset index and drop duplicates\n",
    "df_combined = combined_data.reset_index(drop=True)\n",
    "df_combined.drop_duplicates(inplace=True)\n",
    "\n",
    "df_essays = df_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d4949",
   "metadata": {
    "papermill": {
     "duration": 0.017193,
     "end_time": "2024-01-21T00:13:53.132277",
     "exception": false,
     "start_time": "2024-01-21T00:13:53.115084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Plot Source and Label distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08c90a55",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:53.168956Z",
     "iopub.status.busy": "2024-01-21T00:13:53.168022Z",
     "iopub.status.idle": "2024-01-21T00:13:54.033108Z",
     "shell.execute_reply": "2024-01-21T00:13:54.031973Z"
    },
    "papermill": {
     "duration": 0.885833,
     "end_time": "2024-01-21T00:13:54.035547",
     "exception": false,
     "start_time": "2024-01-21T00:13:53.149714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.25.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"611c747d-ffc7-481d-8897-07077fd0da31\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"611c747d-ffc7-481d-8897-07077fd0da31\")) {                    Plotly.newPlot(                        \"611c747d-ffc7-481d-8897-07077fd0da31\",                        [{\"name\":\"Label 0\",\"x\":[\"persuade_corpus\",\"train_essays\"],\"y\":[25833,1367],\"type\":\"bar\"},{\"name\":\"Label 1\",\"x\":[\"NousResearch\\u002fLlama-2-7b-chat-hf\",\"chat_gpt_moth\",\"cohere-command\",\"darragh_claude_v6\",\"darragh_claude_v7\",\"falcon_180b_v1\",\"gpt-3.5-turbo\",\"gpt2-xl\",\"gpt35_prompt_gen_1\",\"gpt35_prompt_gen_train_rewrite_1\",\"kingki19_palm\",\"llama2_chat\",\"llama_70b_v1\",\"mistral7binstruct_v1\",\"mistral7binstruct_v2\",\"mistralai\\u002fMistral-7B-Instruct-v0.1\",\"palm-text-bison1\",\"radek_500\",\"radekgpt4\",\"text-babbage-001\",\"text-curie-001\",\"text-davinci-001\",\"text-davinci-002\",\"text-davinci-003\",\"train_essays\"],\"y\":[295,1704,248,699,726,747,560,280,11081,990,958,1703,794,1677,1698,295,248,351,141,347,379,354,361,562,2],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Counts of Label 0 and Label 1 per Source\"},\"xaxis\":{\"title\":{\"text\":\"Source\"}},\"yaxis\":{\"title\":{\"text\":\"Count\"}},\"barmode\":\"group\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('611c747d-ffc7-481d-8897-07077fd0da31');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts:\n",
      "Label 0: 27200\n",
      "Label 1: 27200\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_label_distribution(df_essays, plots, task=None):\n",
    "    \n",
    "    if(plots == 1):\n",
    "    # Calculate the count of label 1 in df_essays\n",
    "        label_1_counts = df_essays[df_essays['label'] == 1].groupby('source').size()\n",
    "        label_0_counts = df_essays[df_essays['label'] == 0].groupby('source').size()\n",
    "        data=[\n",
    "            go.Bar(name='Label 0', x=label_0_counts.index, y=label_0_counts.values),\n",
    "            go.Bar(name='Label 1', x=label_1_counts.index, y=label_1_counts.values)\n",
    "        ]\n",
    "        # Create the bar chart using Plotly\n",
    "        fig1 = go.Figure(data=data)\n",
    "\n",
    "        # Update the layout\n",
    "        fig1.update_layout(\n",
    "            title='Counts of Label 0 and Label 1 per Source',\n",
    "            xaxis_title='Source',\n",
    "            yaxis_title='Count',\n",
    "            barmode='group'\n",
    "        )\n",
    "        if(CFG.CLEARML_ON):          \n",
    "            task.get_logger().report_plotly(title=\"Counts of Label 0 and Label 1 per Source\", series=\"data\", figure=fig1)\n",
    "            \n",
    "        # Show the chart using Plotly\n",
    "        fig1.show()\n",
    "        \n",
    "\n",
    "    # Calculate the distribution of 'label' values in df_essays\n",
    "    label_counts = df_essays['label'].value_counts().sort_index()\n",
    "\n",
    "    # Print the label counts\n",
    "    print(\"Label Counts:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"Label {label}: {count}\")\n",
    "\n",
    "    # Plot the distribution using Matplotlib\n",
    "    plt.bar(['0', '1'], label_counts.values)\n",
    "    plt.xlabel('label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of label Values in df_essays')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_distribution(df_essays,1)\n",
    "\n",
    "# Explain the code in the cell. Add this line to each cell\n",
    "if(CFG.EXPLAIN_CODE):\n",
    "    explain_code(_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072c886",
   "metadata": {
    "papermill": {
     "duration": 0.018289,
     "end_time": "2024-01-21T00:13:54.072693",
     "exception": false,
     "start_time": "2024-01-21T00:13:54.054404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Clean Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6328882",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-01-21T00:13:54.113229Z",
     "iopub.status.busy": "2024-01-21T00:13:54.112308Z",
     "iopub.status.idle": "2024-01-21T00:24:06.567707Z",
     "shell.execute_reply": "2024-01-21T00:24:06.566573Z"
    },
    "papermill": {
     "duration": 612.478579,
     "end_time": "2024-01-21T00:24:06.570599",
     "exception": false,
     "start_time": "2024-01-21T00:13:54.092020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 54400/54400 [10:12<00:00, 88.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed in 612.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def pipeline_preprocess_text(df):\n",
    "\n",
    "    PUNCTUATION_TO_RETAIN = '.?!,'\n",
    "\n",
    "    def preprocess_pipeline(text):\n",
    "        try:\n",
    "            # Remove markdown formatting\n",
    "            html = markdown.markdown(text)\n",
    "            text = BeautifulSoup(html, features=\"html.parser\").get_text()\n",
    "            # Replace newlines and remove extra whitespaces\n",
    "            text = re.sub(r'[\\n\\r]+', ' ', text)\n",
    "            text = ' '.join(text.split())\n",
    "            # Remove 'Task' prefix from the prompt\n",
    "            text = re.sub(r'^(?:Task(?:\\s*\\d+)?\\.?\\s*)?', '', text)\n",
    "            text = re.sub('\\n+', '', text)\n",
    "            text = re.sub(r'[A-Z]+_[A-Z]+', '', text)\n",
    "            # Remove punctuation except specified characters\n",
    "            punctuation_to_remove = r'[^\\w\\s' + re.escape(PUNCTUATION_TO_RETAIN) + ']'\n",
    "            text = re.sub(punctuation_to_remove, '', text)\n",
    "            # Tokenize and lemmatize text\n",
    "            tokens = word_tokenize(text)\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "            # Join tokens back to string\n",
    "            return ' '.join(lemmatized_tokens)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in preprocess_pipeline: {e}\")\n",
    "            return text\n",
    "\n",
    "    # Apply the preprocessing pipeline to each text\n",
    "    tqdm.pandas()\n",
    "    start_time = time.time()\n",
    "    df['text'] = df['text'].progress_apply(preprocess_pipeline)\n",
    "    end_time = time.time()\n",
    "    print(f\"Preprocessing completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return df\n",
    "\n",
    "#Usage\n",
    "df_essays = pipeline_preprocess_text(df_essays)\n",
    "\n",
    "if(CFG.CLEARML_ON):\n",
    "    plot_label_distribution(df_essays, 0, task=clearml_handler.task)\n",
    "    clearml_handler.task.upload_artifact(f'df_essays_train_preprocessed_{CFG.DATA_ETL_STRATEGY}', artifact_object=df_essays)\n",
    "    clearml_handler.task.get_logger().report_table(title='df_essays_train_preprocessed_',series='Train Essays Cleaned',\n",
    "                                                iteration=0,table_plot=df_essays)\n",
    "\n",
    "# Explain the code in the cell. Add this line to each cell\n",
    "if(CFG.EXPLAIN_CODE):\n",
    "    explain_code(_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "397a4c6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T00:24:07.612704Z",
     "iopub.status.busy": "2024-01-21T00:24:07.611836Z",
     "iopub.status.idle": "2024-01-21T00:24:07.624329Z",
     "shell.execute_reply": "2024-01-21T00:24:07.623497Z"
    },
    "papermill": {
     "duration": 0.536075,
     "end_time": "2024-01-21T00:24:07.626528",
     "exception": false,
     "start_time": "2024-01-21T00:24:07.090453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a deep copy so i can use the original df_essays later\n",
    "df_essays_copy = df_essays.copy(deep=True) ## for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e4349",
   "metadata": {
    "papermill": {
     "duration": 0.569671,
     "end_time": "2024-01-21T00:24:08.700861",
     "exception": false,
     "start_time": "2024-01-21T00:24:08.131190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Development\n",
    "<img src=\"https://mikewlange.github.io/ai-or-human/images/aiorhuman_develop_model.drawio.png\" alt=\"Alt Text\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be22e54",
   "metadata": {
    "papermill": {
     "duration": 0.508809,
     "end_time": "2024-01-21T00:24:09.712972",
     "exception": false,
     "start_time": "2024-01-21T00:24:09.204163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  BERT-BiLSTM Classifier Model\n",
    "\n",
    "<span style=\"display: inline-block;padding: 10px;background-color: #f4f3ee;border: 1px solid #FF1493;border-radius: 4px;margin-bottom: 10px;line-height: 1.5;color: #333;\" class=\"tip\">The BERT-BiLSTM Classifier model synergistically combines the BERT architecture with a Bidirectional Long Short-Term Memory (BiLSTM) network, enhancing the model's ability to understand context and sequence in text. This model integrates BERT's transformer layers with a BiLSTM network, a dropout layer for regularization, and a fully connected linear layer with ReLU activation, culminating in a linear classification layer.</span>\n",
    "\n",
    "## Rationale\n",
    "\n",
    "-   **BERT's Foundational Strength**: Utilizing the pre-trained BERT layers, the model leverages BERT's deep understanding of language semantics, gained from extensive training on diverse text corpora.\n",
    "-   **Sequence and Context Awareness with BiLSTM**: The addition of BiLSTM layers enables the model to capture contextual information in both forward and backward directions, making it adept at understanding the sequence and flow of the text.\n",
    "-   **Enhanced Text Processing Capabilities**: This architecture is particularly effective for complex classification tasks where understanding the context and sequence of words is crucial.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "-   **Composition**: The model is composed of the BertModel layer followed by BiLSTM layers. This is further connected to a dropout layer for regularization, a fully connected linear layer with ReLU activation, and a final linear layer for classification.\n",
    "-   **BiLSTM Configuration**: The BiLSTM layers are configured with customizable hidden sizes and layer counts, allowing the model to be adapted to different levels of sequence complexity.\n",
    "-   **Loss and Optimization**: The model employs CrossEntropyLoss for loss computation and uses the AdamW optimizer. It focuses on optimizing metrics like accuracy and AUC, with an emphasis on balancing precision and recall.\n",
    "\n",
    "## Adaptability and Use Cases\n",
    "\n",
    "-   **Versatile for Various Text Data**: Given its enhanced contextual understanding, the BERT-BiLSTM Classifier is well-suited for tasks like sentiment analysis, topic classification, and other scenarios where the sequence of text plays a significant role.\n",
    "-   **Customization and Flexibility**: The adjustable parameters of the BiLSTM layers (like hidden size and number of layers) offer flexibility, making the model adaptable to a wide range of text classification challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "880cde80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T00:24:10.733011Z",
     "iopub.status.busy": "2024-01-21T00:24:10.732088Z",
     "iopub.status.idle": "2024-01-21T00:24:10.738195Z",
     "shell.execute_reply": "2024-01-21T00:24:10.737218Z"
    },
    "papermill": {
     "duration": 0.528822,
     "end_time": "2024-01-21T00:24:10.741020",
     "exception": false,
     "start_time": "2024-01-21T00:24:10.212198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(CFG.CLEARML_ON):\n",
    "\n",
    "    clearml_bert_sequence_classifier = ClearMLTaskHandler(\n",
    "        project_name='LLM-detect-ai-gen-text-LIVE/dev/notebook/models/bert_sequence_classifier',\n",
    "        task_name='Model Trainings',\n",
    "    )\n",
    "    clearml_bert_sequence_classifier.task.auto_connect_frameworks={\n",
    "        'matplotlib': True, 'tensorflow': True, 'tensorboard': True, 'pytorch': True, 'scikit': True, \n",
    "        'hydra': True, 'detect_repository': True, 'joblib': True,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d375e6ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T00:24:11.845968Z",
     "iopub.status.busy": "2024-01-21T00:24:11.845605Z",
     "iopub.status.idle": "2024-01-21T05:35:34.532729Z",
     "shell.execute_reply": "2024-01-21T05:35:34.531607Z"
    },
    "papermill": {
     "duration": 18683.388827,
     "end_time": "2024-01-21T05:35:34.637235",
     "exception": false,
     "start_time": "2024-01-21T00:24:11.248408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 00:39:57,401] A new study created in memory with name: bert_best_custom_study\n",
      "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9723\n",
      "precision: 0.9492\n",
      "recall: 0.9983\n",
      "F1: 0.9731\n",
      "auc: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97      4063\n",
      "           1       0.95      1.00      0.97      4097\n",
      "\n",
      "    accuracy                           0.97      8160\n",
      "   macro avg       0.97      0.97      0.97      8160\n",
      "weighted avg       0.97      0.97      0.97      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  17%|        | 1/6 [16:19<1:21:37, 979.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9923\n",
      "precision: 0.9881\n",
      "recall: 0.9966\n",
      "F1: 0.9923\n",
      "auc: 0.9923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      4063\n",
      "           1       0.99      1.00      0.99      4097\n",
      "\n",
      "    accuracy                           0.99      8160\n",
      "   macro avg       0.99      0.99      0.99      8160\n",
      "weighted avg       0.99      0.99      0.99      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|     | 3/6 [49:24<49:28, 989.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9892\n",
      "precision: 0.9806\n",
      "recall: 0.9983\n",
      "F1: 0.9894\n",
      "auc: 0.9892\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      4063\n",
      "           1       0.98      1.00      0.99      4097\n",
      "\n",
      "    accuracy                           0.99      8160\n",
      "   macro avg       0.99      0.99      0.99      8160\n",
      "weighted avg       0.99      0.99      0.99      8160\n",
      "\n",
      "Validation Accuracy: 0.9950\n",
      "precision: 0.9932\n",
      "recall: 0.9968\n",
      "F1: 0.9950\n",
      "auc: 0.9950\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      4063\n",
      "           1       0.99      1.00      1.00      4097\n",
      "\n",
      "    accuracy                           0.99      8160\n",
      "   macro avg       0.99      0.99      0.99      8160\n",
      "weighted avg       0.99      0.99      0.99      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  83%| | 5/6 [1:22:29<16:31, 991.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9936\n",
      "precision: 0.9901\n",
      "recall: 0.9973\n",
      "F1: 0.9937\n",
      "auc: 0.9936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      4063\n",
      "           1       0.99      1.00      0.99      4097\n",
      "\n",
      "    accuracy                           0.99      8160\n",
      "   macro avg       0.99      0.99      0.99      8160\n",
      "weighted avg       0.99      0.99      0.99      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|| 6/6 [1:39:00<00:00, 990.03s/it]\n",
      "[I 2024-01-21 02:19:03,992] Trial 0 finished with value: 0.9949677435167891 and parameters: {'learning_rate': 2.9665565111091976e-05, 'batch_size': 16, 'dropout_rate': 0.07441494793429966, 'fc_layer_size': 128, 'lstm_hidden_size': 128, 'lstm_layers': 2}. Best is trial 0 with value: 0.9949677435167891.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9923\n",
      "precision: 0.9870\n",
      "recall: 0.9978\n",
      "F1: 0.9924\n",
      "auc: 0.9923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      4063\n",
      "           1       0.99      1.00      0.99      4097\n",
      "\n",
      "    accuracy                           0.99      8160\n",
      "   macro avg       0.99      0.99      0.99      8160\n",
      "weighted avg       0.99      0.99      0.99      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9880\n",
      "precision: 0.9805\n",
      "recall: 0.9959\n",
      "F1: 0.9881\n",
      "auc: 0.9880\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      4063\n",
      "           1       0.98      1.00      0.99      4097\n",
      "\n",
      "    accuracy                           0.99      8160\n",
      "   macro avg       0.99      0.99      0.99      8160\n",
      "weighted avg       0.99      0.99      0.99      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  17%|        | 1/6 [16:22<1:21:52, 982.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9919\n",
      "precision: 0.9934\n",
      "recall: 0.9905\n",
      "F1: 0.9919\n",
      "auc: 0.9919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4063\n",
      "           1       0.99      0.99      0.99      4097\n",
      "\n",
      "    accuracy                           0.99      8160\n",
      "   macro avg       0.99      0.99      0.99      8160\n",
      "weighted avg       0.99      0.99      0.99      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|      | 2/6 [32:45<1:05:31, 982.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9925\n",
      "precision: 0.9917\n",
      "recall: 0.9934\n",
      "F1: 0.9926\n",
      "auc: 0.9925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4063\n",
      "           1       0.99      0.99      0.99      4097\n",
      "\n",
      "    accuracy                           0.99      8160\n",
      "   macro avg       0.99      0.99      0.99      8160\n",
      "weighted avg       0.99      0.99      0.99      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|     | 3/6 [49:08<49:08, 982.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9928\n",
      "precision: 0.9884\n",
      "recall: 0.9973\n",
      "F1: 0.9928\n",
      "auc: 0.9928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      4063\n",
      "           1       0.99      1.00      0.99      4097\n",
      "\n",
      "    accuracy                           0.99      8160\n",
      "   macro avg       0.99      0.99      0.99      8160\n",
      "weighted avg       0.99      0.99      0.99      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|   | 4/6 [1:05:30<32:45, 982.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9951\n",
      "precision: 0.9954\n",
      "recall: 0.9949\n",
      "F1: 0.9951\n",
      "auc: 0.9951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      4063\n",
      "           1       1.00      0.99      1.00      4097\n",
      "\n",
      "    accuracy                           1.00      8160\n",
      "   macro avg       1.00      1.00      1.00      8160\n",
      "weighted avg       1.00      1.00      1.00      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|| 6/6 [1:38:16<00:00, 982.74s/it]\n",
      "[I 2024-01-21 03:57:21,318] Trial 1 finished with value: 0.9950989753702831 and parameters: {'learning_rate': 2.7173365666001045e-05, 'batch_size': 30, 'dropout_rate': 0.07579404291543629, 'fc_layer_size': 64, 'lstm_hidden_size': 128, 'lstm_layers': 2}. Best is trial 1 with value: 0.9950989753702831.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9924\n",
      "precision: 0.9870\n",
      "recall: 0.9980\n",
      "F1: 0.9925\n",
      "auc: 0.9924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      4063\n",
      "           1       0.99      1.00      0.99      4097\n",
      "\n",
      "    accuracy                           0.99      8160\n",
      "   macro avg       0.99      0.99      0.99      8160\n",
      "weighted avg       0.99      0.99      0.99      8160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|| 6/6 [1:38:11<00:00, 981.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      " Value: 0.9951\n",
      " Params: \n",
      " learning_rate: 2.7173365666001045e-05\n",
      " batch_size: 30\n",
      " dropout_rate: 0.07579404291543629\n",
      " fc_layer_size: 64\n",
      " lstm_hidden_size: 128\n",
      " lstm_layers: 2\n"
     ]
    }
   ],
   "source": [
    "# Required Imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "import logging\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    'bert_model_name': CFG.BERT_MODEL,\n",
    "    'num_classes': 2,\n",
    "    'max_length': 128,\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 6,\n",
    "    'train_data': df_essays_copy,\n",
    "    'num_trials': 2,\n",
    "}\n",
    "#clearml_bertmodel_custom.task.connect(model_config)\n",
    "# Load data function\n",
    "def load_data():\n",
    "    texts = model_config['train_data']['text'].str.lower().tolist()  # Lowercase for uncased BERT\n",
    "    labels = model_config['train_data']['label'].tolist()\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.encodings = tokenizer(texts, add_special_tokens=True, max_length=max_length, padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt').to(device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['label'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "class BERTBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes, dropout_rate=0.1, lstm_hidden_size=128, lstm_layers=2):\n",
    "        super(BERTBiLSTMClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.lstm = nn.LSTM(self.bert.config.hidden_size, lstm_hidden_size, lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 2, num_classes)  # *2 for bidirectional\n",
    "        self.relu = nn.ReLU()  # ReLU activation layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        lstm_output, (h_n, c_n) = self.lstm(sequence_output)\n",
    "        pooled_output = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim = 1)\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "texts, labels = load_data()\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_config['bert_model_name'])\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, model_config['max_length'])\n",
    "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, model_config['max_length'])\n",
    "test_dataset = TextClassificationDataset(test_texts, test_labels, tokenizer, model_config['max_length'])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=model_config['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=model_config['batch_size'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=model_config['batch_size'])\n",
    "\n",
    "\n",
    "run_name = f\"run_{int(time.time())}\"\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "writer = SummaryWriter(log_dir=f'{CFG.SCRATCH_PATH}/logs/bertmodel_custom/{run_name}')\n",
    "\n",
    "\n",
    "def train(model, data_loader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    leng = len(data_loader)\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / leng\n",
    "        #logger.info(f\"Epoch {epoch} - Training loss: {avg_loss}\")\n",
    "        writer.add_scalar('Training Loss', avg_loss, epoch)\n",
    "\n",
    "def evaluate(model, data_loader, device, epoch, phase='Validation'):\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "\n",
    "    _labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    precision = precision_score(actual_labels, predictions, average='binary', zero_division=1)\n",
    "    recall = recall_score(actual_labels, predictions, average='binary')\n",
    "    f1 = f1_score(actual_labels, predictions, average='binary', zero_division=1)\n",
    "    auc = roc_auc_score(actual_labels, predictions)\n",
    "    conf_matrix = confusion_matrix(actual_labels, predictions)\n",
    "\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
    "    plt.title(f'{phase} Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f'{phase}_confusion_matrix_epoch_{epoch}.png')\n",
    "    plt.close()\n",
    "\n",
    "    #logger.info(f\"Epoch {epoch} - {phase} Metrics - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "    writer.add_scalar(f'{phase} Accuracy', accuracy, epoch)\n",
    "    writer.add_scalar(f'{phase} Precision', precision, epoch)\n",
    "    writer.add_scalar(f'{phase} Recall', recall, epoch)\n",
    "    writer.add_scalar(f'{phase} F1 Score', f1, epoch)\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc, classification_report(actual_labels, predictions)\n",
    "\n",
    "\n",
    "# Optuna Hyperparameter Optimization\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters for training\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 32)\n",
    "    # Suggest hyperparameters for model architecture\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.01, 0.1)\n",
    "    fc_layer_size = trial.suggest_categorical('fc_layer_size', [64, 128])\n",
    "\n",
    "\n",
    "    lstm_hidden_size = trial.suggest_categorical('lstm_hidden_size', [64, 128])# =128,\n",
    "    lstm_layers=trial.suggest_int('lstm_layers', 2, 4)\n",
    "\n",
    "\n",
    "    #model = BERTBiLSTMClassifier(model_config['bert_model_name'],model_config['num_classes'],dropout_rate,lstm_hidden_size )\n",
    "    model = BERTBiLSTMClassifier(model_config['bert_model_name'], model_config['num_classes'], dropout_rate, fc_layer_size,lstm_layers)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    #total_steps = len(train_dataloader) / model_config['num_epochs'] / model_config['batch_size']\n",
    "    total_steps = len(train_dataloader) * model_config['num_epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    best_val_auc = 0\n",
    "    for epoch in tqdm(range(model_config['num_epochs']), desc='Epoch'):\n",
    "        train(model, train_dataloader, optimizer, scheduler, device, epoch)\n",
    "        accuracy, precision, recall, f1, auc, report = evaluate(model, val_dataloader, device, epoch)\n",
    "\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"precision: {precision:.4f}\")\n",
    "        print(f\"recall: {recall:.4f}\")\n",
    "        print(f\"F1: {f1:.4f}\")\n",
    "        print(f\"auc: {auc:.4f}\")\n",
    "        print(report)\n",
    "\n",
    "        if auc > best_val_auc:\n",
    "            best_val_auc = auc\n",
    "            best_params = {\n",
    "                'learning_rate': learning_rate,\n",
    "                'dropout_rate': dropout_rate,\n",
    "                'fc_layer_size': fc_layer_size\n",
    "            }\n",
    "            torch.save(model.state_dict(), f\"{CFG.SCRATCH_PATH}/bert_finetune_custom_{trial.number}.pt\")\n",
    "\n",
    "    torch.save(best_params, f\"{CFG.SCRATCH_PATH}/best_trial_params.json\")\n",
    "    return best_val_auc\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "bert_best_custom_study = optuna.create_study(direction='maximize', study_name='bert_best_custom_study')\n",
    "bert_best_custom_study.optimize(objective, n_trials=model_config['num_trials'])\n",
    "\n",
    "# Retrain model with best hyperparameters\n",
    "best_trial = bert_best_custom_study.best_trial\n",
    "\n",
    "#Load the model with the best trial\n",
    "best_trial_params = bert_best_custom_study.best_trial.params\n",
    "learning_rate = best_trial_params[\"learning_rate\"]\n",
    "dropout_rate = best_trial_params[\"dropout_rate\"]\n",
    "fc_layer_size = best_trial_params[\"fc_layer_size\"]\n",
    "lstm_hidden_size = best_trial_params[\"lstm_hidden_size\"]\n",
    "lstm_layers = best_trial_params[\"lstm_layers\"]\n",
    "\n",
    "# Pickle the tokenizer, study, and best model\n",
    "with open(f'{CFG.SCRATCH_PATH}/custom_bert_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open(f'{CFG.SCRATCH_PATH}/best_custom_model_study.pkl', 'wb') as f:\n",
    "    pickle.dump(bert_best_custom_study, f)\n",
    "\n",
    "\n",
    "#Initialize the best model with the optimal hyperparameters\n",
    "\n",
    "best_model = BERTBiLSTMClassifier(model_config['bert_model_name'], model_config['num_classes'], dropout_rate, fc_layer_size,lstm_layers)\n",
    "\n",
    "best_model.to(device)\n",
    "\n",
    "#Set up optimizer and scheduler for the best model\n",
    "optimizer = torch.optim.AdamW(best_model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * model_config['num_epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "#Retrain the model with the best hyperparameters\n",
    "for epoch in tqdm(range(model_config['num_epochs']), desc='Epoch'):\n",
    "    train(best_model, train_dataloader, optimizer, scheduler, device, epoch)\n",
    "    evaluate(best_model, val_dataloader, device, epoch)\n",
    "\n",
    "#Save the retrained best model\n",
    "torch.save(best_model.state_dict(), f\"{CFG.SCRATCH_PATH}/bert_bilstm_model.pt\")\n",
    "\n",
    "# For ClearML Model Registration\n",
    "# torch.jit.script(best_model).save('bert_bilstm_model.pt')\n",
    "# OutputModel().update_weights('bert_bilstm_model.pt')\n",
    "#torch.save(best_model.state_dict(), 'bert_bilstm_model.pt')\n",
    "#output_model = OutputModel(task=clearml_bertmodel_custom.task)\n",
    "#output_model.update_weights(f\"{CFG.SCRATCH_PATH}/bert_bilstm_model.pt\")\n",
    "\n",
    "#Print best trial details\n",
    "print(\"Best trial:\")\n",
    "print(f\" Value: {best_trial.value:.4f}\")\n",
    "print(\" Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\" {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8c931",
   "metadata": {
    "papermill": {
     "duration": 0.523893,
     "end_time": "2024-01-21T05:35:35.681581",
     "exception": false,
     "start_time": "2024-01-21T05:35:35.157688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference Pipeline\n",
    "<img src=\"https://mikewlange.github.io/ai-or-human/images/aiorhuman_inference.drawio.png\" width='1000px' alt=\"Inference Pipeline\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa967e",
   "metadata": {
    "papermill": {
     "duration": 0.520322,
     "end_time": "2024-01-21T05:35:36.717582",
     "exception": false,
     "start_time": "2024-01-21T05:35:36.197260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Essays for Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f4926c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T05:35:37.837230Z",
     "iopub.status.busy": "2024-01-21T05:35:37.836322Z",
     "iopub.status.idle": "2024-01-21T05:35:37.857467Z",
     "shell.execute_reply": "2024-01-21T05:35:37.856564Z"
    },
    "papermill": {
     "duration": 0.625827,
     "end_time": "2024-01-21T05:35:37.859879",
     "exception": false,
     "start_time": "2024-01-21T05:35:37.234052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "if(CFG.KAGGLE_RUN == True):\n",
    "    essays_for_inference_final = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "else:\n",
    "# OR LOAD THE CONTEST SUBMISSION DATA \n",
    "        # Get the holdoutdata from traiing. and all the train_essays from the Kaggle competition as out-of-sample data\n",
    "    essays_for_inference = pd.read_pickle('/kaggle/working/df_essays_holdout_1.pkl') # Bulk holdout data from training set \n",
    "    essays_for_inference_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv') # Kaggle competition data that we did not use in your training set\n",
    "\n",
    "    essays_for_inference_train['source'] = 'kaggle-competition'\n",
    "    # Sample Holdout to a couple g's for testing, \n",
    "    essays_for_inference_1 = essays_for_inference[essays_for_inference['label'] == 1].sample(len(essays_for_inference_train))\n",
    "    #essays_for_inference_0 = essays_for_inference[essays_for_inference['label'] == 0].sample(len(essays_for_inference_train))\n",
    "    essays_for_inference_0 = essays_for_inference_train[essays_for_inference_train['generated'] == 0]\n",
    "    # rename essays_for_inference_0.column.generated\n",
    "    essays_for_inference_0.rename(columns={'generated':'label'}, inplace=True)\n",
    "\n",
    "    essays_for_inference_cntat = pd.concat([essays_for_inference_1, essays_for_inference_0])\n",
    "    # shuffle essays_for_inference_cntat\n",
    "    essays_for_inference_final = essays_for_inference_cntat.sample(frac=1).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be7c2b5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T05:35:38.887261Z",
     "iopub.status.busy": "2024-01-21T05:35:38.886888Z",
     "iopub.status.idle": "2024-01-21T05:35:38.905828Z",
     "shell.execute_reply": "2024-01-21T05:35:38.904661Z"
    },
    "papermill": {
     "duration": 0.535112,
     "end_time": "2024-01-21T05:35:38.908018",
     "exception": false,
     "start_time": "2024-01-21T05:35:38.372906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:00<00:00, 353.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed in 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_to_classify = pipeline_preprocess_text(essays_for_inference_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2ad13",
   "metadata": {
    "papermill": {
     "duration": 0.524248,
     "end_time": "2024-01-21T05:35:39.948289",
     "exception": false,
     "start_time": "2024-01-21T05:35:39.424041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference Function BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b1b3d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T05:35:40.993236Z",
     "iopub.status.busy": "2024-01-21T05:35:40.992295Z",
     "iopub.status.idle": "2024-01-21T05:35:41.004717Z",
     "shell.execute_reply": "2024-01-21T05:35:41.003726Z"
    },
    "papermill": {
     "duration": 0.5363,
     "end_time": "2024-01-21T05:35:41.006925",
     "exception": false,
     "start_time": "2024-01-21T05:35:40.470625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def bert_inference(dataframe, model, tokenizer, max_length, device):\n",
    "    \"\"\"\n",
    "    Performs inference on a dataframe using a pre-loaded model and returns softmax probabilities.\n",
    "\n",
    "    Args:\n",
    "    - dataframe (pd.DataFrame): DataFrame containing the texts to classify.\n",
    "    - model (torch.nn.Module): Pre-loaded trained model for inference.\n",
    "    - tokenizer (transformers.PreTrainedTokenizer): Tokenizer for the model.\n",
    "    - max_length (int): Maximum sequence length for tokenization.\n",
    "    - device (torch.device): The device to run the model on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Original DataFrame with additional columns for predictions and probabilities.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "\n",
    "    for _, row in dataframe.iterrows():\n",
    "        text = row['text']\n",
    "\n",
    "        # Tokenize the text\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask)\n",
    "\n",
    "            # Check if output has 'logits' attribute\n",
    "            logits = output.logits if hasattr(output, 'logits') else output\n",
    "\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "            probabilities_ind = torch.nn.functional.softmax(probs, dim=1).cpu().numpy()[0]\n",
    "\n",
    "            # probs = softmax(logits, dim=1)\n",
    "            prediction = torch.argmax(probs, dim=1).cpu().numpy()[0]\n",
    "            # if the prediction is 0 get the softmax min value otherwise get the max value\n",
    "            if (prediction>= 1):\n",
    "                # Get the minimum value of the softmax probabilities\n",
    "                probability = max(probabilities_ind)\n",
    "            else:\n",
    "                probability = min(probabilities_ind)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "            probabilities.append(probability)\n",
    "\n",
    "    dataframe['predicted_label'] = predictions\n",
    "    dataframe['probability'] = probabilities\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "881aa299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T05:35:42.072934Z",
     "iopub.status.busy": "2024-01-21T05:35:42.071962Z",
     "iopub.status.idle": "2024-01-21T05:35:43.205399Z",
     "shell.execute_reply": "2024-01-21T05:35:43.204063Z"
    },
    "papermill": {
     "duration": 1.664422,
     "end_time": "2024-01-21T05:35:43.207886",
     "exception": false,
     "start_time": "2024-01-21T05:35:41.543464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTBiLSTMClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(768, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.07579404291543629, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    'bert_model_name': CFG.BERT_MODEL,\n",
    "    'num_classes': 2,\n",
    "    'max_length': 128,\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 4,\n",
    "    'train_data': df_essays_copy,\n",
    "    'num_trials': 2,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Pickle the tokenizer, study, and best model\n",
    "with open(f'{CFG.SCRATCH_PATH}/custom_bert_tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "with open(f'{CFG.SCRATCH_PATH}/best_custom_model_study.pkl', 'rb') as f:\n",
    "    study = pickle.load(f)\n",
    "\n",
    "\n",
    "    # Retrain model with best hyperparameters\n",
    "best_trial = study.best_trial\n",
    "\n",
    "#Load the model with the best trial\n",
    "best_trial_params = study.best_trial.params\n",
    "learning_rate = best_trial_params[\"learning_rate\"]\n",
    "dropout_rate = best_trial_params[\"dropout_rate\"]\n",
    "fc_layer_size = best_trial_params[\"fc_layer_size\"]\n",
    "lstm_hidden_size = best_trial_params[\"lstm_hidden_size\"]\n",
    "lstm_layers = best_trial_params[\"lstm_layers\"]\n",
    "\n",
    "best_model = BERTBiLSTMClassifier(model_config['bert_model_name'], model_config['num_classes'], dropout_rate, fc_layer_size,lstm_layers)\n",
    "best_model.load_state_dict(torch.load(f\"{CFG.SCRATCH_PATH}/bert_bilstm_model.pt\",map_location=device))\n",
    "#Initialize the best model with the optimal hyperparameters\n",
    "best_model.to(device)\n",
    "\n",
    "print(best_model)\n",
    "custom_bert_inference_results = bert_inference(df_to_classify, best_model, tokenizer, model_config['max_length'], device)\n",
    "\n",
    "if(CFG.KAGGLE_RUN == False):\n",
    "    ## STATISTICS\n",
    "    #Calculate the number of correct predictions\n",
    "    correct_predictions = (custom_bert_inference_results['label'] == custom_bert_inference_results['predicted_label']).sum()\n",
    "    # Calculate the total number of predictions\n",
    "    total_predictions = len(custom_bert_inference_results)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Print the statistics\n",
    "    print(f\"Total predictions: {total_predictions}\")\n",
    "    print(f\"Correct predictions: {correct_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Get the true labels and predicted labels as numpy arrays\n",
    "    true_labels = custom_bert_inference_results['label'].to_numpy()\n",
    "    predicted_labels = custom_bert_inference_results['predicted_label'].to_numpy()\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    if(CFG.CLEARML_ON):\n",
    "        clearml_bertmodel_custom.log_data(data=cm,title='Custom Bert Classifier Inference Results Confusoon Matric')\n",
    "    # Print the confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    incorrect_row_numbers = np.where(true_labels != predicted_labels)[0]\n",
    "\n",
    "    # Print the row numbers of incorrect predictions\n",
    "    print(\"Incorrect Row Numbers:\")\n",
    "    print(incorrect_row_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a1715",
   "metadata": {
    "papermill": {
     "duration": 0.610985,
     "end_time": "2024-01-21T05:35:44.338215",
     "exception": false,
     "start_time": "2024-01-21T05:35:43.727230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0c5d38f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T05:35:45.378908Z",
     "iopub.status.busy": "2024-01-21T05:35:45.378485Z",
     "iopub.status.idle": "2024-01-21T05:35:45.403290Z",
     "shell.execute_reply": "2024-01-21T05:35:45.402206Z"
    },
    "papermill": {
     "duration": 0.544067,
     "end_time": "2024-01-21T05:35:45.405643",
     "exception": false,
     "start_time": "2024-01-21T05:35:44.861576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>0.728805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>0.727913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>0.722915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  generated\n",
       "0  0000aaaa   0.728805\n",
       "1  1111bbbb   0.727913\n",
       "2  2222cccc   0.722915"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to store the submission\n",
    "#essays_for_inference_final = pd.read_csv('/Users/lange/dev/ai-or-biology/scratch/test_essays.csv')\n",
    "#if(CFG.KAGGLE_RUN == True):\n",
    "sub_df = essays_for_inference_final[[\"id\"]].copy()\n",
    "\n",
    "# Add the formatted predictions to the submission DataFrame\n",
    "\n",
    "sub_df[\"generated\"] = custom_bert_inference_results['probability']\n",
    "\n",
    "# Save Submission\n",
    "sub_df.to_csv('submission.csv',index=False)\n",
    "\n",
    "# Display the first 2 rows of the submission DataFrame\n",
    "sub_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 431504,
     "sourceId": 819665,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1455358,
     "sourceId": 2468672,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2889918,
     "sourceId": 4982782,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3946973,
     "sourceId": 6867914,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4278424,
     "sourceId": 7364771,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4300677,
     "sourceId": 7396735,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19364.79002,
   "end_time": "2024-01-21T05:35:49.254886",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-21T00:13:04.464866",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
