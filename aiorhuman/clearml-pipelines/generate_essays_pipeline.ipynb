{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClearML Pipeline - Generate Essays\n",
    "<img src=\"images/generate_essays_drawio_.png\" width='1000ps' alt=\"Alt Text\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: inline-block;padding: 10px;background-color: #f4f3ee;border: 1px solid #FF1493;border-radius: 4px;margin-bottom: 10px;line-height: 1.5;color: #333;\" class=\"tip\"><b>NOTE: The attached dataset was created during the development of this code. In the end, I would say 90% of the \n",
    "essays were created using these exact prompts.</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_generated_essays = pd.read_pickle(f'{CFG.SCRATCH_PATH}/ai_generated.csv/ai_generated.pkl')\n",
    "# ai_rewritten_essays = pd.read_pickle(f'{CFG.SCRATCH_PATH}/ai_rewritten.csv/ai_rewritten_essays.pkl')\n",
    "\n",
    "# df_comb = pd.concat([ai_generated_essays, ai_rewritten_essays], axis=0)\n",
    "# df_comb = df_comb.reset_index(drop=True)\n",
    "# print(df_comb.head())\n",
    "\n",
    "# df_comb.to_csv(f'{CFG.SCRATCH_PATH}/ai_generated_essays_llm_detect_kaggle.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install clearml -q\n",
    "%pip install nltk -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate**\n",
    "<img src=\"images/generate_test_essays_clearml.png\" width='1000ps' alt=\"Alt Text\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Framework for Generating Essays:**\n",
    "\n",
    "Quick overview. What are some ways stuents might use LLM's to conceal the origin of the essays? \n",
    "\n",
    "1. **Simple Topic Essay** \n",
    "Nothing fancy, this is for the student who just wants it done: \n",
    "\n",
    "> <span style=\"display: inline-block;padding: 5px;background-color: #f4f3ee;margin-bottom: 5px;line-height: 1.5;color: #333;\" class=\"tip\"> *Generate a quality and detailed middle or highschool essay that directly addresses the prompt:*  **+ prompt** </scpan>\n",
    "\n",
    "\n",
    "2. **Getting Creative**\n",
    "Now we need to build a prompt that will be labeled as 0 (human generated) when generated by GPT4. \n",
    "\n",
    "\n",
    "> <span style=\"display: inline-block;padding: 5px;background-color: #f4f3ee;margin-bottom: 5px;line-height: 1.5;color: #333;\" class=\"tip\"> *Generate an essay that closely resembles a high-quality, B+ level essay written by a 8th to 12th grade high-school student. The essay should reflect a deep understanding of the topic, with coherent arguments and clear structure. However, to closely mimic human writing, include subtle imperfections typical of a student at this level. These may include - Occasional grammatical errors: Introduce minor grammatical mistakes that a student might make under exam conditions or in a final draft, such as slight misuse of commas,  or occasional awkward phrasing. - Varying sentence structure: Use a mix of simple, compound, and complex sentences, with some variation in fluency to reflect a student's developing writing style. - Personal touch: Include personal opinions, anecdotes, or hypothetical examples where appropriate, to give the essay a unique voice. - Argument depth: While the essay should be well-researched and informed, the depth of argument might not reach the sophistication of a more experienced writer. Arguments should be sound but might lack the nuance a more advanced writer would include. - Conclusion: Ensure the essay has a clear conclusion, but one that might not fully encapsulate all the complexities of the topic, as a student might struggle to tie all threads together neatly. Remember, the goal is to create a piece that balances high-quality content with the authentic imperfections of a human student writer. The essay should be on the following topic:* <b>**+ prompt**</b> </span>\n",
    "\n",
    "3. **Rewrites Prompt**\n",
    "If I were to generate content for Accidamia, I would write everything, including all substantive facts, and ask for a clear rewrite. Does this constitute ai generated content if you ask to rewrite and grammar check? It's exactly API-generated content. It may not be a 'fake' essay per se, but if you send a prompt to an LLM, no matter what the prompt is, it's AI-generated text and should be labeled a 1. \n",
    "\n",
    "> <span style=\"display: inline-block;padding: 5px;background-color: #f4f3ee;margin-bottom: 5px;line-height: 1.5;color: #333;\" class=\"tip\"> *Rewrite the following student essay by enhancing its structure, vocabulary, and overall quality. It's important to keep the same tone. Do not change facts or opinons. Ensure that the content and meaning of the original essay are preserved. Keep the lengths the same within 50 words. :* **+ prompt**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display: inline-block;padding: 10px;background-color: #f4f3ee;border: 1px solid #FF1493;border-radius: 4px;margin-bottom: 10px;line-height: 1.5;color: #333;\" class=\"tip\">If you plan to execute this, input your ClearML account information or comment out the pipeline directives above each function. Note that imports are included within the functions, as ClearML constructs self-contained functions for each pipeline component.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ClearML Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "#%env CLEARML_API_ACCESS_KEY=\n",
    "#%env CLEARML_API_SECRET_KEY=\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "# ClearML - \n",
    "from clearml import Task, Dataset\n",
    "from clearml.automation.controller import PipelineDecorator\n",
    "from clearml import TaskTypes\n",
    "\n",
    "model=\"gpt-3.5-turbo-1106\",\n",
    "#model = \"gpt-4-1106-preview\",\n",
    "\n",
    "### STEP ONE ####################\n",
    "####################\n",
    "####################\n",
    "@PipelineDecorator.component(return_values=[\"unique_prompts_df\"],name='Pull Unique Prompts', cache=True, task_type=TaskTypes.data_processing)\n",
    "def process_csv_files(directory):\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    # Get a list of CSV files in the specified directory\n",
    "    csv_files = glob.glob(directory + '/*.csv')\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each CSV file\n",
    "    for file in csv_files:\n",
    "        if os.path.isfile(file):\n",
    "            # Read the CSV file and append it to the DataFrame\n",
    "            temp_df = pd.read_csv(file)\n",
    "            df = pd.concat([df, temp_df])\n",
    "\n",
    "    # Select the desired columns\n",
    "    selected_columns = ['prompt_name', 'text', 'source', 'prompt', 'fold', 'label']\n",
    "    df_selected = df[selected_columns]\n",
    "\n",
    "    # Select rows where 'prompt' is not NaN\n",
    "    selected_rows = df_selected[df_selected['prompt'].notnull()]\n",
    "    selected_rows = df_selected[df_selected['text'].notnull()]\n",
    "\n",
    "    # Group the DataFrame by 'prompt' column and keep the first occurrence of each group\n",
    "    unique_prompts_df = selected_rows.groupby('prompt').first().reset_index()[['prompt', 'source', 'text', 'label']]\n",
    "    \n",
    "    print(unique_prompts_df.columns.tolist())\n",
    "\n",
    "    return unique_prompts_df\n",
    "\n",
    "### STEP TWO ####################\n",
    "####################\n",
    "####################\n",
    "@PipelineDecorator.component(return_values=[\"sample_prompts_df\"],name='Append Instructions to Prompts', cache=True, task_type=TaskTypes.data_processing)\n",
    "def append_instructions_to_prompts(unique_prompts_df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    ## THIS IS OUR STANDARD PROMPT FOR ESSAY GENERATION\n",
    "    standard_instruction = \"Generate a quality and detailed middle or highschool essay that directly addresses the prompt: \"\n",
    "    \n",
    "    ## ths is THE PROPT CREATED TO FOOL BASELINE MODEL \n",
    "    detailed_instructions = '''Generate an essay that closely resembles a high-quality, B+ level essay written by a 8th to 12th grade \n",
    "    high-school student. The essay should reflect a deep understanding of the topic, with coherent arguments and clear structure. \n",
    "    However, to closely mimic human writing, include subtle imperfections typical of a student at this level. These may include \n",
    "    - Occasional grammatical errors: Introduce minor grammatical mistakes that a student might make under exam conditions or in a \n",
    "    final draft, such as slight misuse of commas,  or occasional awkward phrasing. \n",
    "    - Varying sentence structure: Use a mix of simple, compound, and complex sentences, with some variation in fluency to reflect a \n",
    "    student's developing writing style. \n",
    "    - Personal touch: Include personal opinions, anecdotes, or hypothetical examples where appropriate, to give the essay a unique voice. \n",
    "    - Argument depth: While the essay should be well-researched and informed, the depth of argument might not reach the sophistication of \n",
    "    a more experienced writer. Arguments should be sound but might lack the nuance a more advanced writer would include. \n",
    "    - Conclusion: Ensure the essay has a clear conclusion, but one that might not fully encapsulate all the complexities of the topic, \n",
    "    as a student might struggle to tie all threads together neatly. Remember, the goal is to create a piece that balances high-quality \n",
    "    content with the authentic imperfections of a human student writer. \n",
    "    The essay should be on the following topic:'''\n",
    "\n",
    "    rewrite_instruction = \"Rewrite the following student essay by enhancing its structure, vocabulary, and overall quality. It's important to keep the same tone. Do not change facts or opinons. Ensure that the content and meaning of the original essay are preserved. Keep the lengths the same within 50 words. : \"\n",
    "  \n",
    "    def append_instructions(df):    \n",
    "        df['standard_prompt'] = df['prompt'].apply(lambda x: standard_instruction + x)        \n",
    "        df['altered_prompt'] = df['prompt'].apply(lambda x: detailed_instructions + x) \n",
    "        df['rewrite_prompt'] = df['text'].apply(lambda x: rewrite_instruction + x )    \n",
    "        return df\n",
    "    \n",
    "    # Apply the function to your DataFrame\n",
    "    sample_prompts_df = append_instructions(unique_prompts_df)\n",
    "    print(sample_prompts_df.head())\n",
    "    # Return the updated DataFrame\n",
    "    return sample_prompts_df\n",
    "\n",
    " \n",
    "### STEP THREE ####################\n",
    "####################\n",
    "####################\n",
    "# Function to preprocess text\n",
    "@PipelineDecorator.component(return_values=[\"df_essays\"],\n",
    "                             name='Clean Text', cache=True, task_type=TaskTypes.data_processing)\n",
    "def pipeline_etl_clean_text(df):\n",
    "    import logging\n",
    "    import markdown\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import nltk\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    # Download necessary NLTK packages\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    # Ensure the necessary NLTK packages are downloaded\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while downloading NLTK packages: {e}\")\n",
    "\n",
    "    # Function to remove markdown formatting\n",
    "    def remove_markdown(text):\n",
    "        try:\n",
    "            html = markdown.markdown(text)\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "            return soup.get_text()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in remove_markdown: {e}\")\n",
    "            return text\n",
    "\n",
    "    # Function to remove 'Task' prefix from the prompt\n",
    "    def remove_task_on_prompt(text):\n",
    "        try:\n",
    "            pattern = r'^(?:Task(?:\\s*\\d+)?\\.?\\s*)?'\n",
    "            return re.sub(pattern, '', text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in remove_task_on_prompt: {e}\")\n",
    "            return text\n",
    "\n",
    "    # Function to replace newline and carriage return characters\n",
    "    def replace_newlines(text):\n",
    "        try:\n",
    "            return re.sub(r'[\\n\\r]+', ' ', text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in replace_newlines: {e}\")\n",
    "            return text\n",
    "\n",
    "    # Function to remove extra whitespaces\n",
    "    def remove_extra_whitespace(text):\n",
    "        try:\n",
    "            return ' '.join(text.split())\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in remove_extra_whitespace: {e}\")\n",
    "            return text\n",
    "\n",
    "    # Function to remove punctuation except for specified characters\n",
    "    def remove_punctuation_except(text, punctuation_to_retain):\n",
    "        try:\n",
    "            punctuation_to_remove = r'[^\\w\\s' + re.escape(punctuation_to_retain) + ']'\n",
    "            return re.sub(punctuation_to_remove, '', text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in remove_punctuation_except: {e}\")\n",
    "            return text\n",
    "\n",
    "    def remove_emojis_and_newlines(text):\n",
    "        # Regex pattern for matching emojis\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002500-\\U00002BEF\"  # chinese characters\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            u\"\\U0001f926-\\U0001f937\"\n",
    "                            u\"\\U00010000-\\U0010FFFF\"\n",
    "                            u\"\\u2640-\\u2642\"\n",
    "                            u\"\\u2600-\\u2B55\"\n",
    "                            u\"\\u200d\"\n",
    "                            u\"\\u23cf\"\n",
    "                            u\"\\u23e9\"\n",
    "                            u\"\\u231a\"\n",
    "                            u\"\\ufe0f\"  # dingbats\n",
    "                            u\"\\u3030\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        # Remove newline characters\n",
    "        text = re.sub('\\n+', ' ', text)\n",
    "        # Remove emojis\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        return text\n",
    "\n",
    "    def replace_newlines(text):\n",
    "        return re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    # Function to tokenize and lemmatize text\n",
    "    def process_text(text):\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            return lemmatized_tokens\n",
    "        except Exception as e:\n",
    "\n",
    "            logging.error(f\"Error in process_text: {e}\")\n",
    "            return []\n",
    "    # Main preprocessing logic\n",
    "    try:\n",
    "        PUNCTUATION_TO_RETAIN = '.?!,'  # Punctuation characters to retain\n",
    "        for index, row in df.iterrows():\n",
    "            text = row['prompt']\n",
    "            text = remove_markdown(text)\n",
    "            text = replace_newlines(text)\n",
    "            text = remove_extra_whitespace(text)\n",
    "            text = remove_task_on_prompt(text)\n",
    "            text = remove_punctuation_except(text, PUNCTUATION_TO_RETAIN)\n",
    "            #text = remove_emojis_and_newlines(text)\n",
    "            text = re.sub('\\n+', '', text)\n",
    "            text = re.sub(r'[A-Z]+_[A-Z]+', '', text)\n",
    "            text = replace_newlines(text)\n",
    "            # Remove occurrences of \\n\\n from the text\n",
    "            # text = text.replace('\\n\\n', '')\n",
    "            tokens = process_text(text)\n",
    "            preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "            # Update the 'preprocessed_text' column with the processed text\n",
    "            df.at[index, 'prompt'] = preprocessed_text\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.debug(e)\n",
    "\n",
    "        logging.error(f\"Error in preprocess_text: {e}\")\n",
    "        return text\n",
    "    \n",
    "### STEP FOUR ####################\n",
    "\n",
    "@PipelineDecorator.component(return_values=[\"df\"], name=\"Run Essay Generation\", cache=True, task_type=TaskTypes.data_processing)\n",
    "def run_essay_generation(df): \n",
    "    import pandas as pd\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()  # Replace with your actual API key\n",
    "\n",
    "    def generate_essay(client, prompt):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            #model = \"gpt-4-1106-preview\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \n",
    "                '''You are a high-school essay writer. The essays you genererate should score an A or a B. \n",
    "                 They are between 400 to 700 words long. Only output the essay. Nothing else. No headers. \n",
    "                 No tagging of any kind'''},\n",
    "                \n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    # Process each row to generate essays for both prompts\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Generate essay for the first prompt\n",
    "            prompt1_essay = generate_essay(client, row['standard_prompt'])\n",
    "            df.loc[index, 'standard_essay'] = prompt1_essay\n",
    "\n",
    "            # Generate essay for the second prompt\n",
    "            prompt2_essay = generate_essay(client, row['altered_prompt'])\n",
    "            df.loc[index, 'altered_essay'] = prompt2_essay\n",
    "            \n",
    "            # rewrite the student essay - Only for kaggle supplied data \n",
    "            # Generate essay for the second prompt\n",
    "            prompt3_essay = generate_essay(client, row['rewrite_prompt'])\n",
    "            df.loc[index, 'rewrite_prompt'] = prompt3_essay\n",
    "\n",
    "            print(f\"Processed essays for row {index}\")\n",
    "            #task.get_logger().report_text(f\"Processed essays for row {index}\")\n",
    "            # Save the DataFrame with the generated essays (append mode)\n",
    "            df.to_csv(\"scratch/gpt-3.5-turbo-1106.gen.iter.csv\", index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing essays for row {index}: {e}\")\n",
    "            \n",
    "    print(\"All essays processed and saved to CSV.\")\n",
    "    return df\n",
    "\n",
    "@PipelineDecorator.component(name=\"Save Generated Essays\", cache=True, task_type=TaskTypes.data_processing)\n",
    "def save_generated_essays(df, new_dataset_name, description=\"\", tags=[], file_name=\"dataset.pkl\"):\n",
    "    from pathlib import Path\n",
    "    from clearml import Dataset\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    try:\n",
    "      logging.basicConfig(level=logging.DEBUG)\n",
    "      print(df.head())\n",
    "      file_path = Path(file_name)\n",
    "      pd.to_pickle(df, file_path)\n",
    "      new_dataset = Dataset.create(dataset_project='LLM-detect-ai-gen-text-LIVE/datasets', dataset_name=new_dataset_name)\n",
    "      new_dataset.add_files(str(file_path))\n",
    "      if description:\n",
    "          new_dataset.set_description(description)\n",
    "      if tags:\n",
    "          new_dataset.add_tags(tags)\n",
    "      new_dataset.upload()\n",
    "      new_dataset.finalize()\n",
    "    except Exception as e:\n",
    "      logging.debug(e)\n",
    "\n",
    "    print(f\"New dataset '{new_dataset_name}' uploaded and finalized with description and tags.\")\n",
    "    \n",
    "@PipelineDecorator.pipeline(name=\"Generate Test Essays\", project=\"LLM-detect-ai-gen-text-LIVE/dev/generate-training-essays\")\n",
    "def executing_pipeline(directory, id=1):\n",
    "\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "    # ## STEP ONE - Get unique prompts\n",
    "    print(\"launch step one\")\n",
    "    # Usage example\n",
    "    unique_prompts = process_csv_files(directory)\n",
    "    unique_prompts = unique_prompts.sample(6)  ## FOR TESTING ONLY\n",
    "\n",
    "    # ## STEP TWO - Clean Prompts\n",
    "    print(\"launch step two\")\n",
    "    unique_prompts= pipeline_etl_clean_text(unique_prompts)\n",
    "\n",
    "    # ## Generate the props and append instructions\n",
    "    print(\"launch step three\")\n",
    "    generated_prompts = append_instructions_to_prompts(unique_prompts)\n",
    "\n",
    "    ## Run the essay generation \n",
    "    print(\"launch step four\")\n",
    "    generated_essays = run_essay_generation(generated_prompts.head()) ## Sample two \n",
    "    \n",
    "    ## Save the generated essays\n",
    "    print(\"launch step five\")\n",
    "    save_generated_essays(generated_essays, \"train_essays\", \"GPT Generated Essays for training data\", [\"training\", \"unprocessed\"], \"generated_training_essays.pkl\")\n",
    "    \n",
    "    generated_essays.to_csv(f\"data/gpt-3.5-turbo-1106-{id}.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = 'data'\n",
    "    PipelineDecorator.run_locally()\n",
    "    executing_pipeline(directory, id=55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Assistant API **(unused)**\n",
    "\n",
    "I did not use this in the end, but I love what the Assistant API is evolving into. It's The amount of potential use cases are staggering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# # Initialize OpenAI Client\n",
    "# client = OpenAI(api_key='')\n",
    "\n",
    "# # Configuration settings\n",
    "# CFG = { \n",
    "#     \"MODEL\": \"gpt-3.5-turbo-1106\",\n",
    "#     \"MAX_TOKENS\": 500  \n",
    "# }\n",
    "\n",
    "# # Function to create or retrieve an assistant\n",
    "# def create_assistant(client, model):\n",
    "#     # Create a new assistant\n",
    "#     assistant = client.beta.assistants.create(\n",
    "#         model=model,\n",
    "#         name=\"Essay Generation Assistant v.001\",\n",
    "#         description=\"An assistant for testing purposes\",\n",
    "#         instructions=\"Generate a 500 to 1000 word essay based on the prompt. The should score and A or a B\"\n",
    "#     )\n",
    "#     return assistant\n",
    "\n",
    "# # Function to post a message and retrieve the result\n",
    "# def post_message_and_get_response(client, assistant_id, prompt):\n",
    "#     # Create a new thread\n",
    "#     thread = client.beta.threads.create()\n",
    "\n",
    "#     # Send the prompt message to the thread\n",
    "#     client.beta.threads.messages.create(\n",
    "#         thread_id=thread.id,\n",
    "#         role=\"user\",\n",
    "#         content=prompt\n",
    "#     )\n",
    "\n",
    "#     # Create a run for the thread\n",
    "#     run = client.beta.threads.runs.create(\n",
    "#         thread_id=thread.id,\n",
    "#         assistant_id=assistant_id\n",
    "#     )\n",
    "\n",
    "#     # Wait for run to complete (may need to adjust based on API response time)\n",
    "#     while True:\n",
    "#         run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "#         if run_status.status in ['completed', 'failed']:\n",
    "#             break\n",
    "\n",
    "#     # Retrieve the response\n",
    "#     messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "#     response_message = next((message for message in messages.data if message.role == 'assistant'), None)\n",
    "#     return response_message.content if response_message else \"No response received.\"\n",
    "\n",
    "# # Function to generate essay based on sample prompt\n",
    "# def generate_essay(sample_prompt):\n",
    "#     # Create an assistant\n",
    "#     assistant = create_assistant(client, CFG[\"MODEL\"])\n",
    "\n",
    "#     # Post the message and get the response\n",
    "#     response = post_message_and_get_response(client, assistant.id, sample_prompt)\n",
    "\n",
    "#     return response\n",
    "\n",
    "# # Example usage\n",
    "# # Sample prompt\n",
    "# sample_prompt = 'Generate a quality and detailed middle or highschool essay that directly addresses the prompt: Research and evaluate the benefits of students attending school from home Examine how students with anxiety or depression may benefit from attending school from home Consider the impact that drama and rumors may have on student performance in a traditional school setting Analyze the potential advantages of distance learning on the student body Develop an argument in support of students attending school from home The essay should be well-structured, clear, and concise.' # Use a specific prompt string for testing\n",
    "# essay = generate_essay(sample_prompt)\n",
    "# print(\"Generated Essay:\", essay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from openai import OpenAI\n",
    "# import time\n",
    "# import logging\n",
    "\n",
    "# # Initialize logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# # Initialize OpenAI Client\n",
    "# client = OpenAI()  # Replace with your actual API key\n",
    "\n",
    "# # Configuration settings\n",
    "# CFG = {\n",
    "#     \"MODEL\": \"gpt-3.5-turbo-1106\",\n",
    "#     \"MAX_TOKENS_PER_ESSAY\": 500,\n",
    "#     \"ESSAYS_PER_RUN\": 1\n",
    "# }\n",
    "\n",
    "# # Load your DataFrame with prompts\n",
    "# df = sample_prompts_df.head(1) ## use ours from above \n",
    "\n",
    "# # print(df['standard_prompt'].iloc[0])\n",
    "\n",
    "# # Function to create or retrieve an assistant\n",
    "# def get_or_create_assistant(client, model, get_premade_assistant, assistant_id=None):\n",
    "#     if get_premade_assistant and assistant_id:\n",
    "#         # Retrieve existing assistant\n",
    "#         assistant = client.beta.assistants.retrieve(assistant_id)\n",
    "#     else:\n",
    "#         # Create a new assistant\n",
    "#         assistant = client.beta.assistants.create(\n",
    "#             model=model,\n",
    "#             name=\"Essay Generation Assistant v.001\",\n",
    "#             description=\"An assistant to generate essays\",\n",
    "#             instructions=\"Generate a midle or high scool student essay. Limit the essay to 1,000 tokens. The essay should score an A or a B\"\n",
    "#         )\n",
    "#     return assistant\n",
    "\n",
    "# # Function to create or retrieve a thread\n",
    "# def get_or_create_thread(client, get_previous_thread, thread_id=None):\n",
    "#     if get_previous_thread and thread_id:\n",
    "#         # Retrieve existing thread\n",
    "#         thread = client.beta.threads.retrieve(thread_id)\n",
    "#     else:\n",
    "#         # Create a new thread\n",
    "#         thread = client.beta.threads.create()\n",
    "#     return thread\n",
    "\n",
    "# # Set flags for whether to use premade assistant and thread\n",
    "# get_premade_assistant = False\n",
    "# get_previous_thread = False\n",
    "# assistant_id_to_use = \"\"  \n",
    "# thread_id_to_use = \"\" \n",
    "\n",
    "# # Get or create assistant\n",
    "# assistant = get_or_create_assistant(client, CFG[\"MODEL\"], get_premade_assistant, assistant_id_to_use)\n",
    "\n",
    "# # Process essays with limit based on ESSAYS_PER_RUN\n",
    "# for index, row in df.head(CFG[\"ESSAYS_PER_RUN\"]).iterrows():\n",
    "#     try:\n",
    "#         # Get or create thread\n",
    "#         thread = get_or_create_thread(client, get_previous_thread, thread_id_to_use)\n",
    "\n",
    "#         # Send standard prompt message to the thread\n",
    "#         client.beta.threads.messages.create(\n",
    "#             thread_id=thread.id,\n",
    "#             role=\"user\",\n",
    "#             content=row['standard_prompt']\n",
    "#         )\n",
    "\n",
    "#         # Wait for the response\n",
    "#         while True:\n",
    "#             messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "#             if len(messages.data) > 1:\n",
    "#                 break\n",
    "#             time.sleep(1)  # Adjust sleep time as needed\n",
    "\n",
    "#         # Retrieve and store the standard essay response\n",
    "#         standard_essay_message = next((message for message in messages.data[::-1] if message.role == 'assistant'), None)\n",
    "#         df.at[index, 'standard_essay'] = standard_essay_message.content if standard_essay_message else ''\n",
    "\n",
    "#         # Send altered prompt message to the thread\n",
    "#         client.beta.threads.messages.create(\n",
    "#             thread_id=thread.id,\n",
    "#             role=\"user\",\n",
    "#             content=row['altered_prompt']\n",
    "#         )\n",
    "\n",
    "#         # Wait for the response\n",
    "#         while True:\n",
    "#             messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "#             if len(messages.data) > 2:\n",
    "#                 break\n",
    "#             time.sleep(1)  # Adjust sleep time as needed\n",
    "\n",
    "#         # Retrieve and store the altered essay response\n",
    "#         altered_essay_message = next((message for message in messages.data[::-1] if message.role == 'assistant'), None)\n",
    "#         df.at[index, 'altered_essay'] = altered_essay_message.content if altered_essay_message else ''\n",
    "\n",
    "#         logging.info(f\"Processed essay for prompt at index {index}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error processing essay for prompt at index {index}: {e}\")\n",
    "\n",
    "# # Save DataFrame to CSV\n",
    "# df.to_csv(\"generated_essays.csv\", index=False)\n",
    "# logging.info(\"All essays processed and saved to CSV.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
