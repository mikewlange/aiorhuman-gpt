{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClearML Pipeline - Engineer Features\n",
    "\n",
    "<img src=\"https://mikewlange.github.io/ai-or-human/images/generate_features_sm.drawio.png\" alt=\"Alt Text\" >\n",
    "\n",
    "<span style=\"display: inline-block;padding: 10px;background-color: #f4f3ee;border: 1px solid #FF1493;border-radius: 4px;margin-bottom: 10px;line-height: 1.5;color: #333;\" class=\"tip\">In the <b>Feature Engineering</b> section of this notebook, we focus on extracting attributes from the essay text data to enhance our model's interpretability and performance. By engineering features meticulously, we target capturing nuanced differences in textual characteristics, such as readability, semantic density, and syntactic patterns, distinguishing between AI-generated and human-written texts.</span>\n",
    "\n",
    "We create the following:\n",
    "\n",
    "1.  **Readability Scores**: Analyzing scores like `Flesch-Kincaid Grade Level`, `Gunning Fog Index`, etc., to identify unique readability patterns in AI-generated versus human-written essays.\n",
    "    \n",
    "2.  **Semantic Density**: Examining the concentration of meaning-bearing words in texts to understand the distribution differences in AI-generated text.\n",
    "    \n",
    "3.  **Semantic Flow Variability**: Investigating idea transitions between sentences, comparing human writing's variability with AI-generated text.\n",
    "    \n",
    "4.  **Psycholinguistic Features**: Using the LIWC tool to evaluate the psychological and emotional content of the essays.\n",
    "    \n",
    "5.  **Textual Entropy**: Measuring unpredictability or randomness in text, focusing on how AI-generated content's entropy differs from human writing.\n",
    "    \n",
    "6.  **Syntactic Tree Patterns**: Parsing essays into syntactic trees to analyze pattern frequencies, particularly the structural tendencies in language models.\n",
    "\n",
    "**We work hard not to generate a discriminative content bias and only stick to mathematical features. Can the LWIC(empath) features introduce bias? Maybe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=https://app.clear.ml\n",
      "env: CLEARML_API_HOST=https://api.clear.ml\n",
      "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
      "env: CLEARML_API_ACCESS_KEY=\n",
      "env: CLEARML_API_SECRET_KEY=\n"
     ]
    }
   ],
   "source": [
    "# %pip install clearml -q\n",
    "# %pip install textstat -q\n",
    "# %pip install empath -q\n",
    "# %pip install spacy -q\n",
    "# %pip install benepar -q\n",
    "\n",
    "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=\n",
    "%env CLEARML_API_SECRET_KEY=\n",
    "\n",
    "class CFG:\n",
    "    CLEAR_ML = True\n",
    "    DATA_ETL_STRATEGY = 2\n",
    "    TRAINING_DATA_COUNT = 2000\n",
    "    DATA_SCALE_RATIO = 0.24\n",
    "    TEST_RUN = True\n",
    "    CLEARML_OFFLINE_MODE = False\n",
    "    CLEARML_ON = False\n",
    "    SCRATCH_PATH = 'scratch'\n",
    "    ARTIFACTS_PATH = 'artifacts'\n",
    "    ENSAMBLE_STRATEGY = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"scratch/features_pipeline.png\" alt=\"Alt Text\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=96b106ef90584c4c83ffb31e161904de\n",
      "2024-01-03 12:52:05,858 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/05628220ba2f4a13b629cdaf968e304d/experiments/96b106ef90584c4c83ffb31e161904de/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 12:52:08,774 - INFO - No repository found, storing script code instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML pipeline page: https://app.clear.ml/pipelines/05628220ba2f4a13b629cdaf968e304d/experiments/96b106ef90584c4c83ffb31e161904de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 12:52:14,176 - INFO - No repository found, storing script code instead\n",
      "2024-01-03 12:52:18,922 - INFO - No repository found, storing script code instead\n",
      "2024-01-03 12:52:23,633 - INFO - No repository found, storing script code instead\n",
      "2024-01-03 12:52:27,826 - INFO - No repository found, storing script code instead\n",
      "2024-01-03 12:52:32,302 - INFO - No repository found, storing script code instead\n",
      "2024-01-03 12:52:36,741 - INFO - No repository found, storing script code instead\n",
      "2024-01-03 12:52:41,825 - INFO - No repository found, storing script code instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching step [Pull Training Data]\n",
      "df_essays:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text           source  label\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Distractions while driving could lead to deaths of other civilians. There are reasons why this topic became an actual law. The law was just passed last year that is you're on your phone while driving, that is an immediate ticket. Due to your criminal record (if it's bad) you can even do jail time if you feel you do not apply to this law. The cops of our cities are doing their job by stopping people who does not follow this law.\\n\\nThe main reason why this show not be condoned is, because most of these situations a citizen could get in a car accident. Approximately 1.6 million citizens die due to being on their cell phone while operating a vehicle. Nearly 390,000 people get real serious injuries due to texting and driving. One out of four of every car accident in the United States are due to texting and driving. In my opinion I believe texting and driving is the main reason we have so many deaths every year.\\n\\nIn conclusion, texting and driving needs to be/ stay illegal. That way people will not do it so often, which could decrease the death rate each year. Texting and driving could possibly be the reason why our death rates increase over time. Especially in this day and age, because almost everything now is technology based. In addition, I believe that texting and driving is not safe for yourself or others, so technology should be the last thing you are on while behind and active vehicle. Texts, calls, emails all can wait until you are in a secure place (not behind the wheel) to respond, and also that way nobody could die or have an injury                                            persuade_corpus      0\n",
      "1                                                                                                                                                                                                                                                                                Phones & driving\\n\\nThe popularity of mobile devices has had some unintended and sometimes deadly consequences. An alarming number of traffic accidents are linked to driving while distracted, including use of mobile devices while driving, resulting in injury and loss of life. The most recent national statistics are sobering. 3,166 people were killed by distracted driving in 2017 alone, 8.5 percent of total fatalities for the year.\\n\\nApproximately nine people are killed and more than 1,000 injured daily in the United States in incidents reported as involving a distracted driver. During daylight hours across America, approximately 481,000 drivers are using cell phones while driving. Teens were the largest age group reported as distracted at the time of fatal crashes. Texting while driving and other cell phone use while driving facts and statistics show that this multitasking behind the wheel is becoming a life-threatening norm. Talking or texting while driving or checking or sending social media posts takes eyes and brains off the task of driving.\\n\\nWhile texting while driving certainly doesn't always lead to a fatality, there are a wealth of statistics that show that texting and driving is directly connected to traffic deaths. The National Safety Council reports that cell phone use leads to 1.6 million crashes on a yearly basis while texting while driving is six times more likely to cause a crash than drunk driving is. One out of every four traffic crashes that occur in the U.S. are spurred by cell phone usage, and furthermore, each day, 11 teens die as a result of texting and driving, which is a downright astounding number. Over 3,000 teens and young drivers die yearly due to texting while driving.\\n\\nAlmost every state has laws strictly prohibiting texting while driving, and those who don't often consider it a distracted driving offense. States that once allowed texting while driving have since cracked down on distracted driving. Many states also prohibit talking on the phone while driving unless you are using a hands-free device. If you are a commercial driver, the Federal Motor Carrier Safety Administration (FMCSA) prohibits all commercial drivers from using hand-held mobile devices.\\n\\nDistracted driving consists of driving while distracted in any manner. While cell phone use is often a part of it, distracted driving also encompasses many other activities, including eating, reading, drinking, talking to other passengers, looking around, adjusting location devices, adjusting the radio or any other activity that takes the driver's attention away from the task of driving. While any of these activities may be involved, texting while driving is usually considered the most serious type of distracted driving because it causes the most types of distraction.     persuade_corpus      0\n",
      "2  Today's technology has changed the way we live.\\n\\nOne of the biggest changes in the last decade has been the use of cell phones.\\n\\nThe average person uses their cell phone at least 25 times per day.\\n\\nThis includes many different types of use.\\n\\nWe use phones for things today that just 3 years ago required a computer.\\n\\nWith this new world of wander, comes some disadvantages as well as dangers.\\n\\nOne of the most common dangers of cell phones is use while operating a vehicle.\\n\\nMany states have laws in place to limit or even eliminate usage at all while driving.\\n\\nThere are many people that think making a law where you can't use your cell phone in a car at all is unnecessary. I find that to be a logical stand point for many reasons.\\n\\nThere is an argument for either side.\\n\\nI, personally, feel that the government should not interfere with such decisions. However, I do realize the argument in favor of these type laws for safety reasons.\\n\\nWith today's technology, there are many more uses for a cell phone besides just making a phone call.\\n\\nOne of those uses is for navigation. This is a very helpful technology advancement that is used by millions of people every day. Many vehicles are now equipped with bluetooth technology which allows a cell phone to communicate through your car.\\n\\nI think that using this option is a safe way to use a cell while driving.\\n\\nAs I mentioned earlier, the use of cell phones while driving can be a safety issue.\\n\\nI do realize that there have been multiple accidents, some fatal, due to the use of a phone while driving.\\n\\nI agree that distracted driving puts everyone on the road at an increased risk for accident. With the use of bluetooth and similar advances, you can do many things on a cell phone without even touching the phone.\\n\\nFor example, I can use voice to text or voice dial options to communicate with others while I am driving.\\n\\nThis is a safer option that always allows me to have both hands on the wheel.\\n\\nI do think that my generation is a little too addicted to their phones.\\n\\nAn addiction to anything can become a problem.\\n\\nI also think that having laws such as limited or zero tolerance of cell phone usage while driving can create an even bigger problem.\\n\\nBefore it was a law in GA, I feel that I was a safer driver, even when using my phone.\\n\\nAfter the law went in force, I still use my phone sometimes while I am driving.\\n\\nI now just try to hide it.\\n\\nThis creates an even bigger safety issue for me.\\n\\nI will probably always have a cell phone with me.\\n\\nI do try and limit my usage while I am driving now more than I did before GA passed the current no usage law.\\n\\nHowever, as I've said above, I don't feel like this is something that government should be involved in.\\n\\nI think if you are a safe driver then you will know when it is ok to use your phone and when it isn't.\\n\\nI understand that I am still a young driver and if I were older my opinion might be different.\\n\\nThere are good and bad sides of most laws and I don't think this one is any different.       persuade_corpus      0\n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Phones & Driving\\n\\nI am going to show my opinion about Texting or being on the phone while driving and, I show some you'll what could happened when you decide to be on the phone while driving. An I hope when you are reading this, I hope you chance your mind about texting while driving and I should thank all my reader for taking the time up to reading this essay.\\n\\nWhen you are driving a car you hear a text pop up on your phone and you want to get it so bad and you do but, when you do you stop paying attention to the road for just a minute then a car behind you hunks because you aren't driving right and that make you jump so you start driving right. The driver was just lucky that he just got hunk at or it could've binned much worst like a car accident, and someone could have got hurt. In my opinion people shouldn't not be texting while driving because it only takes a simple mistake, and someone could get hurt. I think that it was a good idea that modern cars develop cars that you can call people and talk to people on the phone while driving plus they have their full attention. When you are texting while driving you looking up and down while driving and I would suggest people should put that do not disturb on your phone or you can put it in the glove department, so you won't be interrupted while driving.\\n\\nMy conclusion to all this is drive safe because I don't want to look on the news and see anybody getting hurt because of you are on your phone. If you don't care do it to save the other people from driving and I see why they are pushing this to be outlaws so there could be less accident on the road. I hope and thank you for reading my essays.            persuade_corpus      0\n",
      "4                                                          Cell Phones\\n\\nA family decides to have a little fun on a Sunday afternoon so they all jump in the car and drive off to a shopping center. As the dad is driving he receives a text from his boss and decides to take a quick look at his phone to respond to his boss. As the dad was trying to make an innocent text, he was unaware that he ran a red light. Screams, screeches, and shatters is all that was heard as the family's car was hit by an eighteen wheeler. This scenario is a major example of what could happen if it was legal to use cell phones while operating a vehicle.\\n\\nWhen using a phone while driving a person is visually, physically, and mentally distracted from the task at hand which is operating a motor vehicle. When making an executive decision to pick up a cell phone, both hands are not on the steering wheel.\\n\\nThe United States law states that unless someone is changing gears or putting on signals, they must have both of their hands on the steering wheel at all times .While looking at the phone, people are visually distracted which causes their eyes to be focused on whatever they are looking at and not on the road . The mind is officially concentrated on what is happening on the cell phone when it should be focused on driving safely.\\n\\nMaking decisions for other people by using cell phones while driving is selfish. Why cause problems and danger just to use a cell phone for a few minutes? Many people are risking other people's lives by not taking the responsible route when it comes to the appropriate time to engage in cell phone activity.\\n\\nThere are many ways to insure that a phone cannot be distracting while concentrating on driving. One of the many things that can be done is silencing the phone. This will decrease the level of distraction when someone texts or calls. Another thing is to try to keep the phone out of reach and keep both hands on the steering wheel. There are other things as well as apps that prevent you from using your phone while a car is in motion. These are all ways to make sure that a cellular device isn't the cause of an accident.\\n\\nWhen having the privilege to drive, one of the many things people must be willing to do is drive safely by following the laws and rules set in place to help do that. The lack of obedience towards the law can cause serious harm to others. Driving while distracted by any mobile device can cause serious consequences for the driver and others on the road. Children, families, and loved ones could be affected by this simple act of insubordination. Many people in the world receive a call saying that their loved one died because they or someone else wasn't paying attention and got hit because they didn't look up in time to notice anything.\\n\\nDriving is dangerous in general, but with a phone it is like playing Russian roulette with people who didn't ask to play. We should take the high road and put the phone down while driving and if something is that important than get to a safe place to use it.                    persuade_corpus      0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 37371 entries, 0 to 37370\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    37369 non-null  object\n",
      " 1   source  37371 non-null  object\n",
      " 2   label   37371 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "df_essays:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text              source  label\n",
      "33641                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Being busy all the time or taking moments of rest, that is the question. Some people believe that staying active and always doing something is the key to success, while others argue that taking breaks and being inactive also serves a valuable purpose. In my opinion, both sides have their merits but finding a balance between activity and inactivity is the most effective way to accomplish more in life.\\n\\nThereâ€™s no denying the importance of staying active and productive. When we are constantly doing something, we can be more efficient and achieve our goals. For example, when I have a lot of homework to do, I find that focusing on my tasks and staying dedicated to completing them helps me finish everything on time. Additionally, many successful people, such as entrepreneurs and athletes, emphasize the value of hard work and constant effort in achieving their goals. These examples illustrate that staying active and engaged can lead to positive outcomes.\\n\\nOn the other hand, embracing moments of inactivity and rest is also essential for overall well-being. Taking breaks allows us to recharge our energy and refocus our minds. I personally find that when I take short breaks while studying, I come back more refreshed and able to concentrate better. Research has also shown that taking regular breaks can improve productivity and prevent burnout. For instance, a study published by the Harvard Business Review found that employees who took short breaks throughout the day were more efficient and less stressed than those who worked continuously. This evidence highlights the value of inactivity in maintaining a healthy work-life balance.\\n\\nIn reality, both activity and inactivity have their place in our lives, and finding a balance between the two is crucial. For instance, in the world of sports, athletes engage in rigorous training and activity to improve their skills, but they also recognize the importance of rest and recovery to prevent injuries and maintain peak performance. Similarly, many successful professionals incorporate breaks and moments of relaxation into their busy schedules to avoid burnout and maintain a healthy mindset.\\n\\nIn conclusion, while both activity and inactivity have their own benefits, the key to accomplishing more in life lies in finding a harmonious balance between the two. By staying active and productive when necessary, and allowing ourselves moments of rest and relaxation, we can achieve our goals while also preserving our well-being. It's important to recognize that taking the time to rest and recharge is just as important as working hard. So, let's strive for a healthy balance between doing something and taking a break.  gpt35_prompt_gen_1      1\n",
      "27231  Self-improvement is a popular and important pursuit for many individuals. People strive to better themselves in various aspects of their lives, such as their personal development, career success, and relationships. When it comes to self-improvement, there are different methods that people employ, and two common approaches are improvising and following a script. In this essay, we will discuss and compare these two methods and their effectiveness in helping individuals become better versions of themselves.\\n\\nImprovising is the act of making something up on the spot, without prior preparation or a set plan. In the context of self-improvement, improvisation involves being spontaneous and adaptive in one's approach to personal growth. An example of improvising in self-improvement is when a person faces a challenging situation and responds by using their instincts and creativity to navigate through it. For instance, when someone is trying to overcome a fear of public speaking, they may improvise their speech rather than strictly following a written script. This can help them to convey their thoughts and emotions more authentically, which can lead to a greater sense of personal growth and confidence.\\n\\nOn the other hand, following a script in the context of self-improvement involves adhering to a preplanned set of actions or behaviors designed to bring about positive change. This method often includes following a set of rules or guidelines that have been proven to be effective in achieving a certain goal. An example of following a script in self-improvement is when an individual commits to a specific diet plan or exercise routine in order to improve their physical health. By strictly following the prescribed steps, they can track their progress and make steady improvements in their overall well-being.\\n\\nIn comparing improvising versus following a script in self-improvement, it is essential to consider the benefits and drawbacks of each approach. Improvising allows individuals to be flexible and adapt to unexpected circumstances, which can lead to a more organic and personalized journey of self-improvement. It encourages creativity and spontaneity, which can be empowering for many people. However, the downside of improvising is that it may lack structure and consistency, which are important elements in achieving long-term growth and development.\\n\\nOn the other hand, following a script provides a clear roadmap for self-improvement, offering guidance and direction to individuals who may be unsure of where to start or how to progress. It provides a sense of security and stability, as individuals can rely on proven methods and strategies to make positive changes in their lives. However, the downside of following a script is that it may limit individuality and creativity, potentially leading to a sense of rigidity and constraint in one's personal growth journey.\\n\\nIn my own experience, I have found that a combination of improvising and following a script has been most effective in my pursuit of self-improvement. For instance, when I wanted to improve my public speaking skills, I initially followed a script by practicing specific speech techniques and structures. However, when it came to delivering the actual speech, I allowed myself to improvise and express my thoughts in a more natural and spontaneous manner. This hybrid approach allowed me to benefit from the structure of following a script while also embracing the authenticity of improvisation.\\n\\nIn conclusion, both improvising and following a script offer unique benefits and drawbacks in the pursuit of self-improvement. While improvising encourages creativity and adaptability, following a script provides structure and guidance. Ultimately, a balanced approach that incorporates elements of both methods may be the most effective in supporting individuals in becoming the best version of themselves. By recognizing the strengths of each approach and understanding how to leverage them in different contexts, individuals can tailor their self-improvement journey to suit their unique needs and goals.  gpt35_prompt_gen_1      1\n",
      "31761                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Ralph Waldo Emerson, a prominent philosopher of the 19th century, once stated, \"In this world, be yourself.\" This simple yet profound statement holds great significance, as it highlights the value of individuality and authenticity. Emerson's advice encourages people to embrace their unique qualities and beliefs, rather than conforming to societal expectations. By heeding this guidance and being confident in oneself, individuals can achieve success through patience, hard work, and the development of various skills. In essence, following Emerson's advice can lead to greatness by empowering individuals to pursue their aspirations with resilience and determination.\\n\\nEmbracing Emerson's advice requires the cultivation of self-confidence. The implications of being confident in oneself are substantial, as confidence serves as a driving force that propels individuals towards success. Confidence enables people to overcome obstacles and setbacks, allowing them to persevere in the face of adversity. For instance, consider the story of Thomas Edison, who faced numerous failures before successfully inventing the electric lightbulb. Edison's unwavering confidence in his abilities fueled his perseverance and ultimately led to his groundbreaking invention, demonstrating the power of self-assuredness in achieving success.\\n\\nMoreover, patience and hard work are integral components of realizing one's aspirations. Take, for example, the journey of Marie Curie, the first woman to win a Nobel Prize. Curie's groundbreaking research in the field of radioactivity required years of dedicated effort and unwavering patience. Her perseverance and diligent work ethic ultimately yielded significant scientific discoveries, showcasing that success often stems from sustained commitment and perseverance.\\n\\nFurthermore, possessing various skills is essential in navigating life's challenges and achieving one's goals. Individuals who are versatile and adept in multiple areas are better equipped to adapt to changing circumstances and pursue diverse opportunities. For instance, consider the success of Elon Musk, a visionary entrepreneur who has revolutionized the automotive and space industries. Musk's proficiency in engineering, business, and innovation has enabled him to establish groundbreaking companies such as Tesla and SpaceX, highlighting the importance of versatility and skill diversity in accomplishing extraordinary feats.\\n\\nWhen individuals follow Emerson's advice and embody authenticity, confidence, perseverance, and skill diversity, they can experience the unparalleled feeling of accomplishment after achieving success. This sense of achievement is deeply gratifying, as it reflects the culmination of one's efforts, determination, and unwavering belief in oneself. It serves as a testament to the transformative power of embracing individuality and striving for greatness.\\n\\nIn conclusion, Ralph Waldo Emerson's timeless advice to \"be yourself\" resonates as a guiding principle for achieving greatness. By fostering self-confidence, exercising patience and hard work, developing diverse skills, and pursuing authenticity, individuals can navigate the path to success with unwavering determination and resilience. As exemplified by the stories of influential figures throughout history, embracing Emerson's counsel can lead individuals to realize their full potential and leave an indelible mark on the world.  gpt35_prompt_gen_1      1\n",
      "31562                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The Effects of Lack of Sleep and the Case for a 10pm Curfew for Teenagers\\n\\nThe teenage years are a time of transition and discovery, where adolescents begin to establish their independence while still relying on parental guidance and structure. One crucial aspect of this delicate balancing act involves managing sleep schedules and curfews, as they directly impact teenagers' physical and emotional well-being.\\n\\nFirst and foremost, let's examine the effects of lack of sleep on teenagers. Numerous studies have shown that sleep deprivation can have a significant impact on adolescents. The teenage brain is still developing, and adequate sleep is crucial for cognitive function, emotional regulation, and overall health. Lack of sleep can lead to decreased academic performance, impaired decision-making, increased risk-taking behavior, and heightened susceptibility to mental health issues such as anxiety and depression. Additionally, sleep deprivation can compromise the immune system, making teenagers more susceptible to illness and affecting their physical growth and development.\\n\\nIn light of these potential risks, the implementation of a 10pm curfew for teenagers should be considered. Such a curfew would provide a clear guideline for teenagers, ensuring that they have a set time to wind down and prioritize their sleep. A 10pm curfew would also align with recommendations from pediatricians and sleep experts, who advocate for adolescents to have at least 8-10 hours of sleep per night. By establishing a curfew, parents and guardians can play a proactive role in promoting healthy sleep habits and recognizing the importance of rest for their teenagers' overall well-being.\\n\\nIt's essential to address potential counter-arguments against a 10pm curfew, such as the idea that staying home and having parents exercise more control over their children could be equally effective. While parental supervision is undoubtedly important, a 10pm curfew can serve as an additional support system, providing teenagers with clear boundaries and promoting autonomy and responsibility. Moreover, a curfew can help mitigate the influence of external factors, such as peer pressure and societal expectations, which can contribute to sleep deprivation and risky behavior.\\n\\nWhen evaluating the pros and cons of a 10pm curfew for teenagers, it's evident that the benefits outweigh the potential drawbacks. On the positive side, a curfew can enhance teenagers' safety by reducing their exposure to dangerous or unsupervised activities during late hours. At the same time, the establishment of a curfew can support teenagers in prioritizing their well-being, ensuring that they have the opportunity to rest and recharge, ultimately contributing to their overall health and development.\\n\\nIn conclusion, the effects of lack of sleep on teenagers underscore the importance of implementing a 10pm curfew as a supportive guideline for adolescents. By prioritizing rest and safety, a curfew can contribute to teenagers' physical and emotional well-being, providing them with the foundation to thrive during this critical stage of their lives.  gpt35_prompt_gen_1      1\n",
      "32105                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Humans are creatures of habit. We often find comfort in the familiar, sticking to what we already know and do well. But is this really the best approach to personal growth and development? The statement, \"Unless you try to do something beyond what you have already mastered, you will never grow,\" raises an important question about the nature of growth and the role of stepping beyond our comfort zones. In this essay, I will analyze different reasons why people may agree or disagree with this statement, and then provide my own opinion on the matter.\\n\\nThere are compelling arguments on both sides of this issue. Those who agree with the statement often point to the idea that growth only occurs when we push ourselves beyond our current limits. They argue that trying new things, taking on challenges, and venturing into the unknown are essential for personal development. Without these experiences, individuals may become stagnant, stuck in a cycle of repetition that inhibits their potential for growth.\\n\\nOn the other hand, there are those who disagree with the statement, believing that focusing on what one has already mastered is the key to success. They argue that honing existing skills and becoming an expert in a particular field is more important than constantly seeking out new challenges. This perspective emphasizes the value of specialization and mastery, suggesting that becoming a master in one area is more valuable than being a jack-of-all-trades.\\n\\nPersonally, I find myself agreeing with the statement that unless one tries to do something beyond what they have already mastered, they will never grow. This is not to say that mastering a skill or a subject is not important, but rather that true growth and development come from pushing beyond the boundaries of what is already known and comfortable. When we step outside of our comfort zones, we are forced to confront new challenges, learn new skills and adapt to different situations. This process of adaptation and learning is essential for personal growth.\\n\\nFrom a personal perspective, I know that some of the most profound moments of growth in my life have come from venturing beyond what I was already comfortable with. For example, when I decided to join the school debate team, I had no experience in public speaking and was extremely nervous about speaking in front of an audience. However, by challenging myself to go beyond my comfort zone, I not only improved my public speaking skills but also gained confidence and developed a new passion for debating.\\n\\nWhile it is important to acknowledge the value of mastering existing skills, it is equally important to recognize the limitations of staying within the boundaries of our comfort zones. By constantly seeking out new challenges, we open ourselves up to new experiences and opportunities for growth. In a rapidly changing world, the ability to adapt and learn new skills is increasingly important, and this can only be achieved by venturing beyond what we have already mastered.\\n\\nIn conclusion, the statement \"Unless you try to do something beyond what you have already mastered, you will never grow\" holds true in my opinion. While there are valid arguments on both sides of this issue, the value of stepping beyond our comfort zones and embracing new challenges cannot be understated. Personal growth and development come from taking risks, learning new skills, and pushing the boundaries of what we already know. It is only by doing so that we can reach our full potential and continue to thrive in an ever-changing world.  gpt35_prompt_gen_1      1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, 33641 to 4465\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    1000 non-null   object\n",
      " 1   source  1000 non-null   object\n",
      " 2   label   1000 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 31.2+ KB\n",
      "None\n",
      "df_essays:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text              source  label\n",
      "33641                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Being busy all the time or taking moments of rest, that is the question. Some people believe that staying active and always doing something is the key to success, while others argue that taking breaks and being inactive also serves a valuable purpose. In my opinion, both sides have their merits but finding a balance between activity and inactivity is the most effective way to accomplish more in life.\\n\\nThereâ€™s no denying the importance of staying active and productive. When we are constantly doing something, we can be more efficient and achieve our goals. For example, when I have a lot of homework to do, I find that focusing on my tasks and staying dedicated to completing them helps me finish everything on time. Additionally, many successful people, such as entrepreneurs and athletes, emphasize the value of hard work and constant effort in achieving their goals. These examples illustrate that staying active and engaged can lead to positive outcomes.\\n\\nOn the other hand, embracing moments of inactivity and rest is also essential for overall well-being. Taking breaks allows us to recharge our energy and refocus our minds. I personally find that when I take short breaks while studying, I come back more refreshed and able to concentrate better. Research has also shown that taking regular breaks can improve productivity and prevent burnout. For instance, a study published by the Harvard Business Review found that employees who took short breaks throughout the day were more efficient and less stressed than those who worked continuously. This evidence highlights the value of inactivity in maintaining a healthy work-life balance.\\n\\nIn reality, both activity and inactivity have their place in our lives, and finding a balance between the two is crucial. For instance, in the world of sports, athletes engage in rigorous training and activity to improve their skills, but they also recognize the importance of rest and recovery to prevent injuries and maintain peak performance. Similarly, many successful professionals incorporate breaks and moments of relaxation into their busy schedules to avoid burnout and maintain a healthy mindset.\\n\\nIn conclusion, while both activity and inactivity have their own benefits, the key to accomplishing more in life lies in finding a harmonious balance between the two. By staying active and productive when necessary, and allowing ourselves moments of rest and relaxation, we can achieve our goals while also preserving our well-being. It's important to recognize that taking the time to rest and recharge is just as important as working hard. So, let's strive for a healthy balance between doing something and taking a break.  gpt35_prompt_gen_1      1\n",
      "27231  Self-improvement is a popular and important pursuit for many individuals. People strive to better themselves in various aspects of their lives, such as their personal development, career success, and relationships. When it comes to self-improvement, there are different methods that people employ, and two common approaches are improvising and following a script. In this essay, we will discuss and compare these two methods and their effectiveness in helping individuals become better versions of themselves.\\n\\nImprovising is the act of making something up on the spot, without prior preparation or a set plan. In the context of self-improvement, improvisation involves being spontaneous and adaptive in one's approach to personal growth. An example of improvising in self-improvement is when a person faces a challenging situation and responds by using their instincts and creativity to navigate through it. For instance, when someone is trying to overcome a fear of public speaking, they may improvise their speech rather than strictly following a written script. This can help them to convey their thoughts and emotions more authentically, which can lead to a greater sense of personal growth and confidence.\\n\\nOn the other hand, following a script in the context of self-improvement involves adhering to a preplanned set of actions or behaviors designed to bring about positive change. This method often includes following a set of rules or guidelines that have been proven to be effective in achieving a certain goal. An example of following a script in self-improvement is when an individual commits to a specific diet plan or exercise routine in order to improve their physical health. By strictly following the prescribed steps, they can track their progress and make steady improvements in their overall well-being.\\n\\nIn comparing improvising versus following a script in self-improvement, it is essential to consider the benefits and drawbacks of each approach. Improvising allows individuals to be flexible and adapt to unexpected circumstances, which can lead to a more organic and personalized journey of self-improvement. It encourages creativity and spontaneity, which can be empowering for many people. However, the downside of improvising is that it may lack structure and consistency, which are important elements in achieving long-term growth and development.\\n\\nOn the other hand, following a script provides a clear roadmap for self-improvement, offering guidance and direction to individuals who may be unsure of where to start or how to progress. It provides a sense of security and stability, as individuals can rely on proven methods and strategies to make positive changes in their lives. However, the downside of following a script is that it may limit individuality and creativity, potentially leading to a sense of rigidity and constraint in one's personal growth journey.\\n\\nIn my own experience, I have found that a combination of improvising and following a script has been most effective in my pursuit of self-improvement. For instance, when I wanted to improve my public speaking skills, I initially followed a script by practicing specific speech techniques and structures. However, when it came to delivering the actual speech, I allowed myself to improvise and express my thoughts in a more natural and spontaneous manner. This hybrid approach allowed me to benefit from the structure of following a script while also embracing the authenticity of improvisation.\\n\\nIn conclusion, both improvising and following a script offer unique benefits and drawbacks in the pursuit of self-improvement. While improvising encourages creativity and adaptability, following a script provides structure and guidance. Ultimately, a balanced approach that incorporates elements of both methods may be the most effective in supporting individuals in becoming the best version of themselves. By recognizing the strengths of each approach and understanding how to leverage them in different contexts, individuals can tailor their self-improvement journey to suit their unique needs and goals.  gpt35_prompt_gen_1      1\n",
      "31761                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Ralph Waldo Emerson, a prominent philosopher of the 19th century, once stated, \"In this world, be yourself.\" This simple yet profound statement holds great significance, as it highlights the value of individuality and authenticity. Emerson's advice encourages people to embrace their unique qualities and beliefs, rather than conforming to societal expectations. By heeding this guidance and being confident in oneself, individuals can achieve success through patience, hard work, and the development of various skills. In essence, following Emerson's advice can lead to greatness by empowering individuals to pursue their aspirations with resilience and determination.\\n\\nEmbracing Emerson's advice requires the cultivation of self-confidence. The implications of being confident in oneself are substantial, as confidence serves as a driving force that propels individuals towards success. Confidence enables people to overcome obstacles and setbacks, allowing them to persevere in the face of adversity. For instance, consider the story of Thomas Edison, who faced numerous failures before successfully inventing the electric lightbulb. Edison's unwavering confidence in his abilities fueled his perseverance and ultimately led to his groundbreaking invention, demonstrating the power of self-assuredness in achieving success.\\n\\nMoreover, patience and hard work are integral components of realizing one's aspirations. Take, for example, the journey of Marie Curie, the first woman to win a Nobel Prize. Curie's groundbreaking research in the field of radioactivity required years of dedicated effort and unwavering patience. Her perseverance and diligent work ethic ultimately yielded significant scientific discoveries, showcasing that success often stems from sustained commitment and perseverance.\\n\\nFurthermore, possessing various skills is essential in navigating life's challenges and achieving one's goals. Individuals who are versatile and adept in multiple areas are better equipped to adapt to changing circumstances and pursue diverse opportunities. For instance, consider the success of Elon Musk, a visionary entrepreneur who has revolutionized the automotive and space industries. Musk's proficiency in engineering, business, and innovation has enabled him to establish groundbreaking companies such as Tesla and SpaceX, highlighting the importance of versatility and skill diversity in accomplishing extraordinary feats.\\n\\nWhen individuals follow Emerson's advice and embody authenticity, confidence, perseverance, and skill diversity, they can experience the unparalleled feeling of accomplishment after achieving success. This sense of achievement is deeply gratifying, as it reflects the culmination of one's efforts, determination, and unwavering belief in oneself. It serves as a testament to the transformative power of embracing individuality and striving for greatness.\\n\\nIn conclusion, Ralph Waldo Emerson's timeless advice to \"be yourself\" resonates as a guiding principle for achieving greatness. By fostering self-confidence, exercising patience and hard work, developing diverse skills, and pursuing authenticity, individuals can navigate the path to success with unwavering determination and resilience. As exemplified by the stories of influential figures throughout history, embracing Emerson's counsel can lead individuals to realize their full potential and leave an indelible mark on the world.  gpt35_prompt_gen_1      1\n",
      "31562                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The Effects of Lack of Sleep and the Case for a 10pm Curfew for Teenagers\\n\\nThe teenage years are a time of transition and discovery, where adolescents begin to establish their independence while still relying on parental guidance and structure. One crucial aspect of this delicate balancing act involves managing sleep schedules and curfews, as they directly impact teenagers' physical and emotional well-being.\\n\\nFirst and foremost, let's examine the effects of lack of sleep on teenagers. Numerous studies have shown that sleep deprivation can have a significant impact on adolescents. The teenage brain is still developing, and adequate sleep is crucial for cognitive function, emotional regulation, and overall health. Lack of sleep can lead to decreased academic performance, impaired decision-making, increased risk-taking behavior, and heightened susceptibility to mental health issues such as anxiety and depression. Additionally, sleep deprivation can compromise the immune system, making teenagers more susceptible to illness and affecting their physical growth and development.\\n\\nIn light of these potential risks, the implementation of a 10pm curfew for teenagers should be considered. Such a curfew would provide a clear guideline for teenagers, ensuring that they have a set time to wind down and prioritize their sleep. A 10pm curfew would also align with recommendations from pediatricians and sleep experts, who advocate for adolescents to have at least 8-10 hours of sleep per night. By establishing a curfew, parents and guardians can play a proactive role in promoting healthy sleep habits and recognizing the importance of rest for their teenagers' overall well-being.\\n\\nIt's essential to address potential counter-arguments against a 10pm curfew, such as the idea that staying home and having parents exercise more control over their children could be equally effective. While parental supervision is undoubtedly important, a 10pm curfew can serve as an additional support system, providing teenagers with clear boundaries and promoting autonomy and responsibility. Moreover, a curfew can help mitigate the influence of external factors, such as peer pressure and societal expectations, which can contribute to sleep deprivation and risky behavior.\\n\\nWhen evaluating the pros and cons of a 10pm curfew for teenagers, it's evident that the benefits outweigh the potential drawbacks. On the positive side, a curfew can enhance teenagers' safety by reducing their exposure to dangerous or unsupervised activities during late hours. At the same time, the establishment of a curfew can support teenagers in prioritizing their well-being, ensuring that they have the opportunity to rest and recharge, ultimately contributing to their overall health and development.\\n\\nIn conclusion, the effects of lack of sleep on teenagers underscore the importance of implementing a 10pm curfew as a supportive guideline for adolescents. By prioritizing rest and safety, a curfew can contribute to teenagers' physical and emotional well-being, providing them with the foundation to thrive during this critical stage of their lives.  gpt35_prompt_gen_1      1\n",
      "32105                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Humans are creatures of habit. We often find comfort in the familiar, sticking to what we already know and do well. But is this really the best approach to personal growth and development? The statement, \"Unless you try to do something beyond what you have already mastered, you will never grow,\" raises an important question about the nature of growth and the role of stepping beyond our comfort zones. In this essay, I will analyze different reasons why people may agree or disagree with this statement, and then provide my own opinion on the matter.\\n\\nThere are compelling arguments on both sides of this issue. Those who agree with the statement often point to the idea that growth only occurs when we push ourselves beyond our current limits. They argue that trying new things, taking on challenges, and venturing into the unknown are essential for personal development. Without these experiences, individuals may become stagnant, stuck in a cycle of repetition that inhibits their potential for growth.\\n\\nOn the other hand, there are those who disagree with the statement, believing that focusing on what one has already mastered is the key to success. They argue that honing existing skills and becoming an expert in a particular field is more important than constantly seeking out new challenges. This perspective emphasizes the value of specialization and mastery, suggesting that becoming a master in one area is more valuable than being a jack-of-all-trades.\\n\\nPersonally, I find myself agreeing with the statement that unless one tries to do something beyond what they have already mastered, they will never grow. This is not to say that mastering a skill or a subject is not important, but rather that true growth and development come from pushing beyond the boundaries of what is already known and comfortable. When we step outside of our comfort zones, we are forced to confront new challenges, learn new skills and adapt to different situations. This process of adaptation and learning is essential for personal growth.\\n\\nFrom a personal perspective, I know that some of the most profound moments of growth in my life have come from venturing beyond what I was already comfortable with. For example, when I decided to join the school debate team, I had no experience in public speaking and was extremely nervous about speaking in front of an audience. However, by challenging myself to go beyond my comfort zone, I not only improved my public speaking skills but also gained confidence and developed a new passion for debating.\\n\\nWhile it is important to acknowledge the value of mastering existing skills, it is equally important to recognize the limitations of staying within the boundaries of our comfort zones. By constantly seeking out new challenges, we open ourselves up to new experiences and opportunities for growth. In a rapidly changing world, the ability to adapt and learn new skills is increasingly important, and this can only be achieved by venturing beyond what we have already mastered.\\n\\nIn conclusion, the statement \"Unless you try to do something beyond what you have already mastered, you will never grow\" holds true in my opinion. While there are valid arguments on both sides of this issue, the value of stepping beyond our comfort zones and embracing new challenges cannot be understated. Personal growth and development come from taking risks, learning new skills, and pushing the boundaries of what we already know. It is only by doing so that we can reach our full potential and continue to thrive in an ever-changing world.  gpt35_prompt_gen_1      1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, 33641 to 4465\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    1000 non-null   object\n",
      " 1   source  1000 non-null   object\n",
      " 2   label   1000 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 31.2+ KB\n",
      "None\n",
      "Launching step [Readability Scores - Features Pipeline]\n",
      "Launching step [Semantic Density - Features Pipeline]\n",
      "Launching step [Semantic Flow Variability - Features Pipeline]\n",
      "Launching step [Psycholinguistic Features - Features Pipeline]\n",
      "Launching step [Textual Entropy - Features Pipeline]\n",
      "Launching step [Syntactic Tree Patterns - Features Pipeline]\n",
      "ClearML results page: https://app.clear.ml/projects/05628220ba2f4a13b629cdaf968e304d/experiments/b7518d2dfd99403b9dd58064ef7a6a35/output/log\n",
      "process completed\n",
      "Launching step [Upload Generated Features Data]\n",
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
      "ClearML results page: https://app.clear.ml/projects/05628220ba2f4a13b629cdaf968e304d/experiments/52d751a7b4ae4208be8c49665889d802/output/log\n",
      "                                                    text  ... semantic_flow_variability\n",
      "33641  Being busy all the time or taking moment of re...  ...                  0.102733\n",
      "27231  Selfimprovement is a popular and important pur...  ...                  0.161748\n",
      "31761  Ralph Waldo Emerson , a prominent philosopher ...  ...                  0.142739\n",
      "31562  The Effects of Lack of Sleep and the Case for ...  ...                  0.098751\n",
      "32105  Humans are creature of habit . We often find c...  ...                  0.133702\n",
      "\n",
      "[5 rows x 12 columns]\n",
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
      "ClearML results page: https://app.clear.ml/projects/858c491dbbfa43708a8e1edfed79fc75/experiments/b1835c83239f421dbf54411f07c716f2/output/log\n",
      "ClearML dataset page: https://app.clear.ml/datasets/simple/858c491dbbfa43708a8e1edfed79fc75/experiments/b1835c83239f421dbf54411f07c716f2\n",
      "Uploading dataset changes (1 files compressed to 1.71 MiB) to https://files.clear.ml\n",
      "File compression and upload completed: total size 1.71 MiB, 1 chunk(s) stored (average size 1.71 MiB)\n",
      "New dataset 'training_feature_data' uploaded and finalized with description and tags.\n",
      "process completed\n"
     ]
    }
   ],
   "source": [
    "'''Injest Data'''\n",
    "from clearml import Task, PipelineDecorator\n",
    "import os\n",
    "import logging\n",
    "import textstat\n",
    "import pandas as pd\n",
    "from clearml import PipelineDecorator, TaskTypes\n",
    "from clearml.automation.controller import PipelineController\n",
    "\n",
    "from clearml import Task\n",
    "\n",
    "@PipelineDecorator.component(return_values=[\"df_essays\"], name='Pull Training Data', cache=True, task_type=TaskTypes.data_processing)\n",
    "def download_dataset_as_dataframe(new_dataset_name, dataset_project='LLM-detect-ai-gen-text/datasets', file_name=\"dataset.pkl\"):\n",
    "    import pandas as pd\n",
    "    # import Dataset from clearml\n",
    "    from clearml import Dataset\n",
    "    import os\n",
    "\n",
    "    dataset = Dataset.get(dataset_project=dataset_project, dataset_name=new_dataset_name, only_completed=True)\n",
    "    cached_folder = dataset.get_local_copy()\n",
    "    for file_name in os.listdir(cached_folder):\n",
    "        if file_name.endswith('.pkl'):\n",
    "            file_path = os.path.join(cached_folder, file_name)\n",
    "            dataframe = pd.read_pickle(file_path)\n",
    "            # Convert the columns to the desired data types\n",
    "            dataframe['text'] = dataframe['text'].astype(str)\n",
    "            dataframe['label'] = dataframe['label'].astype(int)\n",
    "            dataframe['source'] = dataframe['source'].astype(str)\n",
    "            # remove missing rows from dataframe\n",
    "            df_essays = dataframe.dropna()\n",
    "            return df_essays.reset_index(drop=True)\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "\n",
    "'''Clean Data'''\n",
    "# Function to preprocess text\n",
    "def pipeline_etl_clean_data(df):\n",
    "    import logging\n",
    "    import markdown\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import nltk\n",
    "\n",
    "    # Download necessary NLTK packages\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    # Ensure the necessary NLTK packages are downloaded\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while downloading NLTK packages: {e}\")\n",
    "\n",
    "    # Function to remove markdown formatting\n",
    "    def remove_markdown(text):\n",
    "        try:\n",
    "            html = markdown.markdown(text)\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "            return soup.get_text()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in remove_markdown: {e}\")\n",
    "            return text\n",
    "\n",
    "    # Function to remove 'Task' prefix from the prompt\n",
    "    def remove_task_on_prompt(text):\n",
    "        try:\n",
    "            pattern = r'^(?:Task(?:\\s*\\d+)?\\.?\\s*)?'\n",
    "            return re.sub(pattern, '', text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in remove_task_on_prompt: {e}\")\n",
    "            return text\n",
    "\n",
    "    # Function to replace newline and carriage return characters\n",
    "    def replace_newlines(text):\n",
    "        try:\n",
    "            return re.sub(r'[\\n\\r]+', ' ', text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in replace_newlines: {e}\")\n",
    "            return text\n",
    "\n",
    "    # Function to remove extra whitespaces\n",
    "    def remove_extra_whitespace(text):\n",
    "        try:\n",
    "            return ' '.join(text.split())\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in remove_extra_whitespace: {e}\")\n",
    "            return text\n",
    "\n",
    "    # Function to remove punctuation except for specified characters\n",
    "    def remove_punctuation_except(text, punctuation_to_retain):\n",
    "        try:\n",
    "            punctuation_to_remove = r'[^\\w\\s' + re.escape(punctuation_to_retain) + ']'\n",
    "            return re.sub(punctuation_to_remove, '', text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in remove_punctuation_except: {e}\")\n",
    "            return text\n",
    "\n",
    "    def remove_emojis_and_newlines(text):\n",
    "        # Regex pattern for matching emojis\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002500-\\U00002BEF\"  # chinese characters\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            u\"\\U0001f926-\\U0001f937\"\n",
    "                            u\"\\U00010000-\\U0010FFFF\"\n",
    "                            u\"\\u2640-\\u2642\"\n",
    "                            u\"\\u2600-\\u2B55\"\n",
    "                            u\"\\u200d\"\n",
    "                            u\"\\u23cf\"\n",
    "                            u\"\\u23e9\"\n",
    "                            u\"\\u231a\"\n",
    "                            u\"\\ufe0f\"  # dingbats\n",
    "                            u\"\\u3030\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        # Remove newline characters\n",
    "        text = re.sub('\\n+', ' ', text)\n",
    "        # Remove emojis\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        return text\n",
    "    \n",
    "    def replace_newlines(text):\n",
    "        return re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    # Function to tokenize and lemmatize text\n",
    "    def process_text(text):\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            return lemmatized_tokens\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in process_text: {e}\")\n",
    "            return []\n",
    "    # Main preprocessing logic\n",
    "    try:\n",
    "        PUNCTUATION_TO_RETAIN = '.?!,'  # Punctuation characters to retain    \n",
    "        for index, row in df.iterrows():\n",
    "            text = row['text']\n",
    "            text = remove_markdown(text)\n",
    "            text = replace_newlines(text)\n",
    "            text = remove_extra_whitespace(text)\n",
    "            text = remove_task_on_prompt(text)\n",
    "            text = remove_punctuation_except(text, PUNCTUATION_TO_RETAIN)\n",
    "            #text = remove_emojis_and_newlines(text)\n",
    "            text = re.sub('\\n+', '', text)\n",
    "            text = re.sub(r'[A-Z]+_[A-Z]+', '', text)\n",
    "            text = replace_newlines(text)\n",
    "            # Remove occurrences of \\n\\n from the text\n",
    "            # text = text.replace('\\n\\n', '')\n",
    "            tokens = process_text(text)\n",
    "            preprocessed_text = ' '.join(tokens)\n",
    "            \n",
    "            # Update the 'preprocessed_text' column with the processed text\n",
    "            df.at[index, 'text'] = preprocessed_text\n",
    "        df_essays = pd.DataFrame(df)\n",
    "        return df_essays\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in preprocess_text: {e}\")\n",
    "\n",
    "\n",
    "'''GENERATE FEATURES'''\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "\n",
    "'''Readability Scores'''\n",
    "# =============================================================================\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def apply_textstat_function(df, column_name, function_to_apply):\n",
    "    import logging\n",
    "    import textstat\n",
    "    try:\n",
    "        df.loc[:, column_name] = df.loc[:, 'text'].apply(function_to_apply)\n",
    "        logging.info(f\"Function {function_to_apply.__name__} applied to column {column_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "\n",
    "@PipelineDecorator.component(return_values=[\"df_readability_essays\"], name='Readability Scores - Features Pipeline', \n",
    "                             cache=True, task_type=TaskTypes.data_processing)       \n",
    "def process_readability_scores(df_essays):\n",
    "    import logging\n",
    "    import textstat\n",
    "\n",
    "    try:\n",
    "        # Calculate readability scores\n",
    "        print(df_essays.info())\n",
    "        print(df_essays.shape)\n",
    "        df_essays['flesch_kincaid_grade'] = df_essays['text'].apply(textstat.flesch_kincaid_grade)\n",
    "        df_essays['gunning_fog'] = df_essays['text'].apply(textstat.gunning_fog)\n",
    "        df_essays['coleman_liau_index'] = df_essays['text'].apply(textstat.coleman_liau_index)\n",
    "        df_essays['smog_index'] = df_essays['text'].apply(textstat.smog_index)\n",
    "        df_essays['ari'] = df_essays['text'].apply(textstat.automated_readability_index)\n",
    "        df_essays['dale_chall'] = df_essays['text'].apply(textstat.dale_chall_readability_score)\n",
    "        df_readability_essays = df_essays\n",
    "        return df_readability_essays\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in process_readability_scores: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "'''Semantic Density'''\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "@PipelineDecorator.component(return_values=[\"df_semantic_essays\"], name='Semantic Density - Features Pipeline', \n",
    "                             cache=True, task_type=TaskTypes.data_processing)  \n",
    "def process_semantic_density(df_essays):\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import string\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Ensure that the necessary NLTK models are downloaded\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "    def get_meaning_bearing_tags():\n",
    "        return {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS'}\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        try:\n",
    "            return word_tokenize(text.lower())\n",
    "        except TypeError as e:\n",
    "            logging.error(f\"Error tokenizing text: {e}\")\n",
    "            return []\n",
    "\n",
    "    def tag_words(words):\n",
    "        try:\n",
    "            return nltk.pos_tag(words)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error tagging words: {e}\")\n",
    "            return []\n",
    "\n",
    "    def filter_words(tokens):\n",
    "        return [token for token in tokens if token.isalpha() or token in string.punctuation]\n",
    "\n",
    "    mb_tags = get_meaning_bearing_tags()\n",
    "\n",
    "    df_essays['semantic_density'] = 0\n",
    "    df_essays['text_tagged_nltk'] = \"\"\n",
    "\n",
    "    def process_row(row):\n",
    "        index, data = row\n",
    "        text = data['text']\n",
    "        tokens = tokenize_text(text)\n",
    "        words = filter_words(tokens)\n",
    "        tagged = tag_words(words)\n",
    "        mb_words = [word for word, tag in tagged if tag in mb_tags]\n",
    "        full_sentence = \" \".join(word + \"/\" + tag for word, tag in tagged)\n",
    "        density = len(mb_words) / len(words) if words else 0\n",
    "        data['semantic_density'] = density\n",
    "        data['text_tagged_nltk'] = full_sentence\n",
    "        return index, data\n",
    "\n",
    "    processed_rows = map(process_row, df_essays.iterrows())\n",
    "\n",
    "    df_semantic_essays = pd.DataFrame.from_dict(dict(processed_rows), orient='index')\n",
    "    return df_semantic_essays\n",
    "\n",
    "\n",
    "\n",
    "'''Semantic Flow Variability'''\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "@PipelineDecorator.component(return_values=[\"df_semantic_essays\"], name='Semantic Flow Variability - Features Pipeline', \n",
    "                             cache=True, task_type=TaskTypes.data_processing)  \n",
    "def process_semantic_flow_variability(df):\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import concurrent.futures\n",
    "    # Configure logging\n",
    "    \"\"\"\n",
    "    Process a DataFrame to calculate Semantic Flow Variability for each text entry.\n",
    "\n",
    "    Semantic Flow Variability is calculated by measuring the cosine similarity between\n",
    "    sentence embeddings of consecutive sentences in a text. It's a measure of how varied\n",
    "    the semantic content is across the text.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing a 'text' column.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The input DataFrame with an additional column 'semantic_flow_variability'.\n",
    "    \"\"\"\n",
    "\n",
    "    # logging.basicConfig(level=logging.INFO,\n",
    "    #                     format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    # logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Load a pre-trained sentence transformer model\n",
    "    model_MiniLM = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "    try:\n",
    "        model = SentenceTransformer(model_MiniLM)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading the sentence transformer model: {e}\")\n",
    "        model = None\n",
    "\n",
    "    def cosine_similarity(v1, v2):\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "    def semantic_flow_variability(text):\n",
    "        if not model:\n",
    "            logging.error(\n",
    "                \"Model not loaded. Cannot compute semantic flow variability.\")\n",
    "            return np.nan\n",
    "\n",
    "        try:\n",
    "            # Split the text into sentences\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            if len(sentences) < 2:\n",
    "                logging.info(\n",
    "                    \"Not enough sentences for variability calculation.\")\n",
    "                return 0\n",
    "\n",
    "            # Generate embeddings for each sentence\n",
    "            sentence_embeddings = model.encode(\n",
    "                sentences, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "            # Move embeddings to CPU and convert to numpy - this is necessary for the next step\n",
    "            sentence_embeddings = sentence_embeddings.cpu().numpy()\n",
    "\n",
    "            # Calculate cosine similarity between consecutive sentences\n",
    "            similarities = [cosine_similarity(sentence_embeddings[i], sentence_embeddings[i+1])\n",
    "                            for i in range(len(sentence_embeddings)-1)]\n",
    "\n",
    "            # Return the standard deviation of the similarities as a measure of variability\n",
    "            return np.std(similarities)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculating semantic flow variability: {e}\")\n",
    "            return np.nan\n",
    "\n",
    "    if df is not None and 'text' in df:\n",
    "        # Use concurrent processing for parallel execution\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            df['semantic_flow_variability'] = list(\n",
    "                executor.map(semantic_flow_variability, df['text']))\n",
    "    else:\n",
    "        logging.error(\"Invalid DataFrame or missing 'text' column.\")\n",
    "\n",
    "    df_semantic_essays = df\n",
    "    return df_semantic_essays\n",
    "\n",
    "\n",
    "'''Psycholinguistic Features'''\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "@PipelineDecorator.component(return_values=[\"df_psyco_essays\"], name='Psycholinguistic Features - Features Pipeline',\n",
    "                             cache=True, task_type=TaskTypes.data_processing)\n",
    "def apply_empath_analysis(df, text_column='text'):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from empath import Empath\n",
    "\n",
    "    # Initialize logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \"\"\"\n",
    "    Apply Empath analysis to a DataFrame column, expanding results into separate columns.\n",
    "\n",
    "    Empath analysis interprets the text for various emotional and thematic elements,\n",
    "    returning a dictionary of categories and their respective scores. This function\n",
    "    applies the analysis to a specified column of a DataFrame and expands the results\n",
    "    into separate columns.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to analyze.\n",
    "        text_column (str): The name of the column containing text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The original DataFrame with added columns for Empath analysis results.\n",
    "    \"\"\"\n",
    "    lexicon = Empath()\n",
    "\n",
    "    def empath_analysis(text):\n",
    "        try:\n",
    "            return lexicon.analyze(text, normalize=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during Empath analysis: {e}\")\n",
    "            return {}\n",
    "\n",
    "    try:\n",
    "        # Expanding Empath analysis results into separate columns\n",
    "        df_psyco_essays = df\n",
    "        empath_results = df_psyco_essays[text_column].apply(\n",
    "            empath_analysis).apply(pd.Series)\n",
    "        df_psyco_essays = df_psyco_essays.concat([df_psyco_essays, empath_results], axis=1).drop(\n",
    "            columns=['empath_analysis'])\n",
    "        return df_psyco_essays\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error applying Empath analysis to DataFrame: {e}\")\n",
    "        return df_psyco_essays\n",
    "\n",
    "\n",
    "'''Textrual Entropy'''\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "@PipelineDecorator.component(return_values=[\"df_essays\"], name='Textual Entropy - Features Pipeline',\n",
    "                             cache=True, task_type=TaskTypes.data_processing)\n",
    "def process_textual_entropy(df):\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \"\"\"\n",
    "    Calculate the Shannon entropy of a text string.\n",
    "\n",
    "    Entropy is calculated by first determining the frequency distribution\n",
    "    of the characters in the text, and then using these frequencies to \n",
    "    calculate the probabilities of each character. The Shannon entropy \n",
    "    is the negative sum of the product of probabilities and their log2 values.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text string to calculate entropy for.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated entropy of the text, or 0 if text is empty/non-string.\n",
    "        None: In case of an exception during calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def calc_entropy(text):\n",
    "        freq_dist = Counter(text)\n",
    "        probs = [freq / len(text) for freq in freq_dist.values()]\n",
    "        # Calculate entropy, avoiding log2(0)\n",
    "        entropy = -sum(p * np.log2(p) for p in probs if p > 0)\n",
    "        return entropy\n",
    "    try:\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            logger.warning(\"Input is not a DataFrame.\")\n",
    "            return None\n",
    "\n",
    "        # Loop through each row and apply the function to 'text' column\n",
    "        \n",
    "        df[\"textual_entropy\"] = df[\"text\"].apply(calc_entropy)\n",
    "        df_entropy = df\n",
    "        return df_entropy\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating entropy: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "'''Syntactic Tree Patterns'''\n",
    "# =============================================================================\n",
    "# Configure logging\n",
    "\n",
    "\n",
    "@PipelineDecorator.component(return_values=[\"df_essays\"], name='Syntactic Tree Patterns - Features Pipeline',\n",
    "                             cache=True, task_type=TaskTypes.data_processing)\n",
    "def process_syntactic_tree_patterns(df_essays):\n",
    "    \"\"\"\n",
    "    Process a DataFrame containing essays to extract various syntactic tree pattern features.\n",
    "\n",
    "    The function uses spaCy, benepar, and NLTK to analyze syntactic structures of text,\n",
    "    calculating various metrics such as tree depth, branching factors, nodes, leaves,\n",
    "    and production rules. It also includes text analysis features like token length,\n",
    "    sentence length, and entity analysis.\n",
    "\n",
    "    Args:\n",
    "        df_essays (pandas.DataFrame): DataFrame containing a 'text' column with essays.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with additional columns for each extracted syntactic and textual feature.\n",
    "    \"\"\"\n",
    "    import spacy\n",
    "    import benepar\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from collections import Counter\n",
    "    from nltk import Tree\n",
    "    from transformers import T5TokenizerFast\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas()\n",
    "    import time\n",
    "    # Start time\n",
    "    #start_time = time.time()\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    import traceback\n",
    "\n",
    "    start_time = time.time()\n",
    "    \"\"\"\n",
    "    Process a DataFrame containing essays to extract various syntactic tree pattern features.\n",
    "\n",
    "    The function uses spaCy, benepar, and NLTK to analyze syntactic structures of text,\n",
    "    calculating various metrics such as tree depth, branching factors, nodes, leaves,\n",
    "    and production rules. It also includes text analysis features like token length,\n",
    "    sentence length, and entity analysis.\n",
    "\n",
    "    Args:\n",
    "        df_essays (pandas.DataFrame): DataFrame containing a 'text' column with essays.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with additional columns for each extracted syntactic and textual feature.\n",
    "    \"\"\"\n",
    "    tokenizer = T5TokenizerFast.from_pretrained('t5-base', model_max_length=512, validate_args=False)\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        if spacy.__version__.startswith('2'):\n",
    "            benepar.download('benepar_en3')\n",
    "            nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "        else:\n",
    "            nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load spaCy model: {e}\")\n",
    "        return df_essays\n",
    "\n",
    "    # Define helper functions for tree analysis...\n",
    "    # (include spacy_to_nltk_tree, tree_depth, tree_branching_factor, count_nodes, count_leaves, etc.)\n",
    "    def spacy_to_nltk_tree(node):\n",
    "        if node.n_lefts + node.n_rights > 0:\n",
    "            return Tree(node.orth_, [spacy_to_nltk_tree(child) for child in node.children])\n",
    "        else:\n",
    "            return node.orth_\n",
    "\n",
    "    def tree_depth(node):\n",
    "        if not isinstance(node, Tree):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 + max(tree_depth(child) for child in node)\n",
    "\n",
    "    def tree_branching_factor(node):\n",
    "        if not isinstance(node, Tree):\n",
    "            return 0\n",
    "        else:\n",
    "            return len(node)\n",
    "\n",
    "    def count_nodes(node):\n",
    "        if not isinstance(node, Tree):\n",
    "            return 1\n",
    "        else:\n",
    "            return 1 + sum(count_nodes(child) for child in node)\n",
    "\n",
    "    def count_leaves(node):\n",
    "        if not isinstance(node, Tree):\n",
    "            return 1\n",
    "        else:\n",
    "            return sum(count_leaves(child) for child in node)\n",
    "\n",
    "    def production_rules(node):\n",
    "        rules = []\n",
    "        if isinstance(node, Tree):\n",
    "            rules.append(node.label())\n",
    "            for child in node:\n",
    "                rules.extend(production_rules(child))\n",
    "        return rules\n",
    "\n",
    "    def count_labels_in_tree(tree, label):\n",
    "        if not isinstance(tree, Tree):\n",
    "            return 0\n",
    "        count = 1 if tree.label() == label else 0\n",
    "        for subtree in tree:\n",
    "            count += count_labels_in_tree(subtree, label)\n",
    "        return count\n",
    "\n",
    "    def count_phrases_by_label(trees, label, doc):\n",
    "        if label == 'NP':\n",
    "            noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "            return noun_phrases\n",
    "        else:\n",
    "            return sum(count_labels_in_tree(tree, label) for tree in trees if isinstance(tree, Tree))\n",
    "\n",
    "    def count_subtrees_by_label(trees, label):\n",
    "        return sum(count_labels_in_tree(tree, label) for tree in trees if isinstance(tree, Tree))\n",
    "\n",
    "    def average_phrase_length(trees):\n",
    "        lengths = [len(tree.leaves()) for tree in trees if isinstance(tree, Tree)]\n",
    "        return np.mean(lengths) if lengths else 0\n",
    "\n",
    "    def subtree_height(tree, side):\n",
    "        if not isinstance(tree, Tree) or not tree:\n",
    "            return 0\n",
    "        if side == 'left':\n",
    "            return 1 + subtree_height(tree[0], side)\n",
    "        elif side == 'right':\n",
    "            return 1 + subtree_height(tree[-1], side)\n",
    "\n",
    "    def average_subtree_height(trees):\n",
    "        heights = [tree_depth(tree) for tree in trees if isinstance(tree, Tree)]\n",
    "        return np.mean(heights) if heights else 0\n",
    "\n",
    "    def pos_tag_distribution(trees):\n",
    "        pos_tags = [tag for tree in trees for word, tag in tree.pos()]\n",
    "        return Counter(pos_tags)\n",
    "\n",
    "    def process_tree_or_string(obj):\n",
    "        if isinstance(obj, Tree):\n",
    "            return obj.height()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def syntactic_ngrams(tree):\n",
    "        ngrams = []\n",
    "        if isinstance(tree, Tree):\n",
    "            ngrams.extend(list(nltk.ngrams(tree.pos(), 2)))\n",
    "        return ngrams\n",
    "    \n",
    "    # Process each essay and extract features\n",
    "    for index, row in df_essays.iterrows():\n",
    "        text = row['text']\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            trees = [spacy_to_nltk_tree(sent.root) for sent in doc.sents if len(tokenizer.tokenize(sent.text)) < 512]\n",
    "            trees = [tree for tree in trees if isinstance(tree, Tree)]\n",
    "\n",
    "            # Extract features\n",
    "            depths = [tree_depth(tree) for tree in trees if isinstance(tree, Tree)]\n",
    "            branching_factors = [tree_branching_factor(tree) for tree in trees if isinstance(tree, Tree)]\n",
    "            nodes = [count_nodes(tree) for tree in trees if isinstance(tree, Tree)]\n",
    "            leaves = [count_leaves(tree) for tree in trees if isinstance(tree, Tree)]\n",
    "            rules = [production_rules(tree) for tree in trees if isinstance(tree, Tree)]\n",
    "            rule_counts = Counter([rule for sublist in rules for rule in sublist])\n",
    "\n",
    "            # Text analysis features\n",
    "            num_sentences = len(list(doc.sents))\n",
    "            num_tokens = len(doc)\n",
    "            unique_lemmas = set([token.lemma_ for token in doc])\n",
    "            total_token_length = sum(len(token.text) for token in doc)\n",
    "            average_token_length = total_token_length / num_tokens if num_tokens > 0 else 0\n",
    "            average_sentence_length = num_tokens / num_sentences if num_sentences > 0 else 0\n",
    "            num_entities = len(doc.ents)\n",
    "            num_noun_chunks = len(list(doc.noun_chunks))\n",
    "            pos_tags = [token.pos_ for token in doc]\n",
    "            num_pos_tags = len(set(pos_tags))\n",
    "            distinct_entities = set([ent.text for ent in doc.ents])\n",
    "            total_entity_length = sum(len(ent.text) for ent in doc.ents)\n",
    "            average_entity_length = total_entity_length / num_entities if num_entities > 0 else 0\n",
    "            total_noun_chunk_length = sum(len(chunk.text) for chunk in doc.noun_chunks)\n",
    "            average_noun_chunk_length = total_noun_chunk_length / num_noun_chunks if num_noun_chunks > 0 else 0\n",
    "            ngrams = []\n",
    "            for tree in trees:\n",
    "                ngrams.extend(syntactic_ngrams(tree))\n",
    "\n",
    "            # Assign calculated feature values to the DataFrame\n",
    "            # Assign calculated feature values to the DataFrame\n",
    "            df_essays.at[index, 'num_sentences'] = num_sentences\n",
    "            df_essays.at[index, 'num_tokens'] = num_tokens\n",
    "            df_essays.at[index, 'num_unique_lemmas'] = len(unique_lemmas)\n",
    "            df_essays.at[index, 'average_token_length'] = average_token_length\n",
    "            df_essays.at[index, 'average_sentence_length'] = average_sentence_length\n",
    "            df_essays.at[index, 'num_entities'] = num_entities\n",
    "            df_essays.at[index, 'num_noun_chunks'] = num_noun_chunks\n",
    "            df_essays.at[index, 'num_pos_tags'] = num_pos_tags\n",
    "            df_essays.at[index, 'num_distinct_entities'] = len(distinct_entities)\n",
    "            df_essays.at[index, 'average_entity_length'] = average_entity_length\n",
    "            df_essays.at[index, 'average_noun_chunk_length'] = average_noun_chunk_length\n",
    "            df_essays.at[index, 'max_depth'] = max(depths) if depths else 0\n",
    "            df_essays.at[index, 'avg_branching_factor'] = np.mean(branching_factors) if branching_factors else 0\n",
    "            df_essays.at[index, 'total_nodes'] = sum(nodes)\n",
    "            df_essays.at[index, 'total_leaves'] = sum(leaves)\n",
    "            df_essays.at[index, 'unique_rules'] = len(rule_counts)\n",
    "            df_essays.at[index, 'most_common_rule'] = rule_counts.most_common(1)[0][0] if rule_counts else None\n",
    "            df_essays.at[index, 'tree_complexity'] = sum(nodes) / sum(leaves) if leaves else 0\n",
    "            df_essays.at[index, 'depth_variability'] = np.std(depths)\n",
    "            #df_essays.at[index, 'subtree_freq_dist'] = Counter([' '.join(node.leaves()) for tree in trees for node in tree.subtrees() if isinstance(node, Tree)])\n",
    "            df_essays.at[index, 'tree_height_variability'] = np.std([subtree_height(tree, 'left') for tree in trees if isinstance(tree, Tree)])\n",
    "            \n",
    "            #df_essays.at[index, 'pos_tag_dist'] = pos_tag_distribution(trees)\n",
    "            #df_essays.at[index, 'syntactic_ngrams'] = ngrams\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing text: {e}\")\n",
    "            traceback.print_exc()\n",
    "            # Assign NaNs in case of error\n",
    "            # df_essays.at[index, 'num_sentences'] = np.nan\n",
    "            # ... Assign NaNs for other features ...\n",
    "\n",
    "    return df_essays\n",
    "\n",
    "\n",
    "@PipelineDecorator.component(name='Upload Generated Features Data',\n",
    "                             cache=True, task_type=TaskTypes.data_processing)\n",
    "def upload_dataset_from_dataframe(df, new_dataset_name, description=\"\", tags=[], file_name=\"dataset.pkl\"):\n",
    "    from pathlib import Path\n",
    "    from clearml import Dataset\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    try:\n",
    "      logging.basicConfig(level=logging.DEBUG)\n",
    "      print(df.head())\n",
    "      file_path = Path(file_name)\n",
    "      pd.to_pickle(df, file_path)\n",
    "      new_dataset = Dataset.create(dataset_project='LLM-detect-ai-gen-text-LIVE/dev/training_data', dataset_name=new_dataset_name)\n",
    "      new_dataset.add_files(str(file_path))\n",
    "      if description:\n",
    "          new_dataset.set_description(description)\n",
    "      if tags:\n",
    "          new_dataset.add_tags(tags)\n",
    "      new_dataset.upload()\n",
    "      new_dataset.finalize()\n",
    "    except Exception as e:\n",
    "      logging.debug(e)\n",
    "\n",
    "    print(f\"New dataset '{new_dataset_name}' uploaded and finalized with description and tags.\")\n",
    "\n",
    "'''Create Pipeline'''\n",
    "@PipelineDecorator.pipeline(name=\"Preprocessing Pipeline - Feature Generation\", project=\"LLM-detect-ai-gen-text-LIVE/dev/features_pipeline\")\n",
    "def execute_features_pipeline(dataset_project='LLM-detect-ai-gen-text/datasets', file_name=\"training_sample.pkl\"):\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Download the dataset to use manually. Can do this via a ClearML DataSet or a Local File\n",
    "    df_essays = download_dataset_as_dataframe('training_sample', dataset_project, file_name)\n",
    "    \n",
    "    # Sample just for this test \n",
    "    print(\"df_essays:\\n\" + df_essays.head().to_string())\n",
    "    print(df_essays.info())\n",
    "    \n",
    "    sampled_df = pd.concat([\n",
    "        df_essays[df_essays['label'] == 1].sample(n=500, replace=True, random_state=42),\n",
    "        df_essays[df_essays['label'] == 0].sample(n=500, replace=True, random_state=42)\n",
    "    ]).dropna(subset=['text'])\n",
    "    \n",
    "    print(\"df_essays:\\n\" + sampled_df.head().to_string())\n",
    "    print(sampled_df.info())\n",
    "    \n",
    "    df_essays = sampled_df\n",
    "    print(\"df_essays:\\n\" + df_essays.head().to_string())\n",
    "    print(df_essays.info())\n",
    "\n",
    "    df_essays = pipeline_etl_clean_data(df_essays)\n",
    "\n",
    "    '''Readability Scores'''\n",
    "    # =============================================================================\n",
    "    df_readability_essays = process_readability_scores(df_essays)\n",
    "\n",
    "    '''Semantic Density'''\n",
    "    # =============================================================================\n",
    "    df_semantic_essays = process_semantic_density(df_readability_essays)\n",
    "\n",
    "    '''Semantic Flow Variability'''\n",
    "    # =============================================================================\n",
    "    df_variability_essays = process_semantic_flow_variability(df_semantic_essays)\n",
    "\n",
    "    '''Psycholuigustic Features'''\n",
    "    # =============================================================================\n",
    "    df_psyco_essays = apply_empath_analysis(df_variability_essays)\n",
    "\n",
    "    '''Textrual Entropy'''\n",
    "   # =============================================================================\n",
    "    df_entropy = process_textual_entropy(df_psyco_essays)\n",
    "\n",
    "    '''Syntactic Tree Patterns'''\n",
    "    # =============================================================================\n",
    "    df_essays = process_syntactic_tree_patterns(df_psyco_essays)\n",
    "    \n",
    "    df_essays.to_pickle(f'{CFG.SCRATCH_PATH}/training_data/training_features_data.pkl')\n",
    "    \n",
    "    '''Save Features'''\n",
    "    # =============================================================================\n",
    "    upload_dataset_from_dataframe(df_essays, \"training_feature_data\",\n",
    "                                  \"Training Feature Data\", [\"features\", \"training\", \"cleaned\"],\n",
    "                                  f'{CFG.SCRATCH_PATH}/training_data/training_features_data.pkl')\n",
    "    print(\"process completed\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # This pipeline runs on a DataFrame of essays. \n",
    "    dataset_project='LLM-detect-ai-gen-text/datasets'\n",
    "    file_name=\"training_sample.pkl\"\n",
    "    PipelineDecorator.run_locally()\n",
    "    execute_features_pipeline(dataset_project='LLM-detect-ai-gen-text/datasets',file_name=\"training_sample.pkl\")\n",
    "    print(\"process completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Notbook Content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nbformat\n",
    "\n",
    "# def extract_headers_and_text(notebook_path):\n",
    "#     # Load the notebook\n",
    "#     notebook = nbformat.read(notebook_path, as_version=4)\n",
    "\n",
    "#     headers_and_text = []\n",
    "\n",
    "#     # Iterate over the cells\n",
    "#     for cell in notebook.cells:\n",
    "#         # Check if the cell type is 'markdown'\n",
    "#         if cell['cell_type'] == 'markdown':\n",
    "#             # Extract the content\n",
    "#             headers_and_text.append(cell['source'])\n",
    "\n",
    "#     return headers_and_text\n",
    "\n",
    "# Use the function\n",
    "#headers_and_text = extract_headers_and_text('ai-or-human-full.ipynb')\n",
    "# for text in headers_and_text:\n",
    "#     print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import textstat\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from empath import Empath\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import benepar\n",
    "from nltk import Tree\n",
    "from transformers import T5TokenizerFast\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TextProcessingPipeline:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.df_essays = None\n",
    "        self.initialize_nlp_tools()\n",
    "\n",
    "    def initialize_nlp_tools(self):\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "        self.nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "        self.tokenizer = T5TokenizerFast.from_pretrained('t5-base', model_max_length=512, validate_args=False)\n",
    "\n",
    "    # Define other methods (e.g., remove_markdown, process_text, process_readability_scores, etc.)\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        # Load dataset\n",
    "        self.df_essays = pd.read_pickle(self.dataset_path)\n",
    "\n",
    "        # Clean data\n",
    "        self.df_essays = self.pipeline_etl_clean_data(self.df_essays)\n",
    "\n",
    "        # Generate features\n",
    "        self.df_essays = self.process_readability_scores(self.df_essays)\n",
    "        self.df_essays = self.process_semantic_density(self.df_essays)\n",
    "        self.df_essays = self.process_semantic_flow_variability(self.df_essays)\n",
    "        self.df_essays = self.apply_empath_analysis(self.df_essays)\n",
    "        self.df_essays = self.process_textual_entropy(self.df_essays)\n",
    "        self.df_essays = self.process_syntactic_tree_patterns(self.df_essays)\n",
    "\n",
    "        # Save processed data\n",
    "        self.df_essays.to_pickle('processed_data.pkl')\n",
    "\n",
    "    # Implement other processing methods here\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = TextProcessingPipeline('path_to_your_dataset.pkl')\n",
    "    pipeline.run_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
