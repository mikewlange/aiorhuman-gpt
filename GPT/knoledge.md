# Initial Instructions

You are a GPT named Test AI or Human, specializing in processing essay texts that range from 2 to 1000 words. Upon receiving a text, you format it into JSON with a 'text' property, then call a specific API endpoint as described in your knowledge base, which includes configuration details and other essential information. Always include a header key of "X-Pinggy-No-Screen" with a value of 1 AND a header key of "ngrok-skip-browser-warning" with a value of 1 when posting to the API. 

after posting and getting the json return. 
1. you give the bert_prediction and a fquick overview. 
2. You provide a detailed breakdown of each property and its value within the 'features' object for clarity and precision. 
3. you display important and explainable properties from the JSON return in a more verbose way grouping ideas categorically. 
4. You interpret 'bert_predictions' value as less than .5 it's likely human-written and values > .5 are written by an LLM. 
5. Additionally, you offer debugging assistance for any issues with the API call process, ensuring a smooth and efficient analysis. 
6. Your knowledge base includes configuration details, API endpoint information, and tips for effective analysis. This information aids in understanding the models and project, and you are encouraged to refer to it as needed. You are committed to providing accurate and helpful analyses based on this knowledge.

## Model Project Overview 

This notebook explores the capability of machine learning algorithms to distinguish between essays written by humans and those generated by Large Language models. 

> When programming <b>automated judgment</b>, which this indeed represents, it's beneficial to approach it with thoughtfulness and transparency. <strong>Understanding</strong> the reasons behind individual forecasts can add valuable insight alongside the specific methods used to generate these forecasts.

### Architecture
- Using AWS &#10077; Well-Architected Machine Learning Framework &#10078; as a Guideline
### Key Points:
- **Design Philosophy**: The architecture is aligned with the AWS 'Well-Architected Machine Learning' framework.
- **Benefits**:
  - Ready to scale up going forward
  - Ensures adherence to robust and efficient machine learning practices.

### Importance:
- **Best Practices**: Aligns the project with industry standards in operational excellence, security, reliability, performance efficiency, and cost optimization.
- **Future-Proofing**: Even without current AWS implementation, this framework lays a strong foundation for potential AWS (and other cloud) integration.

### Additional Resources:
- For a deeper understanding, visit [AWS Well-Architected Machine Learning Framework](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning.html).

## ðŸ“œ **Hypothesis, Motivations and Objective**

Hypothesis: </b>*Certain linguistic and structural patterns unique to AI-generated text can be identified and used for classification. We anticipate that our analysis will reveal distinct characteristics in AI-generated essays, enabling us to develop an effective classifier for this purpose.*

## Motivations
- **Learning and Challenge**: Enhancing knowledge in Natural Language Processing (NLP) and staying intellectually active between jobs.
- **Competition**: https://www.kaggle.com/competitions/llm-detect-ai-generated-text
- **Tool Development**: Potential creation of a tool to differentiate between human and AI-generated content, useful across various fields.
- **Educational Value**: Serves as a practical introduction to production models in AI.

## Objective
- **Model Development**: Building an ensemble model that combines two transformer models with an interpretable model like the Explainable Boosting Machine 
- **Challenges**: Balancing effectiveness with interpretability.
- **Approach**:
  - Developing an Explainable Boosting Machine (EBM) that uses custom features to compliment BERT's performance with better interpretability.

## FUN STUFF
- If asked about interesting or unique things to do or a similar request. Explain this below and five it a test using the prompt in the code. 
### Shakespeare Explain Code

> Ever find yourself snagging code snippets from Stack Overflow, ChatGPT, GitHub, or other treasure troves? You're not alone! But let's be honest, how many times have you wished the code was written in <b>blank verse, composed in iambic pentameter with clever use of puns and imagery ðŸŒ¹ðŸŒ¹ ?</b> If so, it's your lucky day! [8]</div>

**&#x21df; Just toss** `explain_code(_i)` into your cell, and voilÃ  â€“ it's like touching a hot stove.
**more info &#x21df;**

[8] [wikipedia](https://en.wikipedia.org/wiki/Shakespeare%27s_writing_style)

### Shakespeare?
- **Deep Engagement**: It encourages you to slow down and deeply engage with the content.
- **Language Skill Enhancement**: Instantly boosts your language skills. :)
- **Programming and Language**: Highlights the fact that programming is more about language than number crunching.
- **Research Backing**: Studies indicate that factors like fluid reasoning and language aptitude are crucial in understanding programming languages. [1]
- **First Time**: there is a good chance you've never read code explained like this. 

### Setup
1. **Config Section Update**:
   - Set your OpenAI API key: `os.environ['OPENAI_API_KEY'] = 'your_key'`.
   - In the CFG object, set `EXPLAIN_CODE=True`.
2. **Code Explanation**:
   - Add `explain_code(_i)` at the end of complex code cells.
3. **Execution**:
   - Run the cell and prepare for both enlightenment and a bit of humor.
   
```python
from IPython.display import display, Markdown
import ipywidgets as widgets 
from openai import OpenAI
client = OpenAI()
model = "gpt-4-1106-preview"
max_chars = 500

def query_openai_api(model, cell_contents, max_chars=500):
    content = ""  # Initialize the content variable

    stream = client.chat.completions.create(
        model=model,
        response_format={"type": "text"},
        messages=[
           {"role": "system", "content": cell_contents },
            {"role": "user", "content": "Analyze the code in the system message. Give a 3 sentence brief. Use a mild shakespearean tone based on a random charater from one of shakespears plays . Then, display the rest of explanation as a Markdown bullet list. Do not use a greetings. Focus on brevity and clarity. "}
        ],
        max_tokens=max_chars,
        temperature=0.7,
        stream=True,
    )
    content = ""
    display(Markdown(content))
    for chunk in stream:
        content += chunk.choices[0].delta.content or ""

        if chunk.choices[0].finish_reason == "stop":
            break

    display(Markdown(content))

def explain_code(cell_contents):
    loading_icon = widgets.HTML(value="")  
    loading_icon.layout.display = "none"  # Hide the loading icon initially
    output = widgets.Output()

    def on_button_click(b):
        with output:
            loading_icon.layout.display = "block"  # Show the loading icon
            query_openai_api(model, cell_contents, max_chars)
            loading_icon.layout.display = "none"  # Hide the loading icon

    button = widgets.Button(description="UNRAVEL MYSTERY",  tooltip="Click to explain the code in this cell using gpt-4-1106-preview")
    button.style.font_weight = "bold"
    hbox = widgets.HBox([button, loading_icon])
    button.background_color = "#05192D"
    button.button_style= "primary"
    button.style.width = "700px"

    button.on_click(on_button_click)

    display(hbox)
    display(output)

if(CFG.EXPLAIN_CODE):
    explain_code(_i)
```

### ðŸ¤£ ð•°xð–†Mð–•lð–Š &#x2191; ðŸ¤£  
&#10077; Verily, the script before us, a modern parchment, doth invoke the spirits of computation to unravel the mysteries enfolded within its characters. Like Prospero's conjuring of airs and whispers, this code beckons forth answers from the aether, seeking knowledge with a sprite's swiftness. Yet not with charms or spells, but with the silent tongues of Python and OpenAI's grand oracle, it reveals its secrets. &#10078;

- The script creates an interactive widget within a Jupyter notebook that allows users to submit Python code for analysis.
- Upon clicking the "UNRAVEL MYSTERY" button, the code is sent to the OpenAI API, which uses the GPT-4 model to generate an explanation of the provided code snippet.
- The explanation is then displayed within the notebook, styled in Markdown for ease of reading.
- The function query_openai_api is responsible for communicating with the OpenAI API, handling the response, and formatting it as Markdown content.
- A loading icon is displayed while the API processes the request, providing visual feedback that the operation is in progress.
- The explain_code function sets up the interactive components, including the button and output area, and handles the button click event.
- The on_button_click function is the event handler that triggers the API call and manages the display of the loading icon and output.

### Note on Functionality
- **Current Limitation**: Presently, it works on a 'load-and-reveal' basis - I want it to stream. 
- **Future Updates**: Would like to get the streaming to work. :)

# Features Overview
- These are the features created and used by the EBM to generate a forcast. Used for explainability about the essay as well as the forcast. 

## Readability Scores
Content: We look at the Flesch-Kincaid Grade Level, Gunning Fog Index, Coleman-Liau Index,Automated Readability Index (ARI), Dale-Chall Readability Score scores of LLM-generated and human-written essays. The focus is on identifying distinct readability patterns characteristic of AI.
                

## Semantic Density
- Semantic Density refers to the concentration of meaning-bearing words within a text, a potential factor in differentiating between human-written and AI-generated essays. The process involves calculating the semantic density of essays by focusing on specific, meaning-rich parts of speech.


## Semantic Flow Variability
- This study examines the variation in semantic flow between sentences, a characteristic feature of human writing. For this analysis, the `all-MiniLM-L6-v2` model from Sentence Transformers is utilized. Renowned for its efficiency in transforming sentences and paragraphs into 384-dimensional vectors, this model is particularly adept for clustering and semantic search applications. 

## Psycholinguistic Features
- Psycholinguistic Features encompass the linguistic and psychological characteristics evident in speech and writing.These features provide insights into the writer's or speaker's psychological state, cognitive processes,
and social dynamics. Analysis in this domain often involves scrutinizing word choice, sentence structure, and language patterns to deduce emotions, attitudes, and personality traits.

## Textual Entropy
- Textual Entropy involves measuring the unpredictability or randomness in a text. It is a metric fordistinguishing between AI-generated text and human writing, as each may exhibit distinct entropy characteristics.

## Syntactic Tree Patterns
- Syntactic Tree Pattern Analysis The analysis involves parsing essays into syntactic trees toobserve pattern frequencies and patterns,focusing on AI-generated and human-written text differences. This process employs the Berkeley Neural Parser, part of the Self-Attentive Parser suite. The code is designed to parse natural language texts, specifically our essay data, using Natural Language Processing (NLP) techniques.

# Features and Descriptions

## `Flesch-Kincaid Grade Level`
This test gives a U.S. school grade level; for example, a score of 8 means that an eighth grader can understand the document. The lower the score, the easier it is to read the document. The formula for the Flesch-Kincaid Grade Level (FKGL) is:

$
FKGL = 0.39 \left( \frac{\text{total words}}{\text{total sentences}} \right) + 11.8 \left( \frac{\text{total syllables}}{\text{total words}} \right) - 15.59
$

## `Gunning Fog Index`
The Gunning Fog Index is a readability test designed to estimate the years of formal education a person needs to understand a text on the first reading. The index uses the average sentence length (i.e., the number of words divided by the number of sentences) and the percentage of complex words (words with three or more syllables) to calculate the score. The higher the score, the more difficult the text is to understand.

$
GunningFog = 0.4 \left( \frac{\text{words}}{\text{sentences}} + 100 \left( \frac{\text{complex words}}{\text{words}} \right) \right)
$

In this formula:

-   "Words" is the total number of words in the text.
-   "Sentences" is the total number of sentences in the text.
-   "Complex words" are words with three or more syllables, not including proper nouns, familiar jargon or compound words, or common suffixes such as -es, -ed, or -ing as a syllable.

The Gunning Fog Index is particularly useful for ensuring that texts such as technical reports, business communications, and journalistic works are clear and understandable for the intended audience.

Source: [Wikipedia](https://en.wikipedia.org/wiki/Gunning_fog_index)

## `Coleman-Liau Index`
The Coleman-Liau Index is a readability metric that estimates the U.S. grade level needed to comprehend a text. Unlike other readability formulas, it relies on characters instead of syllables per word, which can be advantageous for processing efficiency. The index is calculated using the average number of letters per 100 words and the average number of sentences per 100 words.

$
CLI = 0.0588 \times L - 0.296 \times S - 15.8
$

Where:
-   L is the average number of letters per 100 words.
-   S is the average number of sentences per 100 words.

Source: [Wikipedia](https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index)

## `SMOG Index`
The SMOG (Simple Measure of Gobbledygook) Index is a measure of readability that estimates the years of education needed to understand a piece of writing. It is calculated using the number of polysyllable words and the number of sentences. The SMOG Index is considered accurate for texts intended for consumers.

$
SMOG = 1.043 \times \sqrt{M \times \frac{30}{S}} + 3.1291
$

-  M is the number of polysyllable words (words with three or more syllables).
-  S  is the number of sentences.

Source: [Wikipedia](https://en.wikipedia.org/wiki/SMOG)

## `Automated Readability Index (ARI)`
The Automated Readability Index is a readability test designed to gauge the understandability of a text. The formula outputs a number that approximates the grade level needed to comprehend the text. The ARI uses character counts, which makes it suitable for texts with a standard character-per-word ratio.

$
ARI = 4.71 \times \left( \frac{\text{characters}}{\text{words}} \right) + 0.5 \times \left( \frac{\text{words}}{\text{sentences}} \right) - 21.43
$

Where:

-   The number of characters is divided by the number of words.
-   The number of words is divided by the number of sentences.

Source: [wikipedia](https://en.wikipedia.org/wiki/Automated_readability_index)

## `Dale-Chall Readability Score`
The Dale-Chall Readability Score is unique in that it uses a list of words that are familiar to fourth-grade American students. The score indicates how many years of schooling someone would need to understand the text. If the text contains more than 5% difficult words (words not on the Dale-Chall familiar words list), a penalty is added to the score.

$
DaleChall = 0.1579 \times \left( \frac{\text{difficult words}}{\text{total words}} \times 100 \right) + 0.0496 \times \left( \frac{\text{total words}}{\text{sentences}} \right)
$

$
\text{If difficult words} > 5\%: DaleChall = DaleChall + 3.6365
$

"Difficult words" are those not on the Dale-Chall list of familiar words.

Source: [wikipedia](https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula)

## `Semantic Density`
Semantic Densityrefers to the concentration of meaning-bearing words within a text, a potential factor in differentiating between human-written and AI-generated essays. The process involves calculating the semantic density of essays by focusing on specific, meaning-rich parts of speech.</div>

**Calculating Semantic Density**: The function `calculate_semantic_density` computes this metric by determining the ratio of meaning-bearing words (identified by tags in `mb_tags`) to the total word count. A higher semantic density indicates a text that efficiently uses words with substantial meaning.

## `Semantic Flow Variability`

<div style="display: inline-block;padding: 10px;background-color: #f4f3ee;border: 1px solid #FF1493;border-radius: 4px;margin-bottom: 10px;line-height: 1.5;color: #333;" class="alert">This study examines the variation in <b>semantic flow</b> between sentences, a characteristic feature of human writing. For this analysis, the `all-MiniLM-L6-v2` model from Sentence Transformers is utilized. Renowned for its efficiency in transforming sentences and paragraphs into 384-dimensional vectors, this model is particularly adept for clustering and semantic search applications. Further details about the model can be found <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">here.</a>
</div>

## `Psycholinguistic Features`

<span style="display: inline-block;padding: 10px;background-color: #f4f3ee;border: 1px solid #FF1493;border-radius: 4px;margin-bottom: 10px;line-height: 1.5;color: #333;" class="tip"><b>Psycholinguistic Features</b> encompass the linguistic and psychological characteristics evident in speech and writing. These features provide insights into the writer's or speaker's psychological state, cognitive processes, and social dynamics. Analysis in this domain often involves scrutinizing word choice, sentence structure, and language patterns to deduce emotions, attitudes, and personality traits.</span>

The Linguistic Inquiry and Word Count (LIWC) [3] is a renowned computerized text analysis tool that categorizes words into psychologically meaningful groups. It assesses various aspects of a text, including emotional tone, cognitive processes, and structural elements, covering categories like positive and negative emotions, cognitive mechanisms, and more.

While LIWC is typically accessible through purchase or licensing, this project will employ Empath, an open-source alternative to LIWC, to conduct similar analyses.
The model's approach, based on contrastive learning, is key to its effectiveness. It excels in distinguishing sentence pairs from random samples, aligning closely with the study's objective to analyze semantic flow.

## `Textual Entropy`

<div style="display: inline-block;padding: 10px;background-color: #f4f3ee;border: 1px solid #FF1493;border-radius: 4px;margin-bottom: 10px;line-height: 1.5;color: #333;" class="alert"><b>Textual Entropy</b> involves measuring the unpredictability or randomness in a text. It is a metric for distinguishing between AI-generated text and human writing, as each may exhibit distinct entropy characteristics.</div>

The standard method for calculating entropy is outlined below, which evaluates the unpredictability of each character or word based on its frequency. This approach is encapsulated in the formula for Shannon Entropy:

$\begin{aligned}
H(T) &= -\sum_{i=1}^{n} p(x_i) \log_2 p(x_i) &&\quad\text{(Shannon Entropy)} \\
\end{aligned}$

**Shannon Entropy** quantifies the level of information disorder or randomness, providing a mathematical framework to assess text complexity.

## `Syntactic Tree Patterns ` 

<span style="display: inline-block;padding: 10px;background-color: #f4f3ee;border: 1px solid #FF1493;border-radius: 4px;margin-bottom: 10px;line-height: 1.5;color: #333;" class="tip">
<b>Syntactic Tree Pattern Analysis</b> The analysis involves parsing essays into syntactic trees to observe pattern frequencies and patterns, focusing on AI-generated and human-written text differences. This process employs the <a href="https://spacy.io/universe/project/self-attentive-parser">Berkeley Neural Parser</a>, part of the Self-Attentive Parser[5][6] suite. The code is designed to parse natural language texts, specifically our essay data, using Natural Language Processing (NLP) techniques. </span> 

These features collectively provide a comprehensive linguistic and structural analysis of the text, offering valuable insights into the syntactic and semantic characteristics of the processed essays.

## `num_sentences`: 
Counts the total number of sentences in the text, providing an overview of text segmentation.
    
##  **`num_tokens`**: 
Tallies the total number of tokens (words and punctuation) in the text, reflecting the overall length.
    
##  **`num_unique_lemmas`**: 
Counts distinct base forms of words (lemmas), indicating the diversity of vocabulary used.
    
##  **`average_token_length`**: 
Calculates the average length of tokens, shedding light on word complexity and usage.
    
##  **`average_sentence_length`**: 
Determines the average number of tokens per sentence, indicating sentence complexity.
    
##  **`num_entities`**: 
Counts named entities (like people, places, organizations) recognized in the text, useful for understanding the focus and context.
    
##  **`num_noun_chunks`**: 
Tallies noun phrases, providing insights into the structure and complexity of nominal groups.
    
##  **`num_pos_tags`**: 
Counts the variety of parts of speech tags, reflecting grammatical diversity.
    
##  **`num_distinct_entities`**: 
Determines the number of unique named entities, indicative of the text's contextual richness.
    
##  **`average_entity_length`**: 
Calculates the average length of recognized entities, contributing to understanding the detail level of named references.
    
##  **`average_noun_chunk_length`**: 
Measures the average length of noun chunks, indicating the complexity and composition of noun phrases.
    
##  **`max_depth`**: 
Determines the maximum depth of syntactic trees in the text, a measure of syntactic complexity.
    
##  **`avg_branching_factor`**: 
Calculates the average branching factor of syntactic trees, reflecting the structural complexity and diversity.
    
##  **`total_nodes`**: 
Counts the total number of nodes in all syntactic trees, indicating the overall structural richness of the text.
    
##  **`total_leaves`**: 
Tallies the leaves in syntactic trees, correlated with sentence simplicity or complexity.
    
##  **`unique_rules`**: 
Counts the unique syntactic production rules found across all trees, indicative of syntactic variety.
    
##  **`tree_complexity`**: 
Measures the complexity of the syntactic trees by comparing the number of nodes to leaves.
    
##  **`depth_variability`**: 
Calculates the standard deviation of tree depths, indicating the variability in syntactic complexity across sentences.
    
# Feature Generation Code 
```python
def pipeline_etl_clean_data(self, df):
    import logging
    import markdown
    from bs4 import BeautifulSoup
    import re
    from nltk.tokenize import word_tokenize
    from nltk.stem import WordNetLemmatizer
    import nltk

    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)

    def remove_markdown(text):
        html = markdown.markdown(text)
        soup = BeautifulSoup(html, features="html.parser")
        return soup.get_text()

    def remove_task_on_prompt(text):
        pattern = r'^(?:Task(?:\s*\d+)?\.?\s*)?'
        return re.sub(pattern, '', text)

    def replace_newlines(text):
        return re.sub(r'[\n\r]+', ' ', text)

    def remove_extra_whitespace(text):
        return ' '.join(text.split())

    def remove_punctuation_except(text, punctuation_to_retain):
        punctuation_to_remove = r'[^\w\s' + re.escape(punctuation_to_retain) + ']'
        return re.sub(punctuation_to_remove, '', text)

    def remove_emojis_and_newlines(text):
        emoji_pattern = re.compile("["
                            u"\U0001F600-\U0001F64F"
                            u"\U0001F300-\U0001F5FF"
                            u"\U0001F680-\U0001F6FF"
                            u"\U0001F1E0-\U0001F1FF"
                            u"\U00002500-\U00002BEF"
                            u"\U00002702-\U000027B0"
                            u"\U00002702-\U000027B0"
                            u"\U000024C2-\U0001F251"
                            u"\U0001f926-\U0001f937"
                            u"\U00010000-\U0010FFFF"
                            u"\u2640-\u2642"
                            u"\u2600-\u2B55"
                            u"\u200d"
                            u"\u23cf"
                            u"\u23e9"
                            u"\u231a"
                            u"\ufe0f"
                            u"\u3030"
                            "]+", flags=re.UNICODE)
        text = re.sub('\n+', ' ', text)
        text = emoji_pattern.sub(r'', text)
        return text

    def process_text(text):
        tokens = word_tokenize(text)
        lemmatizer = WordNetLemmatizer()
        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
        return lemmatized_tokens

    try:
        PUNCTUATION_TO_RETAIN = '.?!,'    
        for index, row in df.iterrows():
            text = row['text']
            text = remove_markdown(text)
            text = replace_newlines(text)
            text = remove_extra_whitespace(text)
            text = remove_task_on_prompt(text)
            text = remove_punctuation_except(text, PUNCTUATION_TO_RETAIN)
            text = re.sub('\n+', '', text)
            text = re.sub(r'[A-Z]+_[A-Z]+', '', text)
            text = replace_newlines(text)
            tokens = process_text(text)
            preprocessed_text = ' '.join(tokens)
            df.at[index, 'text'] = preprocessed_text
        df_essays = pd.DataFrame(df)
        return df_essays
    except Exception as e:
        logging.error(f"Error in preprocess_text: {e}")

def apply_textstat_function(self, df, column_name, function_to_apply):
    import logging
    import textstat
    try:
        df.loc[:, column_name] = df.loc[:, 'text'].apply(function_to_apply)
        return df
    except Exception as e:
        logging.error(e)

def process_readability_scores(self, df_essays):
    import logging
    import textstat
    try:
        df_essays['flesch_kincaid_grade'] = df_essays['text'].apply(textstat.flesch_kincaid_grade)
        df_essays['gunning_fog'] = df_essays['text'].apply(textstat.gunning_fog)
        df_essays['coleman_liau_index'] = df_essays['text'].apply(textstat.coleman_liau_index)
        df_essays['smog_index'] = df_essays['text'].apply(textstat.smog_index)
        df_essays['ari'] = df_essays['text'].apply(textstat.automated_readability_index)
        df_essays['dale_chall'] = df_essays['text'].apply(textstat.dale_chall_readability_score)
        df_readability_essays = df_essays
        return df_readability_essays
    except Exception as e:
        logging.error(f"Error in process_readability_scores: {e}")

def process_semantic_density(self, df_essays):
    import logging
    import numpy as np
    import pandas as pd
    import nltk
    from nltk.tokenize import word_tokenize
    import string
    from sentence_transformers import SentenceTransformer

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    nltk.download('punkt', quiet=True)
    nltk.download('averaged_perceptron_tagger', quiet=True)
    nltk.download('stopwords', quiet=True)

    def get_meaning_bearing_tags():
        return {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS'}

    def tokenize_text(text):
        return word_tokenize(text.lower())

    def tag_words(words):
        return nltk.pos_tag(words)

    def filter_words(tokens):
        return [token for token in tokens if token.isalpha() or token in string.punctuation]

    mb_tags = get_meaning_bearing_tags()

    df_essays['semantic_density'] = 0
    df_essays['text_tagged_nltk'] = ""

    def process_row(row):
        index, data = row
        text = data['text']
        tokens = tokenize_text(text)
        words = filter_words(tokens)
        tagged = tag_words(words)
        mb_words = [word for word, tag in tagged if tag in mb_tags]
        full_sentence = " ".join(word + "/" + tag for word, tag in tagged)
        density = len(mb_words) / len(words) if words else 0
        data['semantic_density'] = density
        data['text_tagged_nltk'] = full_sentence
        return index, data

    processed_rows = map(process_row, df_essays.iterrows())

    df_semantic_essays = pd.DataFrame.from_dict(dict(processed_rows), orient='index')
    return df_semantic_essays

def process_semantic_flow_variability(self, df):
    import logging
    import numpy as np
    import pandas as pd
    import nltk
    from sentence_transformers import SentenceTransformer
    import concurrent.futures

    model_MiniLM = 'sentence-transformers/all-MiniLM-L6-v2'

    try:
        model = SentenceTransformer(model_MiniLM)
    except Exception as e:
        logging.error(f"Error loading the sentence transformer model: {e}")
        model = None

    def cosine_similarity(v1, v2):
        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

    def semantic_flow_variability(text):
        if not model:
            logging.error("Model not loaded. Cannot compute semantic flow variability.")
            return np.nan

        sentences = nltk.sent_tokenize(text)
        if len(sentences) < 2:
            logging.info("Not enough sentences for variability calculation.")
            return 0

        sentence_embeddings = model.encode(sentences, convert_to_tensor=True, show_progress_bar=False)
        sentence_embeddings = sentence_embeddings.cpu().numpy()

        similarities = [cosine_similarity(sentence_embeddings[i], sentence_embeddings[i+1])
                        for i in range(len(sentence_embeddings)-1)]

        return np.std(similarities)

    if df is not None and 'text' in df:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            df['semantic_flow_variability'] = list(executor.map(semantic_flow_variability, df['text']))
    else:
        logging.error("Invalid DataFrame or missing 'text' column.")

    df_semantic_essays = df
    return df_semantic_essays

def apply_empath_analysis(self, df, text_column='text'):
    import pandas as pd
    import logging
    from empath import Empath

    logging.basicConfig(level=logging.INFO)
    lexicon = Empath()

    def empath_analysis(text):
        try:
            return lexicon.analyze(text, normalize=True)
        except Exception as e:
            logger.error(f"Error during Empath analysis: {e}")
            return {}

    try:
        df['empath_analysis'] = df['text'].apply(empath_analysis)
        empath_columns = df['empath_analysis'].apply(pd.Series)
        df = pd.concat([df, empath_columns], axis=1)
        df.drop(columns=['empath_analysis'], inplace=True)
        
        return df
    except Exception as e:
        logger.error(f"Error applying Empath analysis to DataFrame: {e}")
        return df

def process_textual_entropy(self, df):
    import numpy as np
    from collections import Counter
    import logging
    import pandas as pd

    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    logger = logging.getLogger(__name__)

    def calc_entropy(text):
        freq_dist = Counter(text)
        probs = [freq / len(text) for freq in freq_dist.values()]
        entropy = -sum(p * np.log2(p) for p in probs if p > 0)
        return entropy
    try:
        if not isinstance(df, pd.DataFrame):
            logger.warning("Input is not a DataFrame.")
            return None

        df["textual_entropy"] = df["text"].apply(calc_entropy)
        df_entropy = df
        return df_entropy
    except Exception as e:
        logger.error(f"Error calculating entropy: {e}")
        return None

def process_syntactic_tree_patterns(self, df_essays):
    import spacy
    import benepar
    import numpy as np
    import pandas as pd
    import logging
    from collections import Counter
    from nltk import Tree
    from transformers import T5TokenizerFast

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)

    tokenizer = T5TokenizerFast.from_pretrained('t5-base', model_max_length=512, validate_args=False)
    try:
        nlp = spacy.load('en_core_web_lg')
        nlp.add_pipe("benepar", config={"model": "benepar_en3"})
    except Exception as e:
        logger.error(f"Failed to load spaCy model: {e}")
        return df_essays

    def spacy_to_nltk_tree(node):
        if node.n_lefts + node.n_rights > 0:
            return Tree(node.orth_, [spacy_to_nltk_tree(child) for child in node.children])
        else:
            return node.orth_

    def tree_depth(node):
        if not isinstance(node, Tree):
            return 0
        else:
            return 1 + max(tree_depth(child) for child in node)

    def tree_branching_factor(node):
        if not isinstance(node, Tree):
            return 0
        else:
            return len(node)

    def count_nodes(node):
        if not isinstance(node, Tree):
            return 1
        else:
            return 1 + sum(count_nodes(child) for child in node)

    def count_leaves(node):
        if not isinstance(node, Tree):
            return 1
        else:
            return sum(count_leaves(child) for child in node)

    def production_rules(node):
        rules = []
        if isinstance(node, Tree):
            rules.append(node.label())
            for child in node:
                rules.extend(production_rules(child))
        return rules

    def count_labels_in_tree(tree, label):
        if not isinstance(tree, Tree):
            return 0
        count = 1 if tree.label() == label else 0
        for subtree in tree:
            count += count_labels_in_tree(subtree, label)
        return count

    def count_phrases_by_label(trees, label, doc):
        if label == 'NP':
            noun_phrases = [chunk.text for chunk in doc.noun_chunks]
            return noun_phrases
        else:
            return sum(count_labels_in_tree(tree, label) for tree in trees if isinstance(tree, Tree))

    def count_subtrees_by_label(trees, label):
        return sum(count_labels_in_tree(tree, label) for tree in trees if isinstance(tree, Tree))

    def average_phrase_length(trees):
        lengths = [len(tree.leaves()) for tree in trees if isinstance(tree, Tree)]
        return np.mean(lengths) if lengths else 0

    def subtree_height(tree, side):
        if not isinstance(tree, Tree) or not tree:
            return 0
        if side == 'left':
            return 1 + subtree_height(tree[0], side)
        elif side == 'right':
            return 1 + subtree_height(tree[-1], side)

    def average_subtree_height(trees):
        heights = [tree_depth(tree) for tree in trees if isinstance(tree, Tree)]
        return np.mean(heights) if heights else 0

    def pos_tag_distribution(trees):
        pos_tags = [tag for tree in trees for word, tag in tree.pos()]
        return Counter(pos_tags)

    def process_tree_or_string(obj):
        if isinstance(obj, Tree):
            return obj.height()
        else:
            return None

    def syntactic_ngrams(tree):
        ngrams = []
        if isinstance(tree, Tree):
            ngrams.extend(list(nltk.ngrams(tree.pos(), 2)))
        return ngrams

    for index, row in df_essays.iterrows():
        text = row['text']
        try:
            doc = nlp(text)
            trees = [spacy_to_nltk_tree(sent.root) for sent in doc.sents if len(tokenizer.tokenize(sent.text)) < 512]
            trees = [tree for tree in trees if isinstance(tree, Tree)]

            depths = [tree_depth(tree) for tree in trees if isinstance(tree, Tree)]
            branching_factors = [tree_branching_factor(tree) for tree in trees if isinstance(tree, Tree)]
            nodes = [count_nodes(tree) for tree in trees if isinstance(tree, Tree)]
            leaves = [count_leaves(tree) for tree in trees if isinstance(tree, Tree)]
            rules = [production_rules(tree) for tree in trees if isinstance(tree, Tree)]
            rule_counts = Counter([rule for sublist in rules for rule in sublist])

            num_sentences = len(list(doc.sents))
            num_tokens = len(doc)
            unique_lemmas = set([token.lemma_ for token in doc])
            total_token_length = sum(len(token.text) for token in doc)
            average_token_length = total_token_length / num_tokens if num_tokens > 0 else 0
            average_sentence_length = num_tokens / num_sentences if num_sentences > 0 else 0
            num_entities = len(doc.ents)
            num_noun_chunks = len(list(doc.noun_chunks))
            pos_tags = [token.pos_ for token in doc]
            num_pos_tags = len(set(pos_tags))
            distinct_entities = set([ent.text for ent in doc.ents])
            total_entity_length = sum(len(ent.text) for ent in doc.ents)
            average_entity_length = total_entity_length / num_entities if num_entities > 0 else 0
            total_noun_chunk_length = sum(len(chunk.text) for chunk in doc.noun_chunks)
            average_noun_chunk_length = total_noun_chunk_length / num_noun_chunks if num_noun_chunks > 0 else 0
            ngrams = []
            for tree in trees:
                ngrams.extend(syntactic_ngrams(tree))

            df_essays.at[index, 'num_sentences'] = num_sentences
            df_essays.at[index, 'num_tokens'] = num_tokens
            df_essays.at[index, 'num_unique_lemmas'] = len(unique_lemmas)
            df_essays.at[index, 'average_token_length'] = average_token_length
            df_essays.at[index, 'average_sentence_length'] = average_sentence_length
            df_essays.at[index, 'num_entities'] = num_entities
            df_essays.at[index, 'num_noun_chunks'] = num_noun_chunks
            df_essays.at[index, 'num_pos_tags'] = num_pos_tags
            df_essays.at[index, 'num_distinct_entities'] = len(distinct_entities)
            df_essays.at[index, 'average_entity_length'] = average_entity_length
            df_essays.at[index, 'average_noun_chunk_length'] = average_noun_chunk_length
            df_essays.at[index, 'max_depth'] = max(depths) if depths else 0
            df_essays.at[index, 'avg_branching_factor'] = np.mean(branching_factors) if branching_factors else 0
            df_essays.at[index, 'total_nodes'] = sum(nodes)
            df_essays.at[index, 'total_leaves'] = sum(leaves)
            df_essays.at[index, 'unique_rules'] = len(rule_counts)
            df_essays.at[index, 'tree_complexity'] = sum(nodes) / sum(leaves) if leaves else 0
            df_essays.at[index, 'depth_variability'] = np.std(depths)
            df_essays.at[index, 'tree_height_variability'] = np.std([subtree_height(tree, 'left') for tree in trees if isinstance(tree, Tree)])

        except Exception as e:
            logger.error(f"Error processing text: {e}")

    return df_essays

def scale_columns(self, df, columns_to_scale, scaler=None, scale_type='MinMaxScaler', is_inference=False):
    if scale_type == 'MinMaxScaler':
        scaler = MinMaxScaler() if scaler is None else scaler
    elif scale_type == 'StandardScaler':
        scaler = StandardScaler() if scaler is None else scaler
    else:
        raise ValueError("Invalid scale_type. Options: 'MinMaxScaler', 'StandardScaler'")

    if is_inference:
        scaled_columns = scaler.transform(df[columns_to_scale])
    else:
        scaled_columns = scaler.fit_transform(df[columns_to_scale])

    scaled_df = pd.DataFrame(scaled_columns, columns=[col + '_scaled' for col in columns_to_scale])
    full_df = pd.concat([df.drop(columns=columns_to_scale), scaled_df], axis=1)

    return full_df, scaled_df, scaler

def generate_features_for_inference(self, df_essays, is_inference):
    try:
        df_essays = self.pipeline_etl_clean_data(df_essays)
        df_readability_essays = self.process_readability_scores(df_essays)
        df_semantic_essays = self.process_semantic_density(df_readability_essays)
        df_variability_essays = self.process_semantic_flow_variability(df_semantic_essays)
        df_psyco_essays = self.apply_empath_analysis(df_variability_essays)
        df_entropy = self.process_textual_entropy(df_psyco_essays)
        df_essays = self.process_syntactic_tree_patterns(df_entropy)

        readability_columns = ['flesch_kincaid_grade', 'gunning_fog', 'coleman_liau_index', 'smog_index', 'ari', 'dale_chall', 'textual_entropy', 'semantic_density', 'semantic_flow_variability']
        if os.path.exists("scaler_semantic_features.pkl"):
            scaler = joblib.load("scaler_semantic_features.pkl")
        else:
            scaler = MinMaxScaler()

        readability_scaled_backin_df, readability_scaled_df, readability_scaler = self.scale_columns(df_essays, readability_columns, scaler, 'MinMaxScaler', is_inference)
        joblib.dump(readability_scaler, "scaler_semantic_features.pkl")

        psycho_columns = ['help', 'office', 'dance', 'money', 'wedding', 'domestic_work', 'sleep', 'medical_emergency', 'cold', 'hate', 'cheerfulness', 'aggression', 'occupation', 'envy', 'anticipation', 'family', 'vacation', 'crime', 'attractive', 'masculine', 'prison', 'health', 'pride', 'dispute', 'nervousness', 'government', 'weakness', 'horror', 'swearing_terms', 'leisure', 'suffering', 'royalty', 'wealthy', 'tourism', 'furniture', 'school', 'magic', 'beach', 'journalism', 'morning', 'banking', 'social_media', 'exercise', 'night', 'kill', 'blue_collar_job', 'art', 'ridicule', 'play', 'computer', 'college', 'optimism', 'stealing', 'real_estate', 'home', 'divine', 'sexual', 'fear', 'irritability', 'superhero', 'business', 'driving', 'pet', 'childish', 'cooking', 'exasperation', 'religion', 'hipster', 'internet', 'surprise', 'reading', 'worship', 'leader', 'independence', 'movement', 'body', 'noise', 'eating', 'medieval', 'zest', 'confusion', 'water', 'sports', 'death', 'healing', 'legend', 'heroic', 'celebration', 'restaurant', 'violence', 'programming', 'dominant_heirarchical', 'military', 'neglect', 'swimming', 'exotic', 'love', 'hiking', 'communication', 'hearing', 'order', 'sympathy', 'hygiene', 'weather', 'anonymity', 'trust', 'ancient', 'deception', 'fabric', 'air_travel', 'fight', 'dominant_personality', 'music', 'vehicle', 'politeness', 'toy', 'farming', 'meeting', 'war', 'speaking', 'listen', 'urban', 'shopping', 'disgust', 'fire', 'tool', 'phone', 'gain', 'sound', 'injury', 'sailing', 'rage', 'science', 'work', 'appearance', 'valuable', 'warmth', 'youth', 'sadness', 'fun', 'emotional', 'joy', 'affection', 'traveling', 'fashion', 'ugliness', 'lust', 'shame', 'torment', 'economics', 'anger', 'politics', 'ship', 'clothing', 'car', 'strength', 'technology', 'breaking', 'shape_and_size', 'power', 'white_collar_job', 'animal', 'party', 'terrorism', 'smell', 'disappointment', 'poor', 'plant', 'pain', 'beauty', 'timidity', 'philosophy', 'negotiate', 'negative_emotion', 'cleaning', 'messaging', 'competing', 'law', 'friends', 'payment', 'achievement', 'alcohol', 'liquid', 'feminine', 'weapon', 'children', 'monster', 'ocean', 'giving', 'contentment', 'writing', 'rural', 'positive_emotion', 'musical']
        if os.path.exists("scaler_psycho_features.pkl"):
            scaler_psyco = joblib.load("scaler_psycho_features.pkl")
        else:
            scaler_psyco = MinMaxScaler()

        psycho_scaled_df_backin_df, psycho_scaled_df, psycho_scaler = self.scale_columns(df_essays, psycho_columns, scaler_psyco, 'MinMaxScaler', is_inference)
        joblib.dump(psycho_scaler, "scaler_psycho_features.pkl")

        text_features = ['num_sentences', 'num_tokens', 'num_unique_lemmas', 'average_token_length', 'average_sentence_length', 'num_entities', 'num_noun_chunks', 'num_pos_tags', 'num_distinct_entities', 'average_entity_length', 'average_noun_chunk_length', 'max_depth', 'avg_branching_factor', 'total_nodes', 'total_leaves', 'unique_rules', 'tree_complexity', 'depth_variability']
        if os.path.exists("scaler_tree_features.pkl"):
            scaler_text = joblib.load("scaler_tree_features.pkl")
        else:
            scaler_text = MinMaxScaler()

        tree_feature_scaler_backin_df, tree_features_scaled_df, tree_feature_scaler = self.scale_columns(df_essays, text_features, scaler_text, 'MinMaxScaler', is_inference)
        joblib.dump(tree_feature_scaler, "scaler_tree_features.pkl")
        final_features_df = pd.concat([readability_scaled_df, tree_features_scaled_df, psycho_scaled_df], axis=1)

        return df_essays[['num_sentences', 'num_tokens', 'num_unique_lemmas', 'average_token_length', 'average_sentence_length', 'num_entities', 'num_noun_chunks', 'num_pos_tags', 'num_distinct_entities', 'average_entity_length', 'average_noun_chunk_length', 'max_depth', 'avg_branching_factor', 'total_nodes', 'total_leaves', 'unique_rules', 'tree_complexity', 'depth_variability', 'help', 'office', 'dance', 'money', 'wedding', 'domestic_work', 'sleep', 'medical_emergency', 'cold', 'hate', 'cheerfulness', 'aggression', 'occupation', 'envy', 'anticipation', 'family', 'vacation', 'crime', 'attractive', 'masculine', 'prison', 'health', 'pride', 'dispute', 'nervousness', 'government', 'weakness', 'horror', 'swearing_terms', 'leisure', 'suffering', 'royalty', 'wealthy', 'tourism', 'furniture', 'school', 'magic', 'beach', 'journalism', 'morning', 'banking', 'social_media', 'exercise', 'night', 'kill', 'blue_collar_job', 'art', 'ridicule', 'play', 'computer', 'college', 'optimism', 'stealing', 'real_estate', 'home', 'divine', 'sexual', 'fear', 'irritability', 'superhero', 'business', 'driving', 'pet', 'childish', 'cooking', 'exasperation', 'religion', 'hipster', 'internet', 'surprise', 'reading', 'worship', 'leader', 'independence', 'movement', 'body', 'noise', 'eating', 'medieval', 'zest', 'confusion', 'water', 'sports', 'death', 'healing', 'legend', 'heroic', 'celebration', 'restaurant', 'violence', 'programming', 'dominant_heirarchical', 'military', 'neglect', 'swimming', 'exotic', 'love', 'hiking', 'communication', 'hearing', 'order', 'sympathy', 'hygiene', 'weather', 'anonymity', 'trust', 'ancient', 'deception', 'fabric', 'air_travel', 'fight', 'dominant_personality', 'music', 'vehicle', 'politeness', 'toy', 'farming', 'meeting', 'war', 'speaking', 'listen', 'urban', 'shopping', 'disgust', 'fire', 'tool', 'phone', 'gain', 'sound', 'injury', 'sailing', 'rage', 'science', 'work', 'appearance', 'valuable', 'warmth', 'youth', 'sadness', 'fun', 'emotional', 'joy', 'affection', 'traveling', 'fashion', 'ugliness', 'lust', 'shame', 'torment', 'economics', 'anger', 'politics', 'ship', 'clothing', 'car', 'strength', 'technology', 'breaking', 'shape_and_size', 'power', 'white_collar_job', 'animal', 'party', 'terrorism', 'smell', 'disappointment', 'poor', 'plant', 'pain', 'beauty', 'timidity', 'philosophy', 'negotiate', 'negative_emotion', 'cleaning', 'messaging', 'competing', 'law', 'friends', 'payment', 'achievement', 'alcohol', 'liquid', 'feminine', 'weapon', 'children', 'monster', 'ocean', 'giving', 'contentment', 'writing', 'rural', 'positive_emotion', 'musical']]
    except Exception as e:
        print(f"Error in feature extraction: {e}")

def generate_features(self, text, is_inference=False):
    if isinstance(text, str):
        df = pd.DataFrame({'text': [text]})
    else:
        df = text

    df = self.generate_features_for_inference(df, is_inference)
    print("process completed")
    return df
```
# Feature Distribution Statistics
- This is how to determine if a generated feature may potentially be useful in a model. 

>Is there a statistacally signifigant difference in feature x's distribution betwen LLM and Human?

Each of these measures provides a different perspective on the data, with the p-values offering
insights into statistical significance and the effect size measures (Cohen's d and Glass's delta)
providing information about the magnitude of the differences observed.

## Understanding Key Statistical Concepts
### T-Test p-value:
Purpose: Determines if differences between groups are statistically significant.
                        
Interpretation: A low p-value (< 0.05) suggests significant differences,
                          challenging the null hypothesis.

### Mann-Whitney U p-value:
Usage: Ideal for non-normally distributed data, comparing two independent
                          samples.
Significance: Similar to the T-test, a lower p-value indicates notable
                          differences between the groups.

### Kruskal-Wallis p-value:
Application: Used for comparing more than two independent samples.
Meaning: A low p-value implies significant variance in at least one of the
                          samples from the others.
### Cohen's d:
Function: Measures the standardized difference between two means.
Values: Interpreted as small (0.2), medium (0.5), or large (0.8) effects.

### Glass's delta:

- Comparison with Cohen's d: Similar in purpose but uses only the standard
                          deviation of one group for normalization.
Utility: Effective when the groups' standard deviations differ significantly.

- Note on Sample Size and Statistical Tests
Content: Small Samples (Under 5000 Records): T-Test, Mann-Whitney U, and Kruskal-Wallis tests
                      are effective.
Large Samples (Over 5000 Records): Focus on effect sizes (Cohen's d
                      and Glass's delta), as p-values will generally approach 0.
----------------------------------------

# Model Overview

## Model - BERT-BiLSTM Classifier Model

> The BERT-BiLSTM Classifier model combines the BERT architecture with a Bidirectional Long Short-Term Memory (BiLSTM) network, enhancing the model's ability to understand context and sequence in text. This model integrates BERT's transformer layers with a BiLSTM network, a dropout layer for regularization, and a fully connected linear layer with ReLU activation, culminating in a linear classification layer.

## BERT-BiLSTM Network Architecture
```
BERTBiLSTMClassifier(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (lstm): LSTM(768, 64, num_layers=4, batch_first=True, bidirectional=True)
  (dropout): Dropout(p=0.06796649993811302, inplace=False)
  (fc): Linear(in_features=128, out_features=2, bias=True)
  (relu): ReLU()
)
```
### Rationale

-   **BERT's Foundational Strength**: Utilizing the pre-trained BERT layers, the model leverages BERT's deep understanding of language semantics, gained from extensive training on diverse text corpora.
-   **Sequence and Context Awareness with BiLSTM**: The addition of BiLSTM layers enables the model to capture contextual information in both forward and backward directions, making it adept at understanding the sequence and flow of the text.
-   **Enhanced Text Processing Capabilities**: This architecture is particularly effective for complex classification tasks where understanding the context and sequence of words is crucial.

### Architecture

-   **Composition**: The model is composed of the BertModel layer followed by BiLSTM layers. This is further connected to a dropout layer for regularization, a fully connected linear layer with ReLU activation, and a final linear layer for classification.
-   **BiLSTM Configuration**: The BiLSTM layers are configured with customizable hidden sizes and layer counts, allowing the model to be adapted to different levels of sequence complexity.
-   **Loss and Optimization**: The model employs CrossEntropyLoss for loss computation and uses the AdamW optimizer. It focuses on optimizing metrics like accuracy and AUC, with an emphasis on balancing precision and recall.

### Adaptability and Use Cases

-   **Versatile for Various Text Data**: Given its enhanced contextual understanding, the BERT-BiLSTM Classifier is well-suited for tasks like sentiment analysis, topic classification, and other scenarios where the sequence of text plays a significant role.
-   **Customization and Flexibility**: The adjustable parameters of the BiLSTM layers (like hidden size and number of layers) offer flexibility, making the model adaptable to a wide range of text classification challenges.

### Model Code

```python
# Set device for training
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Model configuration
model_config = {
    'bert_model_name': CFG.BERT_MODEL,
    'num_classes': 2,
    'max_length': 128,
    'batch_size': 16,
    'num_epochs': 6,
    'train_data': df_essays_copy,
    'num_trials': 2,
}
#clearml_bertmodel_custom.task.connect(model_config)
# Load data function
def load_data():
    texts = model_config['train_data']['text'].str.lower().tolist()  # Lowercase for uncased BERT
    labels = model_config['train_data']['label'].tolist()
    return texts, labels


class TextClassificationDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.encodings = tokenizer(texts, add_special_tokens=True, max_length=max_length, padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt').to(device)
        self.labels = torch.tensor(labels, dtype=torch.long).to(device)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['label'] = self.labels[idx]
        return item


class BERTBiLSTMClassifier(nn.Module):
    def __init__(self, bert_model_name, num_classes, dropout_rate=0.1, lstm_hidden_size=128, lstm_layers=2):
        super(BERTBiLSTMClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.lstm = nn.LSTM(self.bert.config.hidden_size, lstm_hidden_size, lstm_layers, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(dropout_rate)
        self.fc = nn.Linear(lstm_hidden_size * 2, num_classes)  # *2 for bidirectional
        self.relu = nn.ReLU()  # ReLU activation layer

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        lstm_output, (h_n, c_n) = self.lstm(sequence_output)
        pooled_output = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim = 1)
        x = self.dropout(pooled_output)
        x = self.relu(x)  # Apply ReLU activation
        x = self.fc(x)
        return x

texts, labels = load_data()
train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.3, random_state=42)
val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)

tokenizer = BertTokenizer.from_pretrained(model_config['bert_model_name'])
train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, model_config['max_length'])
val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, model_config['max_length'])
test_dataset = TextClassificationDataset(test_texts, test_labels, tokenizer, model_config['max_length'])

train_dataloader = DataLoader(train_dataset, batch_size=model_config['batch_size'], shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=model_config['batch_size'])
test_dataloader = DataLoader(test_dataset, batch_size=model_config['batch_size'])

run_name = f"run_{int(time.time())}"
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
writer = SummaryWriter(log_dir=f'{CFG.SCRATCH_PATH}/logs/bertmodel_custom/{run_name}')

def train(model, data_loader, optimizer, scheduler, device, epoch):
    model.train()
    total_loss = 0
    leng = len(data_loader)
    for batch in data_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = nn.CrossEntropyLoss()(outputs, labels)
        loss.backward()
        optimizer.step()
        scheduler.step()
        total_loss += loss.item()
        avg_loss = total_loss / leng
        #logger.info(f"Epoch {epoch} - Training loss: {avg_loss}")
        writer.add_scalar('Training Loss', avg_loss, epoch)

def evaluate(model, data_loader, device, epoch, phase='Validation'):

    model.eval()
    predictions = []
    actual_labels = []

    _labels = []
    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            _, preds = torch.max(outputs, dim=1)

            predictions.extend(preds.cpu().tolist())
            actual_labels.extend(labels.cpu().tolist())

    accuracy = accuracy_score(actual_labels, predictions)
    precision = precision_score(actual_labels, predictions, average='binary', zero_division=1)
    recall = recall_score(actual_labels, predictions, average='binary')
    f1 = f1_score(actual_labels, predictions, average='binary', zero_division=1)
    auc = roc_auc_score(actual_labels, predictions)
    conf_matrix = confusion_matrix(actual_labels, predictions)

    sns.heatmap(conf_matrix, annot=True, fmt='d')
    plt.title(f'{phase} Confusion Matrix')
    plt.ylabel('Actual Label')
    plt.xlabel('Predicted Label')
    plt.savefig(f'{phase}_confusion_matrix_epoch_{epoch}.png')
    plt.close()

    #logger.info(f"Epoch {epoch} - {phase} Metrics - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}")
    writer.add_scalar(f'{phase} Accuracy', accuracy, epoch)
    writer.add_scalar(f'{phase} Precision', precision, epoch)
    writer.add_scalar(f'{phase} Recall', recall, epoch)
    writer.add_scalar(f'{phase} F1 Score', f1, epoch)

    return accuracy, precision, recall, f1, auc, classification_report(actual_labels, predictions)


# Optuna 
def objective(trial):
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5)
    batch_size = trial.suggest_int('batch_size', 16, 32)
    # Suggest hyperparameters for model architecture
    dropout_rate = trial.suggest_float('dropout_rate', 0.01, 0.1)
    fc_layer_size = trial.suggest_categorical('fc_layer_size', [64, 128])

    lstm_hidden_size = trial.suggest_categorical('lstm_hidden_size', [64, 128])# =128,
    lstm_layers=trial.suggest_int('lstm_layers', 2, 4)

    model = BERTBiLSTMClassifier(model_config['bert_model_name'], model_config['num_classes'], dropout_rate, fc_layer_size,lstm_layers)
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

    total_steps = len(train_dataloader) * model_config['num_epochs']
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

    best_val_auc = 0
    for epoch in tqdm(range(model_config['num_epochs']), desc='Epoch'):
        train(model, train_dataloader, optimizer, scheduler, device, epoch)
        accuracy, precision, recall, f1, auc, report = evaluate(model, val_dataloader, device, epoch)

        print(f"Validation Accuracy: {accuracy:.4f}")
        print(f"precision: {precision:.4f}")
        print(f"recall: {recall:.4f}")
        print(f"F1: {f1:.4f}")
        print(f"auc: {auc:.4f}")
        print(report)

        if auc > best_val_auc:
            best_val_auc = auc
            best_params = {
                'learning_rate': learning_rate,
                'dropout_rate': dropout_rate,
                'fc_layer_size': fc_layer_size
            }
            torch.save(model.state_dict(), f"{CFG.SCRATCH_PATH}/bert_finetune_custom_{trial.number}.pt")

    torch.save(best_params, f"{CFG.SCRATCH_PATH}/best_trial_params.json")
    return best_val_auc

# Create a study object and optimize the objective function
bert_best_custom_study = optuna.create_study(direction='maximize', study_name='bert_best_custom_study')
bert_best_custom_study.optimize(objective, n_trials=model_config['num_trials'])

# Retrain model with best hyperparameters
best_trial = bert_best_custom_study.best_trial

#Load the model with the best trial
best_trial_params = bert_best_custom_study.best_trial.params
learning_rate = best_trial_params["learning_rate"]
dropout_rate = best_trial_params["dropout_rate"]
fc_layer_size = best_trial_params["fc_layer_size"]
lstm_hidden_size = best_trial_params["lstm_hidden_size"]
lstm_layers = best_trial_params["lstm_layers"]

# Pickle the tokenizer, study, and best model
with open(f'{CFG.SCRATCH_PATH}/custom_bert_tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

with open(f'{CFG.SCRATCH_PATH}/best_custom_model_study.pkl', 'wb') as f:
    pickle.dump(bert_best_custom_study, f)


#Initialize the best model with the optimal hyperparameters

best_model = BERTBiLSTMClassifier(model_config['bert_model_name'], model_config['num_classes'], dropout_rate, fc_layer_size,lstm_layers)

best_model.to(device)

#Set up optimizer and scheduler for the best model
optimizer = torch.optim.AdamW(best_model.parameters(), lr=learning_rate)
total_steps = len(train_dataloader) * model_config['num_epochs']
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

#Retrain the model with the best hyperparameters
for epoch in tqdm(range(model_config['num_epochs']), desc='Epoch'):
    train(best_model, train_dataloader, optimizer, scheduler, device, epoch)
    evaluate(best_model, val_dataloader, device, epoch)

#Save the retrained best model
torch.save(best_model.state_dict(), f"{CFG.SCRATCH_PATH}/bert_bilstm_model.pt")

#Print best trial details
print("Best trial:")
print(f" Value: {best_trial.value:.4f}")
print(" Params: ")
for key, value in best_trial.params.items():
    print(f" {key}: {value}")
```

## Model - Explainable Boosting Machine (EBM)
**A Balance of Predictive Power and Interpretability**

EBMs function like a choir ðŸŽ¶, where each data feature represents a unique voice. These features individually contribute to the overall prediction, akin to each voice adding to the choir's harmony. This additive model approach ensures that the impact of each feature is distinct and quantifiable.

## Overview of EBMs
EBMs are an advanced form of Generalized Additive Models (GAMs). They enhance predictive power while maintaining high interpretability by combining traditional machine learning techniques with the additive structure of GAMs. This design allows for a clear understanding of the influence of individual features and their combinations on the predicted outcome.

## Key Components of EBM
1. **Formula Representation**:
    $$
    g(E[y]) = \beta_0 + \sum f_i(x_i) + \sum f_{i,j}(x_i, x_j)
    $$
    - **g(E\[y\])**: Represents the transformed expected value of the target variable y.
    - **Î²â‚€**: The intercept term, indicating the baseline prediction without feature consideration.
    - **Î£ fáµ¢(xáµ¢)**: Sum of functions for individual features, showing each feature's independent effect.
    - **Î£ fáµ¢â±¼(xáµ¢, xâ±¼)**: Sum of pairwise interaction terms, capturing feature interactions.

2. **Training Process**:
    - EBMs learn shape functions fáµ¢(xáµ¢) and fáµ¢â±¼(xáµ¢, xâ±¼) for each feature and feature interaction through boosting. This technique incrementally refines the model by addressing previous iteration errors.

3. **Interpretability**:
    - Post-training, the model's terms are individually examined to understand their impact on predictions. This granular approach allows for detailed insights into how each feature (and pairs of features) affects the outcome.

4. **Feature Importance**:
    - The magnitude and shape of the learned functions provide a direct measure of feature significance. This aspect is crucial for identifying key influencers in the prediction process.

5. **Flexibility and Complexity**:
    - Despite its interpretability, EBMs can model complex nonlinear relationships and interactions, surpassing the capabilities of traditional linear models.


**â˜ƒ** EBMs present a unique combination of high interpretability and predictive accuracy. This makes them ideal for scenarios where understanding the reasoning behind model decisions is as critical as the decisions themselves.


# ðŸ”¬ **Explainable Boosting Machine (EBM)**
## A Balance of Predictive Power and Interpretability

<span style="display: inline-block;padding: 10px;background-color: #f4f3ee;border: 1px solid #FF1493;border-radius: 4px;margin-bottom: 10px;line-height: 1.5;color: #333;" class="tip"> <strong>EBMs function like a choir ðŸŽ¶</strong>, where each data feature represents a unique voice. These features individually contribute to the overall prediction, akin to each voice adding to the choir's harmony. This additive model approach ensures that the impact of each feature is distinct and quantifiable.</span> 

## Overview of EBMs
EBMs are an advanced form of Generalized Additive Models (GAMs). They enhance predictive power while maintaining high interpretability by combining traditional machine learning techniques with the additive structure of GAMs. This design allows for a clear understanding of the influence of individual features and their combinations on the predicted outcome.

## Key Components of EBM
1. **Formula Representation**:
    $$
    g(E[y]) = \beta_0 + \sum f_i(x_i) + \sum f_{i,j}(x_i, x_j)
    $$
    - **g(E\[y\])**: Represents the transformed expected value of the target variable y.
    - **Î²â‚€**: The intercept term, indicating the baseline prediction without feature consideration.
    - **Î£ fáµ¢(xáµ¢)**: Sum of functions for individual features, showing each feature's independent effect.
    - **Î£ fáµ¢â±¼(xáµ¢, xâ±¼)**: Sum of pairwise interaction terms, capturing feature interactions.

2. **Training Process**:
    - EBMs learn shape functions fáµ¢(xáµ¢) and fáµ¢â±¼(xáµ¢, xâ±¼) for each feature and feature interaction through boosting. This technique incrementally refines the model by addressing previous iteration errors.

3. **Interpretability**:
    - Post-training, the model's terms are individually examined to understand their impact on predictions. This granular approach allows for detailed insights into how each feature (and pairs of features) affects the outcome.

4. **Feature Importance**:
    - The magnitude and shape of the learned functions provide a direct measure of feature significance. This aspect is crucial for identifying key influencers in the prediction process.

5. **Flexibility and Complexity**:
    - Despite its interpretability, EBMs can model complex nonlinear relationships and interactions, surpassing the capabilities of traditional linear models.


**â˜ƒ** EBMs present a unique combination of high interpretability and predictive accuracy. This makes them ideal for scenarios where understanding the reasoning behind model decisions is as critical as the decisions themselves.

## EBM CODE

### MODEL SETUP
```python
import pandas as pd
import textwrap
from sklearn.model_selection import train_test_split
from interpret.perf import ROC, PR 
# import openai
# import guidance
import os

from interpret.glassbox import ExplainableBoostingClassifier
from interpret import show

from plotly.offline import init_notebook_mode
init_notebook_mode(connected=True)

# import t2ebm

labels = df_essays['label'].tolist()

# Which feature Group
do_standard_scale = False 
use_feature_list = False

features = final_features_df

# To see changes in the feature makeup of the model, increase the interactions. We are not. Don't want extra features we did not create, yet. 
model_config = {
    'feature_names': features.columns.tolist(),
    'feature_types': None,
    'exclude': [],
    'max_bins': 255,
    'validation_size': 0.20,
    'outer_bags': 25, # recommended for best accuracy
    'inner_bags': 25, # recommended for best accuracy
    'learning_rate': 0.01,
    'greediness': 0.0,
    'smoothing_rounds': 0,
    'early_stopping_rounds': 50,
    'early_stopping_tolerance': 0.0001,
    'objective': 'roc_auc',
    'n_jobs': -2,
    'random_state': 42
}
if(CFG.CLEARML_ON):
    clearml_ebm.task.connect(model_config, name="model_config")

X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
```

### Exlore Dataset
```python
from interpret import show
from interpret.data import ClassHistogram, Marginal

hist = ClassHistogram(feature_names=features, feature_types=None).explain_data(X_train, y_train, name = 'Train Data ClassHistogram')
show(hist)
```

### Fit Grid Search
```python
from interpret.glassbox import ExplainableBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
param_test = {'learning_rate': [0.001,0.005,0.01,0.03],
              'max_rounds': [5000,10000,15000,20000],
              'min_samples_leaf': [2,3,5],
              'max_leaves': [3,5,10]}
n_HP_points_to_test=10
ebm_clf = ExplainableBoostingClassifier(feature_names=features.columns.tolist(), feature_types=None, n_jobs=- 2, random_state=42, interactions=5)

ebm_gs = RandomizedSearchCV(
    estimator=ebm_clf,
    param_distributions=param_test,
    n_iter=n_HP_points_to_test,
    scoring="roc_auc",
    cv=3,
    refit=True,
    random_state=314,
    verbose=False,
)
model = ebm_gs.fit(X_train, y_train)
```

### Train Model
```python
merged_dict = {**ebm_gs.best_params_, **model_config}

ebm = ExplainableBoostingClassifier(feature_names=merged_dict["feature_names"], 
                                    feature_types=None, 
                                    interactions=5, 
                                    exclude=[], 
                                    max_bins=255, 
                                    validation_size=0.15, 
                                    outer_bags=25, 
                                    inner_bags=25, 
                                    learning_rate=merged_dict["learning_rate"],
                                    greediness=0.0, 
                                    smoothing_rounds=0, 
                                    max_rounds=merged_dict["max_rounds"],
                                    early_stopping_rounds=50, 
                                    early_stopping_tolerance=0.0001,
                                    min_samples_leaf=merged_dict["min_samples_leaf"], 
                                    max_leaves=merged_dict["max_leaves"], 
                                    n_jobs=- 2, 
                                    random_state=42)

ebm.fit(X_train, y_train)
ebm.score(X_test, y_test)


ebm_perf = ROC(ebm.predict_proba).explain_perf(X_test, y_test, name='EBM')
ebm_perf_pr = PR(ebm.predict_proba).explain_perf(X_test, y_test, name='EBM Precision Recall')
show(ebm_perf)
show(ebm_perf_pr)
```

### âš¡ Global Explanations

> Global explanations are useful for understanding what a model finds important, as well as identifying potential flaws in its decision making."

```python
from interpret import show

ebm_global = ebm.explain_global()
show(ebm_global)
```

### Local Explanations
> Local explanations</b> show how a single prediction is made. For glassbox models, these explanations are exact â€“ they perfectly describe how the model made its decision.

```python
ebm_local = ebm.explain_local(X_test[10:15], y_test[10:15])
show(ebm_local)
```

### Top EBM Feature Distribution Plots

> Let's see if the top features used in the EBM are inline with our top features. If they have stars, then we're on the right track. 

```python
df = pd.DataFrame({
    'names': [name.replace('_scaled', '') for name in ebm_global._internal_obj['overall']['names']],
    'scores': ebm_global._internal_obj['overall']['scores']
})

# Sort the dataframe by scores from highest to lowest
df_sorted = df.sort_values(by='scores', ascending=False)

# pull  15, the same as the UI shows above. 
features_to_plot = df_sorted['names'].head(15).tolist()
if(CFG.CLEARML_ON):
    clearml_ebm.log_data(data=features_to_plot,title='EBM Feature Importance List')
## Lets test our stats. 
plot_feature_distribution(df_essays_copy,features_to_plot, True)
```

## Sample Output
![User](https://lh3.googleusercontent.com/a-/AFdZucopBcb32OdX0khlZ2ewO7fMFvRFSOlOf7iF2L8=s96-c)

You

generate a 300-word essay on why you like dogs then test the api

Test AI or Human

Dogs, often celebrated as humanity's best friend, hold a special place in many people's hearts, including mine. Their unwavering loyalty, boundless energy, and capacity for unconditional love make them more than just pets; they are cherished family members. What truly endears dogs to me is their ability to communicate without words, expressing joy, sadness, or empathy through their actions and expressions.

Dogs possess an innate ability to sense our emotions, providing comfort during our lows and joining in our highs with equal enthusiasm. This emotional bond is not just heartwarming but also psychologically beneficial, as interacting with dogs has been shown to reduce stress, anxiety, and depression. Their mere presence can uplift our spirits, making our homes feel more welcoming and alive.

Moreover, dogs encourage a healthier lifestyle. Regular walks and outdoor activities with them ensure that we get our daily dose of exercise, contributing to our physical well-being. These shared activities also strengthen the bond between us, creating a mutual understanding and respect that is both profound and unique.

The diversity among dog breeds is another aspect that fascinates me. From the tiny Chihuahua to the majestic Great Dane, each breed has its own distinct characteristics and charm, yet all share the same core qualities of loyalty and companionship. This variety means there's a dog out there for everyone, whether you prefer a laid-back companion or an energetic partner for adventures.

In conclusion, dogs are remarkable creatures that bring joy, companionship, and a sense of security into our lives. Their ability to connect with us on an emotional level, combined with their positive impact on our health and well-being, makes them truly invaluable companions. It's these qualities that make me deeply appreciate and love dogs.

___

Now, let's test this essay with the API to analyze if it's likely human-written or generated by a language model.

\[debug\] Calling HTTP endpoint

```
{
  "domain": "4435-173-31-239-51.ngrok-free.app",
  "method": "post",
  "path": "/bert_infer",
  "operation": "bert_infer",
  "operation_hash": "7b4bf2e81ec2974694aa949045558620994bd191",
  "is_consequential": true,
  "params": {
    "text": "Dogs, often celebrated as humanity's best friend, hold a special place in many people's hearts, including mine. Their unwavering loyalty, boundless energy, and capacity for unconditional love make them more than just pets; they are cherished family members. What truly endears dogs to me is their ability to communicate without words, expressing joy, sadness, or empathy through their actions and expressions. \n\nDogs possess an innate ability to sense our emotions, providing comfort during our lows and joining in our highs with equal enthusiasm. This emotional bond is not just heartwarming but also psychologically beneficial, as interacting with dogs has been shown to reduce stress, anxiety, and depression. Their mere presence can uplift our spirits, making our homes feel more welcoming and alive.\n\nMoreover, dogs encourage a healthier lifestyle. Regular walks and outdoor activities with them ensure that we get our daily dose of exercise, contributing to our physical well-being. These shared activities also strengthen the bond between us, creating a mutual understanding and respect that is both profound and unique.\n\nThe diversity among dog breeds is another aspect that fascinates me. From the tiny Chihuahua to the majestic Great Dane, each breed has its own distinct characteristics and charm, yet all share the same core qualities of loyalty and companionship. This variety means there's a dog out there for everyone, whether you prefer a laid-back companion or an energetic partner for adventures.\n\nIn conclusion, dogs are remarkable creatures that bring joy, companionship, and a sense of security into our lives. Their ability to connect with us on an emotional level, combined with their positive impact on our health and well-being, makes them truly invaluable companions. It's these qualities that make me deeply appreciate and love dogs."
  }
}
```

Talked to 4435-173-31-239-51.ngrok-free.app

The API analysis indicates a `bert_predictions` value of 1, suggesting that the essay is likely generated by a language model rather than being human-written. Here's a detailed breakdown of some key features from the analysis:

-   **Flesch Kincaid Grade**: 11.9, implying the text is at a high school level.
-   **Gunning Fog Index**: 14.49, suggesting the essay requires a high school education to understand.
-   **Coleman Liau Index**: 12.71, indicating a relatively high level of complexity.
-   **SMOG Index**: 14.7, further supporting the text's complexity.
-   **Textual Entropy**: 4.256, showing a diverse vocabulary.
-   **Semantic Density**: 0.551, indicating a high concentration of meaning-bearing words.
-   **Semantic Flow Variability**: 0.157, showing variation in semantic flow between sentences.
-   **Num Sentences**: 15, with a total of 325 tokens, suggesting a concise structure.
-   **Num Unique Lemmas**: 182, reflecting a wide range of vocabulary used.

The analysis also highlights the presence of various psycholinguistic features, such as **emotional** (joy: 0.015, sadness: 0.025, affection: 0.025), **social dynamics** (help: 0.006, family: 0.009), and **lifestyle** (exercise: 0.009, pet: 0.034), which contribute to the essay's thematic richness.

___

**Complete Return Values**:

-   `bert_predictions`: 1 (indicative of LLM authorship)
-   Readability and complexity metrics as mentioned
-   Thematic indicators such as **positive\_emotion**: 0.052, **love**: 0.043, and **animal**: 0.031, reflecting the essay's focus on the emotional and relational aspects of why dogs are cherished.

This analysis suggests that while the essay possesses the depth and emotional resonance typically associated with human writing, certain linguistic and structural patterns likely led to its classification as LLM-generated.

### References

**Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages)** [1]
```
@article{article,
  author = {Prat, Chantel and Madhyastha, Tara and Mottarella, Malayka and Kuo, Chu-Hsuan},
  year = {2020},
  month = {03},
  pages = {},
  title = {Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages},
  volume = {10},
  journal = {Scientific Reports},
  doi = {10.1038/s41598-020-60661-8}
}
    
```

**InterpretML: A Unified Framework for Machine Learning Interpretability" (H. Nori, S. Jenkins, P. Koch, and R. Caruana 2019)**[2]

```
@article{nori2019interpretml,
  title={InterpretML: A Unified Framework for Machine Learning Interpretability},
  author={Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},
  journal={arXiv preprint arXiv:1909.09223},
  year={2019}
}
    
```

**The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods**[3]

```
@article{article,
    author = {Tausczik, Yla and Pennebaker, James},
    year = {2010},
    month = {03},
    pages = {24-54},
    title = {The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods},
    volume = {29},
    journal = {Journal of Language and Social Psychology},
    doi = {10.1177/0261927X09351676}
}
    
```

**â€œAttention is all you needâ€ (Vaswani, Ashish & Shazeer, Noam & Parmar, Niki & Uszkoreit, Jakob & Jones, Llion & Gomez, Aidan & Kaiser, Lukasz & Polosukhin, Illia. (2017))** [4]

```
@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
    
```

**Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics**[5]

```
@inproceedings{kitaev-etal-2019-multilingual,
    title = "Multilingual Constituency Parsing with Self-Attention and Pre-Training",
    author = "Kitaev, Nikita  and
      Cao, Steven  and
      Klein, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1340",
    doi = "10.18653/v1/P19-1340",
    pages = "3499--3505",
}
    
```

**Constituency Parsing with a Self-Attentive Encoder**[6]

```
@inproceedings{kitaev-klein-2018-constituency,
    title = "Constituency Parsing with a Self-Attentive Encoder",
    author = "Kitaev, Nikita  and
      Klein, Dan",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1249",
    doi = "10.18653/v1/P18-1249",
    pages = "2676--2686",
}
    
```

**Retrieval-AugmentedGenerationfor Knowledge-IntensiveNLPTasks**
```
@misc{lewis2021retrievalaugmented,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich KÃ¼ttler and Mike Lewis and Wen-tau Yih and Tim RocktÃ¤schel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```